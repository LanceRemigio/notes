\section{Inventing the Factorial Function}
The goal of this section is construct a function \( f(x)  \), defined on all of \( \R  \) with the property that \( f(n) = n!  \) for all \( n \in \N  \). This can be done easily by defining a piecewise function such that  
\[ f(x) = 
\begin{cases}
    n! \ &\text{if } n \leq x < n+1, n \in \N \\ 
    1 \ &\text{if } x < 1.
\end{cases}  \] Some questions we can explore is its continuity, differentiability (if differentiable then how many times?). Our goal now is to define a function that extends the definition of the factorial \( n!  \) in a meaningful way to non-natural \(  n \). 
\subsubsection{Exercise 8.4.1} For each \( n \in \N  \), let 
\[  n \# = n + (n -1 ) + (n-2) + \dots + 2 + 1. \]
\begin{enumerate}
    \item[(a)] Without looking ahead, decide if there is a natural way to define \( 0 \#  \). How about \( (-2) \#  \)? Conjecture a reasonable value for \(  \frac{ 7 }{ 2 }  \# \).
        \begin{proof}[Solution]
        
        \end{proof}
    \item[(b)] Now prove \( n \# = \frac{ 1 }{ 2 }  n (n+1) \) for all \( n \in \N  \), and revisit part (a).
        \begin{proof}
        The statement above is clearly true for \( n =1  \). Now assume \(  n \#   = \frac{ 1 }{ 2 }  n (n+1) \) holds for \(  1 \leq n \leq k -1  \). We want to show that \( n \#  \) holds for the \( k  \)th case. By using the definition of \( n \#  \), we can write 
        \begin{align*}
            k \# &= k + (k-1)\#  \\
                 &= k + \frac{ 1 }{ 2 }  k (k-1) \\
                 &= \frac{ 1 }{ 2 }  (k^{2} + k) \\
                 &= \frac{ 1 }{ 2 } k (k+1).
        \end{align*}
        Since \( n \# = \frac{ 1 }{ 2 }  n (n+1)  \) holds for the \( k  \)th case, we know that it holds for any \( n \in \N  \).
        \end{proof}
\end{enumerate}
We can replace the discrete variable \( n \in \N   \) for values of \( x \in \R  \) and the resulting formula 
\[  x \# = \frac{ 1 }{ 2 }  x (x+1)  \] will still make sense.

\subsection{The Exponential Function} 

How is the exponential function like \( 2^{x}  \) defined on \( \R  \)? Typically, \( 2^{x} \) is defined through a series of domain expansions. Starting with the function defined on \( \N  \), we can expand its domain by using reciprocals, then to \(  \Q  \) using roots, and then \( \R  \) using continuity. Our goal in this section is to expand the domain of \( 2^{x }   \) using a different method. 

Our first step is to properly define the natural exponential function \( e^{x}  \). Recall in chapter 6, we constructed a series expansion for \( e^{x}  \). This time, we do the opposite direction; that is, create a proper definition of \( e^{x}  \). We can do this by using the results we have found in our studies of power series expansions. 

Define 
\[  E(x) = \sum_{ n=0  }^{ \infty  } \frac{ x^{n}  }{  n!  } = 1 + x + \frac{ x^{2}  }{ 2!  } + \frac{ x^{3} }{ 3!  } + \dotsb . \]

\subsubsection{Exercise 8.4.2} Verify that the series converges absolutely for all \( x \in \R  \), that \( E(x)  \) is differentiable on \( \R  \), and \( E'(x) = E(x)  \).
\begin{proof}[Solution]
First we prove that the series above converges absolutely for all \( x \in \R  \). Let \( x \in \R  \). Observe that 
\[  \Big| \sum_{ n=0  }^{ \infty  } \frac{ x^{n}  }{  n!  }  \Big| \leq \sum_{ n=0  }^{ \infty  } \Big| \frac{ x^{n}  }{ n!  }  \Big|.    \] By using the ratio test for power series found in section 6.5, we have
\begin{align*}
    \Big| \frac{ a_{n+1} }{ a_{n} }  \Big| &= \Big| \frac{ x^{n+1} }{ (n+1)!  } \cdot \frac{ n!  }{ x^{n} }  \Big|  \\
                                           &= \frac{ | x  |  }{ n+1  } \xrightarrow{n\rightarrow\infty} 0. \\
\end{align*}
Since the limit above is \(  0  \), we know that the series 
\[  \sum_{ n=0  }^{ \infty  } \frac{ x^{n}  }{  n!  } \tag{1} \] converges absolutely for all \( x \in \R  \). Given any compact set in \( \R  \), we know that the convergence of (1) to \( E(x)  \) is uniform. Hence, it must be continuous on any \( A \subseteq \R  \) and differentiable \( n  \) times. Differentiating 
\[  E(x) = \sum_{ n= 0 }^{ \infty  } \frac{ x^{n}  }{ n!  }  \] and reordering indices we find that \( E'(x) = E(x)  \).
\end{proof}

\subsubsection{Exercise 8.4.3} 
\begin{enumerate}
    \item[(a)] Use the results of Exercise 2.8.7 and the binomial formula to show that \( E(x+y) = E(x)E(y)  \) for all \( x , y \in \R  \).
    \begin{proof}
    Let \( x, y \in \R  \). By definition of \( E(x+y)  \), using the binomial formula, we can write 
    \begin{align*}
        E(x+y)    &= \sum_{ n=0  }^{ \infty  } \frac{ (x+y)^{n}  }{  n!  }  \\
                  &= \sum_{ n=0 }^{ \infty  } \sum_{ k=0 }^{ \infty  } \frac{ y^{k } \cdot x^{n-k} }{  k! (n-k)!  }  \\
                  &= \sum_{ n=0  }^{ \infty  } \sum_{ k=0  }^{ \infty  } \frac{ y^{k }  }{  k!  }  \cdot \frac{ x^{n-k} }{ (n-k)!  } \\
                  &=  \Big[ \sum_{ m=0 }^{ \infty  } \frac{ x^{m} }{ m! }  \Big] \Big[ \sum_{ k = 0  }^{ \infty  } \frac{ y^{ k }  }{ k!  }  \Big] \tag{\( n-k = m \)}. \\
    \end{align*}
    Since 
    \begin{align*}
        E(x) &= \sum_{ m=0  }^{ \infty  } \frac{ x^{m}  }{ m!  },  \\
        E(y) &= \sum_{ k=0  }^{ \infty  } \frac{ y^{ k }  }{  k!  } \\
    \end{align*}
    both converge absolutely (by Exercise 2.8.7), we can write 
    \[  E(x+y) = E(x)E(y). \]
    \end{proof}
    \item[(b)] Show that \( E(0) = 1, E(-x) = 1 / E(x)  \), and \( E(x) > 0  \) for all \( x \in \R  \).
        \begin{proof}
        Let \( x \in \R  \). The first fact immediately follows when \( x = 0  \). Now let us show the second fact. Using the first fact and part(a), we can write 
        \[   1 = E(0) = E(x - x ) = E(x) E(-x) \iff E(x)E(-x) = 1.   \] 
        Dividing through by \( E(x)  \) on both sides leads us to our result 
        \[  E(-x) = \frac{ 1 }{ E(x)  }. \]  For the last fact, observe that \( E(x) > 0  \) follows immediately when we consider any \( x \geq 0  \). Suppose we let \( x  \) be negative, then using the fact that 
        \[  E(-x) = \frac{ 1 }{ E(x)  }   \] where \( E(x) > 0  \) for any \( x > 0  \) implies that \( E(-x) = \sum_{ n=0 }^{ \infty  } \frac{ (-1)^{n} x^{n}  }{  n!  } > 0    \).
        \end{proof}
\end{enumerate}

The takeaway here is that the power series \( E(x)  \) contains all the "normal" properties that is associated with the exponential function \( e^{x} \). 

\subsubsection{Exercise 8.4.4} Define \( e = E(1)  \). Show \( E(n) = e^{n}  \) and \( E(m/n) = (\sqrt[n]{e})^{m} \) for all \( m,n \in \Z  \).
\begin{proof}
Let \( P(n)  \) be the statement that \( E(n) = e^{n} \) for all \(  n \in \Z  \). Let our base case be \( n = 1  \). Then by definition, we must have \( E(1) = e  \). Now assume \( E(n) = e^{n}   \) holds for all \( n \in \Z^{+} \). We want to show that \( E(n+1) = e^{n+1 } \) holds. Observe that by part (a) of Exercise 4.4.3, we have 
\begin{align*}
    E(n+1) &= E(n) \cdot E(1)  \\
           &= e^{n} \cdot e \\
           &= e^{n+1}.
\end{align*}
Hence, \( E(n) = e^{n}  \) for all \( n \in \Z^{+} \). To show that the statement also holds for all \( n \in \Z^{-} \), we can just multiply \( n  \) by a negative to get 
\begin{align*}
    E(-n) &= \frac{ 1 }{ E(n) }  \\
          &= \frac{ 1  }{ e^{n}  } \\
          &= e^{-n}.
\end{align*}
Lastly, we show \( E(m/n) = (\sqrt[n]{e} )^{m } \) for all \( n,m \in \Z  \). Let \( n,m \in \Z  \). Observe that 
\[  E(1)  = E \Big( \frac{ n }{ n }  \Big) = ( \sqrt[n]{e})^{n} \implies E(1/n) = \sqrt[n]{e}.  \] Furthermore, we can rewrite \( m / n  \) in the following way where 
\[  \frac{ m }{ n }  = \sum_{ i=0 }^{ m  } \frac{ 1 }{ n }. \]
Then we see that 
\begin{align*}
    E \Big( \frac{ m }{ n }  \Big) &= E \Big( \sum_{ i=0 }^{ m  } \frac{ 1 }{ n }  \Big) \\
                                   &= E \Big( \frac{ 1 }{ n }  \Big) \cdot E \Big( \frac{ 1 }{ n }  \Big) \cdot E \Big( \frac{ 1 }{ n }  \Big) \cdot \ \dotsb \  m \text{ times} \\ 
                                   &= \sqrt[n]{e} \cdot \sqrt[n ]{e } \cdot \sqrt[n]{e} \cdot \ \dotsb \ m \text{ times} \\
                                   &= (\sqrt[n]{e})^{m}.
\end{align*}
\end{proof}

To complete our list of properties of \( e^{x}  \), all we need is its behavior as \( x \to \pm \infty  \).

\begin{definition}{}{}
    Given \( f : [a,\infty )  \to \R \), we say that \( \lim_{ x \to \infty  }  f(x) = L  \) if, for all \( \varepsilon > 0  \), there exists \( M > a  \) such that whenever \( x \geq M  \) it follows that \( |  f(x) - L  | < \varepsilon  \).
\end{definition}

\subsubsection{Exercise 8.4.5} Show \( \lim_{ x \to \infty  }  x^{n} e^{-x} = 0  \) for all \( n = 0,1,2, \dots \ \). To get started notice that when \( x \geq 0   \), all the terms in (1) are positive.
\begin{proof}
    Let \( \varepsilon > 0  \) and \( n \in \N  \). Choose \( M = 1 / \varepsilon  > a   \). Then observe that for any \( x \geq M  \), we have
    \begin{align*}
        \Big| \frac{ x^{n} }{  e^{x} }  - 0  \Big| &= \frac{ x^{n}  }{  e^{x} }  
                                                   <  \frac{ x^{n}  }{  x^{n+1} } 
                                                   = \frac{ 1 }{ x } 
                                                   < \varepsilon.
    \end{align*}
    Hence, \( \lim_{ x \to \infty  }  x^{n} e^{-x} = 0. \)
\end{proof}

\subsection{Other Bases}

Having established a rigorous foundation for \( e^{x}  \), we can now do the same for \( t^{x}  \) for any real number \( t > 0   \). 

\subsubsection{Exercise 8.4.6} 
\begin{enumerate}
    \item[(a)] Explain why we know \( e^{x}  \) has an inverse function; that is, let's call it \( \log(x)  \) defined for any real \(  x > 0  \) and satisfying
        \begin{enumerate}
            \item[(i)] \( \log(e^{y}) = y  \) for all \(  y\in \R  \) and 
            \item[(ii)] \( e^{\log(x)} = x  \), for all \( x > 0  \).
        \end{enumerate}
        \begin{proof}[Solution]
        If we are considering \( f(x) = e^{x} \) defined on \( (0,\infty ) \), then we get that \( f(x)  \) is a bijective function for all \( x \in (0, \infty ) \). To see why, suppose we let \( x, y \in (0,\infty )  \). Since \( \log(x)  \) is defined for all \(  x \in (0,\infty ) \), we can say that 
        \begin{align*}
            E(x) &= E(y) \\
             e^{x}    &= e^{y} \\
             \log(e^{x}) &= \log(e^{y}) \\
             x &= y. 
        \end{align*}
        Hence, \( E(x) = e^{x} \) is an injective function. Now let's show surjectivity. Then letting \( x = \log(y)  \), observe that 
        \[  E(x) = e^{x} = e^{\log(y)} = y. \] Hence, \( E(x)  \) is a surjective function. Since \( E(x)  \) is both injective and surjective, we know that \( E(x)  \) must be bijective and thus must have an inverse function.
        \end{proof}
    \item[(b)] Prove \( (\log x )' = 1 / x   \). (See Exercise 5.2.12.)
        \begin{proof}
        Let \( y = f(x) = e^{x} \). Using the result from Exercise 5.2.12, the fact that \( f'(x) = e^{x} \), and \( e^{\log(x)}  \), we get that 
        \begin{align*}
            (\log x)' &= \frac{ 1 }{ f'(x)  }  \\
                      &= \frac{ 1 }{ e^{\log(x)} } \\
                      &= \frac{ 1 }{ x }.
        \end{align*}
        \end{proof}
    \item[(c)] Fix \( y > 0  \) and differentiate \( \log(xy)  \) with respect to \( x  \). Conclude that 
        \[  \log(xy) = \log(x) + \log(y) \ \text{for all } x,y > 0. \]
        \begin{proof}
        Let \( x, y \in (0,\infty )  \) with \( x = e^{y}   \) and \( y = e^{x} \). Our logarithm properties, we then have \( \log(x) = y  \) and \( \log(y) = x  \). Then by using the properties of \( e^{x}  \) and \( \log(x)   \), observe that  
        \begin{align*}
            \log(xy) &= \log(e^{y} \cdot e^{x} ) \\
                     &= \log(e^{y+x}) \\
                     &= y + x \\
                     &= \log(x) + \log(y).
        \end{align*} 
        Hence, we have 
        \[  \log(xy) = \log(x) + \log(y). \]

        \end{proof}
    \item[(d)] For \( t > 0  \) and \( n \in \N  \), \( t^{n} \) has the usual interpretation as \( t \cdot t \dotsb t  \) ( \( n \) times).
        Show that 
        \[  t^{n} = e^{n \log t } \ \text{for all } n \in \N.   \]
        \begin{proof}
            Let \( t>0  \) and \( n \in \N  \). Observe that \( t = e^{\log(t)} \) and then 
            \[  t^{n} = \Big( e^{\log(t)}  \Big)^{n} = e^{n \log (t)}. \]
        \end{proof}
\end{enumerate}

\begin{definition}{}{}
   Given \( t > 0  \), define the exponential function \( t^{x} \) to be 
   \[  t^{x} = e^{x \log t } \ \text{for all } x \in \R. \]
\end{definition}

\subsubsection{Exercise 8.4.7}  
\begin{enumerate}
    \item[(a)] Show \( t^{m/n} = (\sqrt[n]{t})^{m}  \) for all \( m,n \in \N  \).
        \begin{proof}
        Let \( m, n \in \N  \). Then 
        \[  t^{m/n} = (t^{1/n})^{m} = (\sqrt[n]{t})^{m}. \]
        \end{proof}
    \item[(b)] Show \( \log(t^{x}) = x \log t \), for all \( t > 0  \) and \( x \in \R \).
        \begin{proof}
        Let \( t > 0  \) and \( x \in \R  \). Then observe that 
        \begin{align*}  
            t^{x} = e^{x \log t } &\implies \log(t^{x}) = \log(e^{x\log t }) \\ 
                                  &\implies \log (t^{x} ) = x \log t.
        \end{align*}
        \end{proof}
    \item[(c)] Show \( t^{x} \) is differentiable on \( \R  \) and find the derivative.
        \begin{proof}
        Let \( x,t \in \R  \). To show that \( f(x) = t^{x}  \) is differentiable, we can use the definition of differentiability. Using the fact that \( t^{x} = e^{x \log t }  \), we have    
        \begin{align*}
            f'(c) &= \lim_{ x \to c } \frac{ t^{x} - t^{c} }{  x -c  }  \\
                  &= \lim_{ x \to c }  \frac{ e^{x \log t } - e^{c \log t }  }{ x -c  }.  \\
        \end{align*}
        Observe that \( g(x) = e^{x \log t}  \) is differentiable. Hence, the limit in the last equality exists and therefore \( f'(c)  \) exists. Using the Chain Rule, we get
        \[  f'(x) = (t^{x} )' = (e^{x \log t })' = \log(t) e^{x \log t } = \log (t) t^{x}.\]
        \end{proof} 
\end{enumerate}

The strategy we have been partaking in so far is a similar to how we would define what \( n!  \) would mean if it was replaced by \( x  \in \R  \) instead of \( n \in \N  \). 

\subsection{The Functional Equation}

Our goal now is to somehow extend the domain of the factorial from the set of natural number; that is, 
\[  n! = n (n-1)! \ \text{for all }  n \in \N   \] 
all the way to the set of real numbers with 
\[  x! = x(x-1)! \ \text {for all } x \in \R. \]
Of course, we cannot forget about \(  n = 1   \) implying that \( 0! = 1  \).

\subsubsection{Exercise 8.4.8} Inspired by the fact that \( 0! = 1  \) and \( 1! = 1  \), let \( h(x)  \) satisfy 
\begin{enumerate}
    \item[(i)] \( h(x) = 1  \) for all \( 0 \leq x \leq 1  \), and 
    \item[(ii)] \( h(x) = x h(x-1)  \) for all \( x \in \R  \).
\end{enumerate}
\begin{enumerate}
    \item[(a)] Find a formula for \( h(x)  \) on \( [1,2]  \), \( [2,3]  \), and \( [n, n +1 ] \) for arbitrary \( n \in \N  \).
        \begin{proof}[Solution] 
        On \( [1,2]  \), observe that 
        \[  h(2) = 2 \cdot h(1) = 2 \cdot 1 \cdot h(0) = 2 \]
        and likewise 
        \[  h(1) = 1. \] This tells us that \( h(x)  \) on \( [1,2]  \) must be defined as \( h(x) =x  \). Whereas on \( [2,3]  \), we have 
        \[  h(3) = 3 \cdot h(2) = 3 \cdot 2 \cdot h(1) = 3! = 3 \]
        and 
        \[  h(2) = 2 \cdot h(1) = 2 \cdot 1 \cdot h(0) = 2! = 2. \]
        which tells us that \( h(x)  \) on \( [2,3]  \) must be defined as \( h(x) = x (x-1)  \). On \( [n,n+1] \), observe that 
        \[  h(n) = n \cdot h(n-1) = n \cdot (n-1) \cdot h(n-2) = n!  \]
        and  
        \[  h(n+1) = (n+1) \cdot n \cdot (n-1) \cdot (n-2) = (n+1)!. \]
        This tells us that \( h(x)  \) on \( [n,n+1]  \) will be defined as 
        \[  h(x) = \prod_{i=1}^{n-1} x - i\] which can be proven using induction. 
        \end{proof}
    \item[(b)] Now do the same for \( [-1,0], [-2,-1] , \) and \( [-n, -n+1]  \).
        \begin{proof}[Solution] 
        
        \end{proof}
    \item[(c)] Sketch \( h  \) over the domain \( [-4,4]  \).
        \begin{proof}[Solution]
            \textbf{To do.}
        \end{proof}
\end{enumerate}
Our function above \( h(x)  \) satisfies \( h(n) = n!   \) and it is at least continuous for \( x \geq 0  \). However, we still run into the problem where our piecewise function contains non-differentiable corners. We conclude that from the exercise above that \( x!  \) will have the same asymptotic behavior as \( h  \) at negative integers \( x  \). Hence, it won't be defined on \( x \in \Z^{-} \).

\subsection{Improper Riemann Integrals} 

Our goal in this section is to provide a rigorous foundation for the formula
\[  \int_{ 0 }^{ \infty  }  e^{-t} \ dt. \] 
This is know in our regular Calculus classes as the \textit{improper Riemann integral} which is defined by taking the limit of "proper" integrals over unbounded regions such as \( [0,\infty ] \). 

\begin{definition}{}{}
    Assume \( f  \) is defined on \( [a,\infty ) \) and integrable on every interval of the form \( [a,b]  \). Then define \( \int_{ a }^{ \infty  }  f  \) to be  
    \[  \lim_{ b \to \infty  }  \int_{ a }^{ b } f,  \] provided the limit exists. In this case, we say the improper integral \( \int_{ a }^{ \infty  }  f  \) \textit{converges}. 
\end{definition}

\subsubsection{Exercise 8.4.9} 
\begin{enumerate}
    \item[(a)] Show that the improper integral \( \int_{ a }^{ \infty  } f  \) converges if and only if, for all \( \varepsilon > 0 \), there exists \( M > a  \) such that whenever \( d > c \geq M  \) it follows that 
        \[  \Big| \int_{ c }^{ d } f  \Big|  < \varepsilon. \]
        (In one direction it will be useful to consider the sequence \( a_{n} = \int_{ a }^{ a+ n  } f  \).)
        \begin{proof}
        For the forwards direction, suppose that the improper integral \( \int_{ a }^{ \infty  } f  \) converges. Let \( \varepsilon > 0  \). By assumption, we can find an \( M > a  \) such that whenever \(  d >  c \geq M  \), it follows that  
        \begin{align*}
            \Big| \int_{ a }^{ d } f  - L  \Big| &< \frac{ \varepsilon  }{ 2  } \ \ \text{whenever } d \geq M > a,  \\
            \Big| \int_{ a }^{ c } f - L  \Big| &< \frac{ \varepsilon  }{ 2  }  \ \ \text{whenever } c \geq M > a.
        \end{align*}
        Observe that 
        \[  \int_{ c }^{ d } f = \int_{ c }^{ a } f + \int_{ a }^{ d } f = \int_{ a }^{ d } f  - \int_{ a }^{ c } f.   \]
        Then we have 
        \begin{align*}
            \Big| \int_{ c }^{ d } f  \Big| &= \Big| \int_{ a }^{ d } f  - \int_{ a }^{ c } f  \Big|  \\
                                            &\leq \Big| \int_{ a }^{ d } f - L  \Big| + \Big| L - \int_{ a }^{ c } f  \Big| \\
                                            &< \frac{ \varepsilon  }{ 2  }  + \frac{ \varepsilon  }{ 2 } = \varepsilon. \\
        \end{align*}
        
        Now assume the converse. Let \( \varepsilon > 0 \). We want to show that 
        \[  \lim_{ n \to \infty  }  \int_{ a }^{ a + n }  f  = L. \] By assumption, there exists a natural number \( N > a     \) such that whenever \( a + n > n \geq N  \), we have 
        \begin{align*}
            \Big| \int_{ a }^{ a + n  } f  -  L  \Big| &= \Big| \Big(  \int_{ a }^{ n  }  f  + \int_{ n }^{ a + n } f \Big)  - L  \Big|  \\
                                                       &= \Big| \Big( \int_{ a }^{ n }  f   -L \Big)  + \int_{ n }^{  a+ n } f   \Big| \\
                                                       &\leq \Big| \int_{ a }^{ n } f  - L  \Big| + \Big| \int_{ n }^{ a+n  } f  \Big| \\
                                                       &< \frac{ \varepsilon  }{ 2  } + \frac{ \varepsilon  }{ 2  }  \\
                                                       &= \varepsilon.
        \end{align*}
        \end{proof}
    \item[(b)] Show that if \( 0 \leq f \leq g  \) and \( \int_{ a }^{ \infty  } g   \) converges then \( \int_{ a }^{ \infty  } f   \) converges.
        \begin{proof}
        Let \( \epsilon > 0  \). Our goal is to show that there exists an \( M > a  \) such that whenever \(  d > c \geq M  \), we have 
        \[  \Big| \int_{ c }^{ d }  f  \Big| < \epsilon.  \] 
        Since \(  0 \leq  f \leq  g  \) and \( \int_{ 0 }^{ \infty  }  g   \) converges, there exists an \( M > a  \) such that whenever \( d > d \geq M  \), we have that   
        \begin{align*}
            \Big| \int_{ c }^{ d } f  \Big| &\leq \Big| \int_{ c }^{ d }  g  \Big|  
                                            < \epsilon. 
        \end{align*}
        Hence, we must have that \( \int_{ 0 }^{ \infty  } f  \) converges as well.
        \end{proof}
    \item[(c)] Part (a) is a Cauchy criterion, and part (b) is a comparison test. State and prove an absolute convergence test for improper integrals.  
        \begin{proof}
        
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 8.4.10} 
\begin{enumerate}
    \item[(a)] Use the properties of \( e^{t} \) previously discussed to show 
        \[  \int_{ 0 }^{ \infty  }  e^{-t} \  dt = 1 . \]
        \begin{proof}[Solution]
        Using part (i) of FTC implies
        \begin{align*}
            \int_{ 0 }^{ \infty  }  e^{-t} \ dt &= \lim_{ b \to \infty  } \int_{ 0 }^{ b }  e^{-t} \ dt \\
                                                &= \lim_{ b \to \infty  }  \Big[ - e^{-b} + e^{0} \Big] \\
                                                &= 0 + 1 \\
                                                &= 1.
        \end{align*}
        \end{proof}
    \item[(b)] Show
        \[  \int_{ 0  }^{ \infty  }  e^{-t} \  dt, \ \text{for all } \alpha > 0. \tag{3} \]
        \begin{proof}
        Let \( \alpha > 0  \). Using part (i) of FTC, we have
        \begin{align*}
            \int_{ 0 }^{ \infty  }  e^{- \alpha t} \ dt &= \lim_{ b \to \infty  }  \int_{ 0 }^{ b }  e^{-\alpha t } \  dt \\
                                                        &= \lim_{ b \to \infty  }  \Big[ \frac{ -e^{- b t } }{ b } + \frac{ e^{0} }{ \alpha }    \Big] \\
                                                        &= 0 + \frac{ 1 }{ \alpha } \\
                                                        &= \frac{ 1 }{\alpha }.
        \end{align*}
        \end{proof}
\end{enumerate}

Let us now consider the left side of (3). Differentiating the left hand side, we certainly get the following 
\[  \Big[ \frac{ 1 }{ \alpha  }  \Big]' = \frac{ -1 }{ \alpha^{2} }. \] On the right hand side of (3), however, it is not so obvious whether or not we can "distribute" differentiation inside the integral of (3). Let us pretend that we can so we have 
\[  [e^{- \alpha t }]' = e^{- \alpha t } \cdot (-t). \]
Now let us actually find out if our conjecture that 
\[  \frac{ 1 }{ \alpha^{2}  }  = \int_{ 0 }^{ \infty  }  t e^{-\alpha t } \ dt.  \]

\subsubsection{Exercise 8.4.11} 
\begin{enumerate}
    \item[(a)] Evaluate \( \int_{ 0 }^{ b  } t e^{- \alpha t } \  dt \) using the integration-by-parts formula from Exercise 7.5.6. The result will be an expression in \( \alpha  \) and \( b  \).
        \begin{proof}[Solution]
        Using the integration-by-parts formula, we get that 
        \begin{align*}
            \int_{ 0 }^{ b } t e^{- \alpha t } \  dt &= \Big[  \frac{ - t  }{ \alpha  }  e^{- \alpha t } \Big]_{0}^{b} + \frac{ 1 }{ \alpha  } \int_{ 0 }^{ b } e^{- \alpha t } \  dt \\
                                                     &= \Big[ \frac{ -b e^{-\alpha b } }{ \alpha  }  \Big] +  \frac{ 1 }{ \alpha  } \Big[ \frac{ -1  }{ \alpha  } e^{- \alpha t }  \Big]_{0}^{b} \\
                                                     &= \Big[ \frac{ -b e^{-\alpha b } }{ \alpha  }  \Big] +  \frac{ 1 }{ \alpha  } \Big[ \frac{ -1  }{ \alpha  } e^{- \alpha b  } + \frac{ 1 }{ \alpha  }   \Big] \\
                                                     &= \frac{ -b e^{-\alpha b } }{ \alpha  } - \frac{ e^{- \alpha b }  }{ \alpha^{2}  }  + \frac{ 1 }{ \alpha^{2} }    \\
        \end{align*}
        \end{proof}
    \item[(b)] Now compute \( \int_{ 0 }^{ \infty  }  t e^{-\alpha t } \  dt  \) and verify equation (4).
        \begin{proof}[Solution]
        Letting \( b \to \infty  \) in the result in part (a), gives us 
        \[  \int_{ 0 }^{ \infty  }  t e^{- \alpha t } \  dt = \lim_{ b \to \infty  }  \int_{ 0 }^{ b } t e^{- \alpha t } \ dt = \frac{ 1 }{ \alpha^{2}  }. \]
        \end{proof}
\end{enumerate}

Since the above (4) ended up working out, we have to now create a rigorous foundation for why this works. 

\subsection{Differentiating Under the Integral}

Suppose we have a function of two variables \( f(x,t)  \) that is defined for all \(  x \in [a,b]  \) and \( t \in [c, d]  \). The domain for  \( f  \) can be called the \textit{rectangle \( D  \)} in \( \R^{2} \). 

Let's say that we have \( f  \) continuous at some point \( (x_{0}, t_{0}) \) in \(  D  \)? To have this make more sense, observe that we have a different metric under \( \R^{2} \) which contains the Euclidean distance formula 
\[  \lVert (x,t) - (x_{0}, t_{0}) \rVert = \sqrt{ (x - x_{0})^{2} + (t - t_{0})^{2}  }. \]

\begin{definition}{}{}
   A function \( f: D \to \R   \)  is continuous at \( (x_{0}, t_{0})  \) if for all \( \epsilon > 0  \), there exists \( \delta > 0  \) such that whenever \( \lVert (x,t) - (x_{0}, t_{0})  \rVert < \delta  \), it follows that 
   \[  | f(x,t) - f(x_{0}, t_{0})  | < \epsilon. \]
\end{definition}



