
\section{Power Series}

We can express functions in the form of power series where it takes the form of 
\[  f(x) = \sum_{ n=0  }^{  \infty  } a_n x^n = a_0 + a_1 x + a_2 x^2 + \dots . \]
We want to be able to find all possible \( x \in \R \) such that the above series converges. 

\begin{tcolorbox}
\begin{thm}
If a power series \( \sum_{ n=0  }^{ \infty  } a_n x^n  \) converges at some point \( x_0 \in \R  \), then it converges absolutely for any \( x  \) satisfying \( | x  |  < | x_0  |  \).
\end{thm}
\end{tcolorbox}

\begin{proof}
Assume the power series \( \sum_{ n=0  }^{ \infty  } a_n x^n  \) converges at some point \( x_0 \in \R  \). Since the sequence of terms \( (a_n x_0^n) \) converges to zero, we know that they must be bounded. Hence, there exists \( M > 0  \) such that \( | a_n x_0^n |  \leq M  \) for all \( n \in \N  \). If \( x \in \R  \) satisfies the property that \( | x  |  < | x_0  |  \), then we have that 
\[  | a_n x^n  |  = | a_n x_0^n  | \Big| \frac{ x  }{  x_0  }  \Big|^n \leq M \Big| \frac{ x  }{  x_0  }  \Big|^n.\] This tells us that the series 
\[  \sum_{ n=0  }^{  \infty   } M \Big| \frac{ x  }{  x_0  }  \Big|^n  \] is a geometric series with \( | x / x_0  |  <  1  \) which converges. Hence, we can use the comparison test, to state that the original series \( \sum_{ n = 0  }^{  \infty  } a_n x^n  \) converges absolutely.
\end{proof}

This theorem tells us a few things:
\begin{enumerate}
    \item[(a)] The set of points for which a given power series converges must be either \( \{ 0  \}  \), \( \R  \), or some bounded interval that is centered at \( x = 0  \). 
    \item[(b)] The strict inequality in the condition tells us that the intervals may come in the following forms; either, \( (-R , R ) \), \( [-R, R ] \), \( (-R, R ] \), or \( [-R , R ] \).
    \item[(c)] We denote the value \( R  \) in the intervals above as the \textit{radius of convergence} of a power series which can be either \( 0  \) or \( \infty  \) to represent \( \{ 0  \}  \) or \( \R   \) respectively.
\end{enumerate} 

Below are questions to be answered about the properties of power series:
\begin{enumerate}
    \item[(a)] Continuity
    \item[(b)] Differentiability
    \item[(c)] Term-by-term differentiability
    \item[(d)] Behavior of endpoints.
\end{enumerate}
\subsection{Establishing Uniform Convergence}

\begin{tcolorbox}
\begin{thm}
    If a power series \( \sum_{ n= 0 }^{  \infty   } a_n x^n \) converges absolutely at a point \( x_0  \), then it converges uniformly on the closed interval \( [-c , c ] \), where \( c  = | x_0  |  \).
\end{thm}
\end{tcolorbox}

\begin{proof}
    Suppose a power series \( \sum_{ n=0  }^{  \infty   } a_n x^n  \) converges absolutely at a point \( x_0  \). Then the series \(  \sum_{ n=0  }^{  \infty  } | a_n x_0^n |  \) converges. Let \( x \in [-c , c ] \) where \( c = | x_0  |  \). We proceed via the Weierstrass M-test to show that \( \sum_{ n=0  }^{ \infty  } a_n x^n  \) converges uniformly. We observe that 
    \[  | a_n x^n  |  \leq a_n c^n = a_n | x_0  |^n = a_n | x_0^n | .  \]
    This tells us that 
    \[  \sum_{ n=0 }^{  \infty   } | a_n x^n | \leq \sum_{ n=0  }^{ \infty  } a_n | x_0^n  |.   \]
    Since the right side of the above inequality converges, we know that \( \sum_{ n=0  }^{ \infty  } a_n x^n  \) must converge uniformly on any \( x \in [-c ,c ]  \).
\end{proof}
A few remarks about this result:
\begin{enumerate}
    \item[(a)] Any \( x \in (-R, R ) \) is contained in the interior of a closed interval \( [-c , c ]  \subseteq (-R ,R )\).
    \item[(b)] If the interval above was open instead of closed, then the limit of the series above is necessarily continuous on this interval. 
\end{enumerate}

Some questions we can ask about this result are:
\begin{enumerate}
    \item[(a)] Can a power series converge at an endpoint of the interval of convergence? 
    \item[(b)] Does the behavior of the power series on an open interval necessarily imply that it will be convergent at \( x = R  \)? 
    \item[(c)] What happens when we \textit{conditionally} convergent power series?
\end{enumerate}

\subsection{Abel's Theorem}

\begin{tcolorbox}
    \begin{lem}[Abel's Lemma]
    Let \( b_n  \) satisfy \( b_1 \geq b_2 \geq b_3 \geq \dots \geq 0,  \) and  let \( \sum_{ n=1 }^{ \infty  } a_n  \) be a series for which the partial sums are bounded. In other words, assume there exists \( A > 0  \) such that 
    \[  | a_1 + a_2 + \dots + a_n  |  \leq A  \] for all \( n \in \N  \). Then, for all \( n \in \N  \), 
    \[  \Big| \sum_{ n=1 }^{ \infty  } a_n b_n  \Big|  \leq Ab_1. \]
    \end{lem}
\end{tcolorbox}

\begin{proof}
Let \( s_n = \sum_{ k=1 }^{ n  } a_k  \) be a bounded sequence of partial sums for the series 
\[  \sum_{ n=1  }^{  \infty  } a_n.  \] Hence, there exists some \( A > 0  \) such that \( | x_n  | \leq A  \). Using the Summation-by-parts formula, we have that 
\begin{align*}
    \Big| \sum_{ k=1 }^{ n } a_k b_k  \Big| &= \Big| s_n y_{n+1} + \sum_{ k=1 }^{ n } s_k (y_k - y_{k+1}) \Big|  \\
                                            &\leq | s_n | | y_{n+1} | + \Big| \sum_{ k=1 }^{ n } s_k (y_k - y_{k+1}) \Big| \\
                                            &\leq Ay_1 + \sum_{ k=1 }^{ n } A (y_k - y_{k+1}) \\
                                            &= Ay_1 + A(y_1 - y_{n+1}) \\
                                            &\leq Ay_1.
\end{align*}
\end{proof}

We can use this bound to prove the next theorem about proving convergence at one of the endpoints of an interval.

\begin{tcolorbox}
    \begin{thm}[Abel's Theorem]
        Let \( g(x) = \sum_{ n=0  }^{  \infty  } a_n x^n  \) be a power series that converges at the point \( x = R > 0  \). Then the series converges uniformly on the interval \( [0,R] \). A similar result holds if the series converges at \( x = -R  \).
    \end{thm}
\end{tcolorbox}

\begin{proof}
Let us rewrite \( g(x)  \) into the following form:
\[  g(x) = \sum_{ n=0  }^{  \infty  } a_n x^n = \sum_{ n=0  }^{ \infty  } (a_n R^n ) \Big( \frac{ x  }{ R  }  \Big)^n. \]
Let \( \epsilon > 0  \). We can show that the series above converges uniformly by showing that it satisfies the Cauchy Criterion. Since \( \sum_{ n=0  }^{ \infty  } a_n R^n  \) converges where \( R > 0  \), we know that there exists an \( N \in \N  \) such that for any \( n > m \geq N \), we have that
\[  \Big| \sum_{ k = m+1    }^{ n } a_n R^n  \Big| < \frac{ \epsilon  }{ 2  }. \]
Using Abel's lemma, we can fix any \( m \in \N  \) and use \( \epsilon / 2  \) as an upper bound. Furthermore, we utilize the fact that \(  (x / R )^{m+j}  \) is a monotone decreasing sequence of functions. Hence, we can write 
\begin{align*}
    \Big| \sum_{ k=m+1  }^{ n  } (a_k R^k ) \Big( \frac{ x  }{ R  }  \Big)^k \Big| &\leq \frac{ \epsilon  }{ 2  }  \Big( \frac{ x  }{ R  }  \Big)^{m+1} < \epsilon \\ 
\end{align*}
by Abel's Lemma. The same process goes for when \( x = -R  \).
\end{proof}

\subsection{The Success of Power Series}

We can summarize the two theorems above in the following theorem.

\begin{tcolorbox}
\begin{thm}
If a power series converges pointwise on the set \( A \subseteq \R  \), then it converges uniformly on any compact set \( K \subseteq A  \).
\end{thm}
\end{tcolorbox}

\begin{proof}
    A compact set \( K \subseteq A  \) contains both a maximum \( x_1  \) and minimum \( x_0  \). Since \( K \subseteq A  \), we know that \( x_0, x_1 \in A  \). Since \( g(x) = \sum_{ n=0  }^{  \infty  } a_n x^n  \) converges pointwise on \( A  \), we know that the series converges uniformly on \( [x_0, x_1 ] \) and hence also on \( K  \) by Abel's Theorem.
\end{proof}

We can utilize this fact about a power series converging on a compact set to show that the power series is continuous at every point in the compact set. To show differentiability, however, requires a slightly more complicated set of assumptions. In order to do this, we need to show that 
\[  \sum_{ n=0  }^{ \infty  } a_n x^n  \] is differentiable, and that we can differentiate each term in the infinite series given we know that 
\[  \sum_{ n=1  }^{  \infty  } na_n x^{n-1} \] converges uniformly.

\begin{tcolorbox}
\begin{thm}
    If \( \sum_{ n=0  }^{  \infty  } a_n x^n  \) converges for all \( x \in (-R , R ) \), then the differentiated series \( \sum_{ n=1  }^{  \infty  } na_n x^{n-1} \) converges at each \( x \in (-R , R ) \) as well. Consequently, the convergence is uniform on compact sets contained in \( (-R , R ) \).
\end{thm}
\end{tcolorbox}

\begin{proof}
Exercise 6.5.5.
\end{proof}

A couple things to note about this result.
\begin{enumerate}
    \item[(i)] It is possible to have a power series converge at \( x =R  \), but its differentiated series to diverge at this point. An example of a power series that satisfies this property is the series 
        \[  \sum_{ n=1  }^{  \infty  } \frac{ x^n  }{ n } \]
        at \( x = -1  \). 
    \item[(ii)] If we happen to have the differentiated series converge at the point \( x = R  \), then we can use Abel's Theorem to imply uniform convergence of the differentiated series on any compact set that contains \( R  \).
\end{enumerate}

\begin{tcolorbox}
\begin{thm}
Assume 
\[  f(x) = \sum_{ n=0 }^{ \infty  } a_n x^n  \] converges on an interval \( A \subseteq \R  \). The function \( f  \) is continuous on \( A  \) and differentiable on any open interval \( (-R , R ) \subseteq A \). The derivative is given by 
\[  f'(x) = \sum_{ n=1  }^{  \infty  } n a_n x^{n-1}. \]
Moreover, \( f  \) is infinitely differentiable on \( (-R ,R ) \), and the successive derivatives can be obtained via term-by-term differentiation of the appropriate series.
\end{thm}
\end{tcolorbox}

\begin{proof}
We can use Theorem 6.5.4 to explain why \( f  \) is continuous. By Theorem 6.5.5, we can use the Term-By-Term Differentiability Theorem to verify the formula \( f' \). Despite not having our differentiated power series diverge at the endpoints of our interval, the radius of convergence is not altered. We can use an induction argument to show that the power series can be differentiated an infinite number of times.
\end{proof}

\subsection{Exercises}

\subsubsection{Exercise 6.5.1} Consider the function \( g  \) defined by the power series 
\[  g(x) = x - \frac{ x^2  }{ 2  }  + \frac{ x^3  }{  3  }  - \frac{ x^4  }{ 4  }  + \frac{ x^5  }{  5  }  - \dotsb . \]
\begin{enumerate}
    \item[(a)] Is \( g  \) defined on \( (-1,1)?  \) Is it continuous on this set? Is \( g  \) defined on \( (-1,1]  \)? Is it continuous on this set? What happens on \( [-1,1]  \)? Can the power series for \( g(x) \) possibly converge for any other points \( | x  | > 1  \)? Explain.
        \begin{proof}[Solution]
        
        \end{proof}
    \item[(b)] For what values of \( x \) is \( g'(x)  \) defined? Find a formula for \( g' \).
        \begin{proof}[Solution]
        
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 6.5.3} Use the Weierstrass M-test to prove Theorem 6.5.2.
\begin{proof}
    Suppose a power series \( \sum_{ n=0  }^{  \infty   } a_n x^n  \) converges absolutely at a point \( x_0  \). Then the series \(  \sum_{ n=0  }^{  \infty  } | a_n x_0^n |  \) converges. Let \( x \in [-c , c ] \) where \( c = | x_0  |  \). We proceed via the Weierstrass M-test to show that \( \sum_{ n=0  }^{ \infty  } a_n x^n  \) converges uniformly. We observe that 
    \[  | a_n x^n  |  \leq a_n c^n = a_n | x_0  |^n = a_n | x_0^n | .  \]
    This tells us that 
    \[  \sum_{ n=0 }^{  \infty   } | a_n x^n | \leq \sum_{ n=0  }^{ \infty  } a_n | x_0^n  |.   \]
    Since the right side of the above inequality converges, we know that \( \sum_{ n=0  }^{ \infty  } a_n x^n  \) must converge uniformly on any \( x \in [-c ,c ]  \).
\end{proof}



\subsubsection{Exercise 6.5.3 (Term-by-term Antidifferentiation).} Assume \( f(x) = \sum_{ n=0  }^{  \infty  } a_n x^n  \) converges on \( (-R,R ) \). 
\begin{enumerate}
    \item[(a)] Show 
        \[  F(x) = \sum_{ n=0  }^{ \infty  } \frac{ a_n  }{  n + 1 } x^{n+1} \]
        is defined on \( (-R,R ) \), find a power series representation for \( g  \).
        \begin{proof}
            First we show that \( F(x)  \) converges uniformly on \( (-R,R ) \). Since \( \sum_{ n=0  }^{ \infty  } a_n x^n  \) on \( (-R ,R ) \), we know that the sequence of partial sums must be bounded by some \( M > 0  \). Let \( \epsilon > 0   \). Choose \( N \in \N  \) such that for any \(  n > m \geq N  \), we have that  
            \begin{align*}
                \Big| \sum_{ k = m +1  }^{  n } \frac{ a_k  }{  k +1  } x^{k+1} \Big| &= \Big| \sum_{ k= m+1  }^{ n } (a_k x^k) \Big( \frac{ x }{ k+1  }  \Big) \Big|  \\
                                                                                      &\leq M \Big( \frac{ x }{ m+2  }  \Big) \\
                                                                                      &< M \Big( \frac{ R  }{ m+2  }  \Big) \\
                                                                                      &< \epsilon
            \end{align*}
            by Abel's Lemma.
            Since the power series \( \sum_{ n=0  }^{ \infty  } \frac{ a_n  }{ n + 1  }  x^{n+1}  \) converges on \( (-R,R ) \), we know that \( F(x)  \) must be continuous on \( (-R ,R ) \) and differentiable by Theorem 6.5.6. Hence, we can differentiate according to the formula in Theorem 6.5.6 to write that \[  F'(x) = \sum_{ n=0  }^{ \infty  } a_n x^n = f(x). \]
        \end{proof}
    \item[(b)] Antiderivatives are not unique. If \( g  \) is an arbitrary function satisfying \( g'(x) = f(x)  \) on \( (-R,R ) \), find a power series representation for \( g  \).
        \begin{proof}[Solution]
        Suppose \( g  \) is an arbitrary function satisfying \( g'(x) = f(x)  \) on \( (-R ,R ) \); that is, 
        \[  g'(x) = \sum_{ n=0  }^{ \infty  } a_n x^n. \]
        Since \( f(x) = F'(x)  \) and the fact that Antiderivatives are not unique, we can write 
        \[  g(x) = c +  \sum_{ n=0  }^{ \infty  } \frac{ a_n  }{  n+1  } x^{n+1}\]
        where \( c  \) is some constant.
        \end{proof}
\end{enumerate} 


\subsubsection{Exercise 6.5.5} 
\begin{enumerate}
    \item[(a)] If \( s  \) satisfies \( 0 < s < 1  \), show \( ns^{n-1} \) is bounded for all \( n \geq 1  \).
        \begin{proof}
            Let \( x  \) satisfy \( 0 < s < 1  \). Let us proceed by inducting on \( n  \) where \( P(n) \) is statement that for every \( n \geq 1  \), we have that \( ns^{n-1}  \) is bounded. Let our base case be \( n = 1  \). Then using the fact that \( 0 < s < 1  \), we have that \(  0 < ns^{n-1} < 1 \). Now suppose \( P(n)  \) holds for every \( n \leq k -1  \). We want to show that \( P(n) \) holds for the \( kth \) case. Observe that 
            \begin{align*}
                (k-1)s^{k-2} &= \frac{ ks^k - s^k }{ s^2 }    \\
            \end{align*}
            which means that for any \( 0 < s < 1  \), we have 
            \[  ks^k \leq s^2(k -1) + s^k < (k-1) + 1 = k  .  \]
            Hence, we have that \( ns^{n-1}  \) is bounded for all \( n \geq 1  \).
            \[   \]
        \end{proof}
    \item[(b)] Given an arbitrary \( x \in (-R ,R ) \), pick \( t  \) to satisfy \( | x  |  < t < R  \). Use this start to construct a proof for Theorem 6.5.6.
        \begin{proof}
        Let \( x \in (-R ,R ) \) be arbitrary. Pick \( t \in (-R ,R ) \) such that \( | x  |  < t < R  \). By using this bound on \( t  \), we can write the following
        \[  | na_n x^{n-1}  | = | n x^{n-1} \cdot a_n  | \leq | n t^{n-1}  \cdot a_n |.\]
        From part (a), we know that \( 0 < s < 1  \) implies that \( n s^{n-1} \) is bounded. Hence, there exists \( M > 0  \) such that
        \[  n t^{n-1} = n s^{n-1} \Big( \frac{ t  }{ s  }  \Big)^{n-1} \leq M \Big( \frac{ t }{ s }  \Big)^{n-1}.  \]
        Using the Weierstrass M-test, we can see that the bound above forms the following series
        \[  \sum_{ n=1  }^{ \infty  } Ma_n \Big( \frac{ t }{ s  }  \Big)^{n-1} \tag{1}.\]
        Since \( \sum_{ n=0 }^{ \infty  } a_n x^n  \) converges on (-R, R ), we know that (1) must converge and thus 
        \[  \sum_{ n=0  }^{ \infty  } | na_n x^{n-1}  |  \] must also converge.
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 6.5.6} Previous work on geometric series (Example 2.7.5) justifies the formula 
\[  \frac{ 1 }{ 1 -x  } = 1 + x + x^2 + x^3 + x^4 + \dotsb, \text{~ for all } |  x  |  < 1. \]
Use the results about power series in this section to find values for \( \sum_{ n= 1  }^{ \infty  } n/ 2^n  \) and \( \sum_{ n=1  }^{ \infty  } n^2 / 2^n \). The discussion in Section 6.1 may be helpful.
\begin{proof}[Solution]

\end{proof}


\subsubsection{Exercise 6.5.7} Let \( \sum a_n x^n \) be a power series with \( a_n \neq 0  \) and assume 
\[  L = \lim_{ n \to \infty  } \Big| \frac{ a_{n+1} }{ a_{n} }  \Big|  \] exists.
\begin{enumerate}
    \item[(a)] Show that if \( L \neq 0  \), then the series converges for all \( x \in (- 1/ L, 1 / L ) \). (The advice in Exercise 2.7.9 may be helpful).
        \begin{proof}
        Since 
        \[  L = \lim_{ n \to \infty  } \Big| \frac{ a_{n+1} }{ a_{n} }  \Big|  \]
        where \( L \neq 0  \), we know that the sequence \( (a_{n+1}/ a_{n}) \) must be bounded by some \( M > 0  \); that is, \( | \frac{ a_{n+1} }{ a_n }   | \leq M  \). This implies that 
        \[  | a_{n+1}  |  \leq M | a_n  |. \]
        \end{proof}
        Now choose \( N \in \N  \) such that for any \(  n \geq N  \), we have that 
        \[  | a_N x^{N} | \leq  M | a_n | |x|^{n}. \tag{1}\]
        Since \( L \neq 0  \) and \( | x  |  < 1 / L  \), we must have that 
        \[  M | a_n | |x|^n < M a_n  \Big( \frac{ 1 }{ L  }  \Big)^{n} \tag{2}.  \]
        Since the right side of \( (2) \) forms the geometric series
        \[  \sum M a_n \Big( \frac{ 1 }{ L }  \Big)^n,\] we know that 
        \[  \sum a_n x^n  \] must converge absolutely by the Weierstrass M-test and hence it must converge uniformly for every \( x \in  (-1/L , 1 /L ) \).
    \item[(b)] Show that if \( L = 0  \), then the series converges for all \( x \in \R  \).
        \begin{proof}
        Let \( L = 0  \) and let \( x \in \R  \) be arbitrary. Since \( L < 1  \), we can  \( r' \in (L , 1 ) \) such that 
        \[  | a_n | \leq | a_N | (r')^n.\]
        Using the Weiertrass M-test, we know that \( r'x < 1  \) must imply that 
        \[  | a_n x^n  | \leq | a_N  | (r' x )^n \leq | a_N  | .\]
        Since \( L < 1  \), we know that the series 
        \[  \sum_{ N=1 }^{ \infty  } | a_N |   \] converges and hence the power series 
        \[  \sum_{ n=1  }^{ \infty  } a_n x^n \] must also converge for any \( x \in \R  \).
        
        \end{proof}
    \item[(c)] Show that \( (a)  \) and \( (b)  \) continue to hold if \( L  \) is replaced by the limit 
        \[  L' = \lim_{ n \to \infty  }  s_n \text{~ where ~} s_n = \sup \Big\{  \Big| \frac{ a_{k+1} }{ a_k }  \Big| : k \geq n  \Big\}. \]
        (General properties of the \textit{limit superior} are discussed in Exercise 2.4.7.)
        \begin{proof}
            Let \( L' \neq 0  \). Since \( s_n \to L'  \), we know that \( s_n  \) must be bounded. Hence, there exists some \( M > 0  \) such that \( | s_n | \leq M  \) for all \( n \in \N  \). Let \( x \in (-1/L', 1/L) \). Using the Weierstrass M-test, we can bound \( \frac{ a_{n+1} }{ a_n }  x^n  \) by its supremum (i.e the sequence \( s_n \)) such that 
            \begin{align*}
            \Big| \frac{ a_{n} }{ a_{N} } \cdot x^n  \Big| &\leq | s_n | | x |^n \\
                                                             &\leq M \Big( \frac{ 1 }{ L' }  \Big)^n. \\
            \end{align*}
            Since the last inequality forms the following geometric series
            \[  \sum M \Big( \frac{ 1 }{ L' }  \Big)^n. \]
            Hence, the power series 
            \[  \sum a_n x^n  \] must converge on \( (-1/L', 1 / L' ) \).
        To show that the power series converges for all \( x \in \R  \), we can employ the same process in part (b) since \( \lim | a_{n+1} / a_{n} | = \lim s_n   \).
        \end{proof}
\end{enumerate}


\subsubsection{Exercise 6.5.8} 
\begin{enumerate}
    \item[(a)] Show that power series representation are unique. If we have 
        \[  \sum_{ n=0 }^{ \infty  } a_n x^n = \sum_{ n=0 }^{ \infty  } b_n x^n \]
        for all \( x  \) in an interval \( (-R ,R ) \), prove that \( a_n = b_n  \) for all \( n \in \N  \).
        \begin{proof}
       Since 
       \[  \sum_{ n=0  }^{ \infty  } a_n x^n = \sum_{ n=0 }^{ \infty  } b_n x^n  \tag{1} \]
       for all \( x \in (-R ,R ) \), we know that the series above must also be differentiable and follow the formula below
       \[  \sum_{ n=1 }^{ \infty  } na_n x^{n-1} = \sum_{ n=1 }^{ \infty  } nb_n x^{n-1} \tag{2} \] by Theorem 6.5.6. If \( n = 0  \), we see that (1) implies 
       \[  a_0 = b_0. \] Now suppose \( n = 1  \), then (2) implies that 
       \[  a_1 = b_1. \] An inductive argument can be used to show that for every \( n \geq 0  \) with \( n  \) corresponding to the nth derivative of \( f  \), we can show that \( a_n = b_n  \) for all \( n \geq 0  \).
        \end{proof}
    \item[(b)] Let \( f(x) = \sum_{ n=0  }^{ \infty  } a_n x^n  \) converge on \( (-R ,R ) \), and assume \( f'(x) = f(x)  \) for all \( x \in (-R ,R ) \) and \( f(0) = 1  \). Deduce the values of \( a_n \).
        \begin{proof}[Solution]
        Assume \( f'(x) = f(x)  \). Since 
        \[  f(x) = \sum_{ n=0  }^{ \infty  } a_n x^n  \] converges for all \( x \in (-R ,R ) \), we know that \( f  \) must also be differentiable and satisfy the following formula 
        \[  f'(x) = \sum_{ n=1  }^{ \infty  } na_n x^{n-1}. \]  Furthermore, we know that \( f  \) is \textit{infinitely differentiable}. Since \( f'(x) = f(x)  \) we can change indices of the power series representation of \( f'(x)  \) so that we may use the proposition from part (a) to have 
        \[  \sum_{ n=0  }^{ \infty  } a_n x^n = \sum_{ n=0 }^{ \infty  } (n+1) a_{n+1} x^n \]
       implies \( a_n = b_n  \) for all \(  n \geq 0  \) where 
       \[  b_n = (n+1)a_{n+1}. \]
       Note that \( f(0) = 1  \) which implies that \( a_0 = a_1  \) since \( a_n = (n +1) a_{n+1} \) for all \( n \geq 0  \). Since each \( a_n  \) is defined recursively, we know that for \( n =2  \) that 
       \[  a_2 = \frac{ a_{1} }{ 2  } = \frac{ 1 }{ 1 \cdot 2 }  . \]
       For \( n = 3  \), we apply the same reasoning to get 
       \[  a_{3} = \frac{ a_{2} }{ 3  } = \frac{ 1 }{ 1 \cdot 2 \cdot 3  }.  \] Likewise, \( n =4  \) gives us 
       \[  a_4 = \frac{ a_{3} }{ 4  } = \frac{ 1 }{  1 \cdot 2 \cdot 3 \cdot 4  }.\] Continuing the pattern for \( n \geq 0  \), we arrive at the form of \( a_n  \) which is just 
       \[  a_n = \frac{ f(0) }{ n! }. \]
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 6.5.10}  Let \( g(x) = \sum_{ n=0  }^{ \infty  } b_n x^n  \) converge on \( (-R ,R ) \), and assume \( (x_n) \to 0  \) with \( x_n \neq 0  \). If \( g(x_n) = 0  \) for all \( n \in \N  \), show that \( g(x)  \) must be identically zero on all of \( (-R ,R ) \).
\begin{proof}
Since the series 
\[  g(x) = \sum_{ n=0 }^{ \infty  } b_n x^n  \] converges on \( (-R ,R ) \); that is, uniformly for any compact set \( K \subseteq (-R ,R ) \) and the fact that \( g_n(x) = b_n x^n  \) is continuous for all \( n \geq 0  \), we know by the Continuity theorem that \( g(x)  \) must be a continuous function. Since \( (x_n) \to 0 \) with \( x_n \neq 0  \) and the fact that \( g(x_n) = 0  \) for all \( n \in \N  \), we know that \( g(x_n) \to 0  \). But this is equivalent to saying that 
\[  \lim_{ x \to 0  } g(x) = 0 = g(0).\]
But we need to show that \( g(x) = 0  \) for all \( x \in (-R ,R ) \). Since \( g(x)  \) converges on \( (-R ,R ) \), we know that \( g(x) \) must be differentiable and must follow 
\[  g'(x) = \sum_{ n=1  }^{ \infty  } n b_n x^{n-1}. \]
Since \( g(x_n) = 0  \) for all \( n \in \N  \), it must also follow that the differentiability of \( g  \) implies that \( g'(x_n) = g(x_n) = 0  \); that is, for every \( i \geq 0  \), we must have 
\[ g^{i} (x_{n})  = g^{i+1} (x_{n}) = 0  \] where 
\begin{align*}
    g^{i}(x_n) &= \sum_{ k = i  }^{ \infty  } (k - i )! b_k (x_n)^{k - i}  \\
    g^{i+1}(x_n) &= \sum_{ k= i +1  }^{ \infty  } (k - (i +1))! b_k (x_n)^{k - (i+1)} 
\end{align*} By part (a) of Exercise 6.5.8, we must have 
\[  b_i = b_{i+1} \]  for all \( i \geq 0  \). Hence, for every \( b_n  \) must be zero showing that for any \( x \in (-R ,R ) \) that \( g(x) = 0  \).
\end{proof}












