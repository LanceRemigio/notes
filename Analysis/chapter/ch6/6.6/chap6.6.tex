\section{Taylor Series}

In this section, our goal is to develop some theory for infinitely differentiable functions such as 
    \[  \sin(x) = a_0 + a_{0} x  + a_{2} x^{2} + a_{3} x^{3} + a_{4} x^{4} + \dotsb \]
so that we can find suitable coefficients \( a_{n} \) given some nonzero values of \( x  \).

\subsection{Manipulating Series}
In section 6.1, we encountered 
\[  \frac{ 1 }{ 1-x  } = 1 + x + x^{2} + x^{3} + x^{4} + \dotsb, \text{ for all } | x  | < 1. \tag{1} \]
We can apply Theorem 6.5.7 to arrive at the following series representation
\[  \frac{ 1 }{ (1-x)^2  } = 1 + 2x + 3 x^{2} + 4 x^{3} + 5 x^{4} + \dotsb, \text{ for all } | x  | < 1.   \]

We can use term-by-term antidifferentiation (proven in Exercise 6.5.4) to arrive at the original function. An example of this is 
\[  \frac{ 1 }{ 1 + x^2  }  = 1 - x^{2} + x^{4 } - x^{6 } + x^{8 } - \dotsb, \text{ for all } | x  | < 1. \]
Antidifferentiating each term of the power series above, we arrive at 
\[  \arctan(x) = x - \frac{ 1 }{ 3 }  x^{3} + \frac{ 1 }{ 5 } x^{5} - \frac{ 1 }{ 7 } x^{7} + \dotsb,  \]
for all \( x \in (-1,1) \). Note that the power series representation above is also valid when \( x = \pm 1  \). The same methods can be used to find the series representations for functions such as \( \ln ( 1 + x ) \) and \( x  / (1 + x^2 )^2 \).

\subsection{Taylor's Formula for the Coefficients} 

Given an infinitely differentiable function \( f  \) defined on some interval centered at zero, if we assume that a function \( f  \) has a power series expansion, we can be able to find the every coefficient.

\begin{tcolorbox}
    \begin{thm}[Taylor's Formula]
    Let 
    \[  f(x) = a_0 + a_{1}x + a_{2} x^{2} + a_{3} x^{3} + \dotsb \]
    be defined on some nontrivial interval centered at zero. Then, 
    \[ a_n = \frac{ f^{(n)}(0) }{ n! }. \]
    \end{thm}
\end{tcolorbox}

\begin{proof}
Exercise 6.6.3
\end{proof}

We can use our new formula to derive the \textit{Taylor Series} for \( \sin(x)  \). To get \( a_{0} \), all we have to do is let \( x = 0  \) into the formula above so that 
\[  a_{0} = \sin(0) = 0. \] Then for \( n =1  \), we get 
\[  a_{1} = \frac{ f^{(1)}(0) }{ 1! } = \cos(0) = 1.  \]
Then likewise we have \( a_{1} = \cos(0) = 1  \), \( a_{2} = - \sin(0) / 2! = 0  \), and then \( a_{3} = - \cos(0) / 3! = -1/ 3! \) and so on. Hence, we are left with the following series
\[  x - \frac{ x^{3} }{ 3! }  + \frac{ x^{5} }{ 5! }  - \frac{ x^{7} }{ 7! }  + \dotsb .  \]
Note that this is the power series representation of \( \sin(x)  \). Generally, if a function \( f(x)  \) can be expressed as a power series
\[  f(x) = \sum_{ n=0 }^{ \infty  } a_{n} x^{n} \] then we are guaranteed to have 
\[  a_n = \frac{ f^{(n)}(0) }{ n! } \]
if the power series is centered at \( x = 0  \). But is the converse true? 

A few questions: 
     If we have 
        \[  a_n = \frac{ f^{(n)}(0) }{ n! }  \] for all \( n \geq 0  \) does the series
        \[  \sum_{ n=0 }^{ \infty  } a_n x^{n} \] converge to \( f(x)  \) on some nontrivial set of points?  Does it even converge at all? The limit different from \( f(x)  \)? Our question as of now is whether or not the following sequence of partial sums
        \[  S_{N}(x) = a_{0} + a_{1} x  + a_{2} x^{2} + \dotsb + a_{N} x^{N }  \] for the Taylor series expansion of \( f(x)  \) actually converges to \( f(x)  \); that is, 
        \[ \lim_{ N \to \infty  }  S_{N}(x) = f(x) \]
        for some values of \( x  \) besides zero.

    \subsection{Lagrange's Remainder Theorem}

The idea of the Remainder theorem is to express the error between the function \( f  \) and partial sum \( S_N  \) in terms 
\[  E_N (x) = f(x) - S_N(x).\]

\begin{tcolorbox}
    \begin{thm}[Lagrange's Remainder Theorem]
    Let \( f  \) be differentiable \( N + 1  \) times on \( (-R ,R ) \), define \( a_n = f^{n}(0) / n!  \) for \( n = 0, 1 , \dots , N  \), and let 
    \[  S_N(x) = a_0 + a_{1} x + a_{2}x^{2} + \dotsb + a_{N}x^{N}. \]
    Given \( x \neq 0  \) in \( (-R ,R ) \), there exists a point \( c  \) satisfying \( | c  |  < | x  |  \) where the error function \( E_N(x) = f(x) - S_N(x)  \) satisfies 
    \[  E_N(x) = \frac{ f^{(N+1)}(c) }{ (N+1)!  } x^{N+1}. \]
    \end{thm}
\end{tcolorbox}

Components that make up the theorem:
\begin{enumerate}
    \item[(i)] Showing that \( S_N(x) \to  f(x)  \) is equivalent to showing that \( E_N(x) \to 0  \).
    \item[(ii)] The factorial \( (N+1)! \) on the denominator helps to make the error small as \( N  \) tends to infinity. 
    \item[(iii)] The \( x^{N+1}  \) term on the numerator has the potential to grow depending on how far \(  x  \) is chosen from the origin.
    \item[(iv)] The second term on the numerator \( f^{(N+1)} (c)  \) can be handled by introducing some upper bound either from a compact set or based on the behavior of \( f  \).
\end{enumerate}

\begin{ex}
Consider the Taylor series for \( \sin (x)  \) from earlier. We can ask how well does our sequence of partial sums approximate \( \sin(x)  \) when \( N=5  \); that is, how well does  
\[  S_{5}(x) = x - \frac{ 1 }{ 3! } x^{3} + \frac{ 1 }{ 5! } x^{5} \]
approximate \( \sin(x)  \) on the interval \( [-2, 2 ] \). By using Lagrange's Remainder Theorem we can assert that 
\[  E_5(x) = \sin(x) - S_5(x) = \frac{ -\sin(c)  }{ 6! } x^{6} \]
for some \( c \in (-| x |, | x | ) \). Since we don't know the value of \( c  \), we can still use the fact that \( | \sin(c)  | \leq 1 \) to assert 
\[  | E_5(x) | = \Big| \frac{ - \sin(c)  }{ 6! } x^{6} \Big| \leq \frac{ 2^{6} }{ 6! } \tag{\( x \in [-2,2] \)}.  \]
We can show that \( S_N(x)  \) converges uniformly to \( \sin(x)  \) on \( [-2,2] \) by observing that \( | f^{(N+1)}(c) | \leq 1  \). Thus, 
\[  | E_N(x)  | = \Big| \frac{ f^{(N+1)}(c) }{ (N+1)! } x^{N+1} \Big| \leq \frac{ 1 }{ (N+1)! }  2^{N+1} \] for \( x \in [-2,2] \). We know that factorials grow faster than exponentials. Hence, we know that \( E_N(x) \to 0  \) on \( [-2,2] \).
\end{ex}

\begin{proof}[Proof of Lagrange's Remainder Theorem]
    Note that the Taylor coefficients are chosen so that the function \( f  \) and the polynomial \( S_N  \) have the same derivatives at \( 0  \), at least up to through the \( N \)th derivative, after which \( S_N  \) becomes the zero function. That is, we have \( f^{(n)}(0) = S_N^{(n)}(0) \) for all \( 0 \leq n \leq N  \), which implies the error function \( E_{N}(x) = f(x) - S_{N}(x)  \) satisfies 
    \[  E_N^{(n)}(0) = 0  \] for all \( n = 0,1,2, \dots, N .  \)
    Our goal is to use the Generalized Mean Value Theorem from Chapter 5. To simplify our notation, let us assume \( x > 0  \) and apply the theorem to the error function \( E_{N}(x)  \) and the polynomial \( x^{N+1} \) on the interval \( [0,x] \). Thus, there exists \( x_1 \in (0,x ) \) such that 
    \[  \frac{ E_{N}(x) }{ x^{N+1}  } = \frac{ E'_N(x_1) }{ (N+1) x_{1}^N  }. \]
    Now apply the Generalized Mean Value Theorem again to the functions \( E'_{N }(x)  \) and \( (N+1) x^N  \) on the interval \( [0,x_{1}] \) to get that there exists a point \( x_2 \in (0,x_1) \) where
    \[  \frac{ E_{N}(x) }{ x^{N+1}  } = \frac{ E'_N(x_1) }{ (N+1) x_{1}^N  } = \frac{ E_N"(x_2) }{ (N+1)Nx_2^{N-1} } . \]
    Continuing in this manner we find 
    \[  \frac{ E_{N}(x)  }{ x^{N+1} }  =  \frac{ E_{N}^{N+1} (x_{N+1}) }{ (N+1)! x_{N+1}^{N-N} } =  \frac{ E_{N}^{N+1} (x_{N+1}) }{ (N+1)! }  \] 
    where \( x_{N+1} \in (0, x_{N}) \subseteq \dots \subseteq (0,x) \). Now set \( c = x_{N+1} \). Since \( S_{N}^{N+1}(x) = 0  \), we know that \( E_{N}^{N+1}(x) = f^{(N+1)}(x) \) and it follows that 
    \[ E_{N}(x) = \frac{ f^{(N+1)}(c) }{ (N+1)! } x^{N+1}  \] as desired.
\end{proof} 

\subsection{Taylor Series Centered at \( a \neq 0  \).}

The series expansion of a function need not be centered only at \( a = 0  \). If the function \( f \) is defined at any other neighborhood of \( a \neq 0  \) and infinitely differentiable at \( a  \), then the Taylor series expansion takes the following form where 
\[  \sum_{ n=0 }^{ \infty  } c_{n} (x-a)^{n} \text{ where } c_{n} = \frac{ f^{(n)}(a) }{ n! }. \]
Setting up our Error function \( E_{N}(x) = f(x) - S_{N}(x)  \) as before, we can reformulate Lagrange's Remainder Theorem in the following fashion where there exists some value \( c \in (a, x) \) such that 
\[  E_N(x) = \frac{ f^{(N+1)}(c) }{ (N+1)! } (x-a)^{N+1}. \]

\subsection{A Counterexample}

The Lagrange's Remainder Theorem is useful in determining how well behaved the sums of the Taylor series \( S_N(x) \) approximate \( f(x) \), but it leaves the to question whether or not the sequence of partial sums actually converges to \( f(x) \). Let 
\[  g(x) = 
\begin{cases}
    e^{-1/x^2} &\text{ for } x \neq 0  \\
    0 &\text{ for } x = 0. 
\end{cases} \]
We can compute the Taylor coefficients for this function. It can be found that \( a_{0 }= g(0) = 0  \). For take the derivative of \( g(x)  \) at \( x = 0  \)  and get 
\[  a_1 = g'(0) = \lim_{ x \to 0 }  \frac{ g(x) - g(0)  }{ x - 0  } = \lim_{ x \to 0 }  \frac{ e^{-1/x^2} }{ x  } = \lim_{ x \to 0 }  \frac{ 1 /x  }{ e^{1/x^2} }   \] where taking the limit produces an \( \infty / \infty \) situation that calls for L'Hopital's rule to be applied as \( x \to 0  \). Hence, we have that 
\[  a_{1} = \lim_{ x \to 0 }  \frac{ -1/x^2  }{  e^{1/x^2} } (-2/x^3) = \lim_{ x \to 0  } \frac{ x }{ 2 e^{1/x^2} } = 0.\]
Since \( a_1 = 0  \), we know that \( g  \) must be flat at the origin. Furthermore, we find that \( g^{(n)}(0) = 0  \) for all \( n \in \N  \).
However, there is a caveat to this conclusion. We have a function that is infinitely differentiable whose Taylor series expansion converges uniformly to \( 0  \) function but \( g(x) \neq 0  \) everywhere except for \( x = 0  \). This means our convergence does not lead to \( g(x)  \) but leads to something else all together. This unfortunately tells us that not all infinitely differentiable function can be represented in terms of its Taylor series.

\subsection{Exercises}

\subsubsection{Exercise 6.6.1} The derivation in Example 6.6.1 shows the Taylor series for \( \arctan(x)  \) is valid for all \(  x \in (-1,1) \). Notice, however, that the series also converges when \( x =1  \). Assuming that \( \arctan(x)  \) is continuous, explain why the value of the series at \( x = 1  \) must necessarily be \( \arctan(1)  \). What interesting identity do we get in this case?
\begin{proof}[Solution]
We know the series equals the value of \( \arctan(1) \) at \( x = 1  \) because we know that the term-by-term antidifferentiation of the series
\[  \frac{ 1 }{ 1 + x^2  } = 1 - x^{2} + x^{4} - x^{6} + x^{8} - \dotsb  \tag{1}\]
that converges for all \( x \in (-1,1)  \) produces the series 
\[  \arctan(x) = x - \frac{ x^{3} }{ 3  } + \frac{ x^{5} }{ 5 } - \frac{ x^{7} }{ 7 } + \dotsb .  \tag{2}\]
that also converges for \( x \in (-1 ,1 ) \). Since we have convergence of (2) for all \( x \in (-1,1) \) and the fact that each term in the series representation of (2) is continuous shows that at \( x = 1  \), we must have equality of the series representation to \( \arctan(1)  \). This leads us to the interesting identity that 
\[  \arctan(1) = \frac{ \pi  }{ 4 }  = 1 - \frac{ 1 }{ 3 } + \frac{ 1 }{ 5 }  - \frac{ 1 }{ 7 }  + \frac{ 1 }{ 9 } - \dotsb . \]
\end{proof}

\subsubsection{Exercise 6.6.2} Starting from one of the previously generated series in this section, use manipulations similar to those in Example 6.6.1 to find Taylor series representations for each of the following functions. For precisely what values of \( x \) is each series representation valid? 
\begin{enumerate}
    \item[(a)] \( x \cos(x^2)  \)
        \begin{proof}[Solution]
        For this problem, we will use the \( \frac{ d }{ dx }  \) notation to compute our derivatives. We know that 
        \[  \sin (x) = \sum_{ n=0 }^{ \infty  } \frac{ (-1)^n x^{2n+1} }{ (2n+1)! }.\]
        Since \( \sin(x)  \) is an infinitely differentiable function, we can take the derivative of its Taylor expansion. Hence, we have 
        \begin{align*}
            \frac{d  }{d x } [ \sin(x) ] &= \frac{d  }{d x } \Big[ \sum_{ n=0 }^{ \infty  } \frac{ (-1)^{n}  }{ (2n+1)! } x^{2n+1} \Big] \\
                                         &= \sum_{ n=0  }^{ \infty  } \frac{d  }{d x }  \Big[  \frac{ (-1)^{n}  }{ (2n+1)! } x^{2n+1}\Big] \\
                                         &= \sum_{ n=0  }^{ \infty  } \frac{ (-1)^n }{ (2n)! }  x^{2n}.
        \end{align*}
        This means that 
        \[  \cos(x) = \sum_{ n=0  }^{ \infty  } \frac{ (-1)^n  }{ (2n)! } x^{2n}. \] Substituting \( x = x^2  \) and multiplying by \( x  \) gives us 
        \[  x \cos(x^2) = \sum_{ n=0  }^{ \infty  } \frac{ (-1)^{n} }{ (2n)! } x^{4n+1} \]
        which holds for any \( x \in (-R ,R ) \) where \( R > 0  \).
        \end{proof}
    \item[(b)] \( x / (1+ 4x^2)^2 \)
        \begin{proof}[Solution]
        Take the function \( f(x) = \frac{ 1 }{ 1 - x  }  \) which has the following Taylor Expansion 
        \[  \frac{ 1 }{ 1 - x  } = \sum_{ n=0  }^{ \infty  } x^n  \] which is defined for all \( | x  |  < 1  \). Taking the derivative of \( f(x)  \) we get 
        \[  \frac{\text{d}  }{\text{d} x }  \Big[ \frac{ 1 }{ 1 - x  }    \Big] =  \frac{ 1   }{ (1 - x )^2  }   \]
        which has a Taylor series expansion of 
        \[  \frac{ 1 }{ (1-x)^2 } = \sum_{ n=1 }^{ \infty  } n x^{n-1} .  \]
        Letting \( x = -4x^2  \) and multiplying by \( x  \), we arrive at the following Taylor series expansion 
        \[  \frac{ x  }{ (1 + 4x^2 )^2  } = \sum_{ n=1  }^{ \infty  } (-4)^{n-1} n  x^{2n - 1 }  \]
        which holds for all \( x \in (-R ,R ) \).
        \end{proof}
    \item[(c)] \( \text{log}(1+x^2) \)
        \begin{proof}[Solution]
            Our goal is to use the Term-by-term Anti-differentiation to write a Taylor series expansion for  \(  \text{log}(1+x^2) \). Set \( F(x) = \text{log}(1-x) \) and \( f(x) = \frac{ 1 }{ 1 - x  }  \). Since the series expansion for \( f(x)  \); that is, 
            \[  f(x) =\sum_{ n = 0  }^{ \infty  } x^n \] is defined for any \( | x  | < 1  \), we have that 
            \[  F(x) = \sum_{ n=0 }^{ \infty  } \frac{ 1 }{ n+1  } x^{n+1} \]
            which satisfies  \( F'(x) = f(x)  \). Letting \( x = - x^2  \) we get that 
            \[  \text{log}(1 + x^2 ) =  \sum_{ n=0  }^{ \infty  } \frac{ (-1)^{n+1}  }{ n+1  } x^{2n+2}.\]
        \end{proof}
\end{enumerate}






\subsubsection{Exercise 6.6.3} Derive the formula for the Taylor Coefficients given in Theorem 6.6.2.
\begin{proof}
    Suppose \( f(x) = \sum_{ n=0 }^{ \infty  } a_n x^n  \) converges for all \( x \in (-R ,R ) \) Since \( f  \) is infinitely differentiable by Theorem 6.5.7, we can take derivatives of \( f  \) where \( f^{n}(0)= n! a_n    \) implies that 
    \[  a_n = \frac{ f^{(n)}(0) }{ n! }. \]
    \[   \]
\end{proof}

\subsubsection{Exercise 6.6.4} Explain how Lagrange's Remainder Theorem can be modified to prove 
\[  1 - \frac{ 1 }{ 2 } + \frac{ 1 }{ 3 } - \frac{ 1 }{ 4 }  + \frac{ 1 }{ 5 } - \frac{ 1 }{ 6 } + \dotsb = \text{log}(2). \]
\begin{proof}[Solution]

\end{proof}

\subsubsection{Exercise 6.6.5} 
\begin{enumerate}
    \item[(a)] Generate the Taylor coefficients for the exponential function \( f(x) = e^{x} \), and then prove that the corresponding Taylor series converges uniformly to \( e^{x} \) on any interval of the form \( [-R,R] \).
        \begin{proof}
        To generate the Taylor coefficients for \( f(x) = e^{x} \) we can just use the formula given to us via Theorem 6.6.2 and the fact that \( f^{(n)}(0) = f^{(n+1)}(0) \) for all \( n \geq 0  \) where \( f^{(n)}(0) = e^{0}= 1  \), to write 
        \[  a_n = \frac{ f(0)  }{ n! } = \frac{ 1 }{ n! }. \]
        Hence, we can define the following power series 
        \[  S_N(x) = \sum_{ N=0  }^{ \infty  } \frac{ x^N  }{ N! }. \]
        To show that \( S_n(x) \to f(x)  \) where \( f(x) = e^{x} \), we will use Lagrange's Remainder theorem. Given an \(  x \in (-R ,R ) \) non-zero, suppose there exists a point \( c  \) satisfying \( | c  | < | x  |  \) such that \( E_N(x) = e^{x} - S_{N}(x)   \) satisfies  
        \[  E_{N}(x) =  \frac{ f^{(N+1)}(c) x^{N+1} }{ (N+1)! }  =  \frac{ e^c x^{N+1} }{ (N+1)! } .   \]
        Since \( x \in (-R ,R ) \), we can produce the following bound 
        \[  | E_N(x)  | = \Big| \frac{ e^c x^{N+1}  }{ (N+1)!  }  \Big| \leq \frac{ e^{c} R^{N+1} }{ (N+1)! }.   \]
        Since the term on the right side converges to zero uniformly on \( (-R ,R ) \), we know that \( E_N(x) \to 0  \) which means that \(  S_N(x) \to e^{x} \) uniformly on \( (-R ,R ) \).
        \end{proof}
    \item[(b)] Verify the formula \( f'(x) = e^{x} \).
        \begin{proof}[Solution]
            If we take the Taylor Expansion of \( e^{x} \) which is defined for all \( x \in [-R ,R ] \), we differentiate 
            \[  e^{x} = \sum_{ n=0  }^{ \infty  } \frac{ x^{n} }{ n! }  \] via Theorem 6.5.7 to get the following series representation 
            \[  f'(x) = \sum_{ n=1 }^{ \infty  } \frac{ n x^{n-1} }{ n! } =   \sum_{ n=1 }^{ \infty  } \frac{ x^{n-1} }{ (n-1)! }. \]
            We can reorder our indices to get 
            \[  f'(x) = \sum_{ n=0  }^{ \infty  } \frac{ x^{n} }{ n! } = e^{x}. \]
        \end{proof}
    \item[(c)] Use a substitution to generate the series for \( e^{-x} \), and then informally calculate \( e^{x} \cdot e^{-x} \) by multiplying together the two series and collecting common powers of \( x \).
        \begin{proof}[Solution]
        To generate the series for \( e^{-x} \), let \( x = -x  \). Then 
        \[  f(-x) = e^{-x} = \sum_{ n=0  }^{ \infty  } \frac{ (-1)^{n}  }{ n! } x^{n} \]
        To attain the Taylor expansion of \( e^{x} \cdot e^{-x } \) we can multiply the two series together. Since we are just collecting powers of \( x  \), we can try and form a formula for the summation of the coefficients of the Taylor expansions of \( e^{x} \) and \( e^{-x } \). By using the formula from section 2.7 where 
        \[  \sum_{ i=0 }^{ n } d_k \]
        with 
        \[  d_n = \sum_{ i=0 }^{ n } a_{i} b_{i -n}. \]
        Then we have 
        \begin{align*}
            e^{x} \cdot e^{-x} &=  \Big( \sum_{ n=0 }^{ \infty  } \frac{ x^{n} }{ n! }  \Big) \Big( \sum_{ m=0 }^{ \infty  } \frac{ (-1)^{m} x^{m} }{ m! }  \Big)  \\
                               &= \sum_{ n=0 }^{ \infty  } \Big( \sum_{ i=0 }^{ n } \frac{ (-1)^{n-i } }{ i! (n-i)! }  \Big) x^n.
        \end{align*}
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 6.6.6} Review the proof that \( g'(0) = 0 \) for the function 
\[  g(x) = 
\begin{cases}
     e^{-1/x^2} &\text{ for } x \neq 0 \\
    0 &\text{ for } x = 0
\end{cases} \]
introduced at the end of this section.
\begin{enumerate}
    \item[(a)] Compute \( g'(x)  \) for \( x \neq 0  \). Then use the definition of the derivative to find \( g"(0) \).
        \begin{proof}[Solution]
        Since \( x \neq 0  \) we can use the Chain Rule to get 
        \[  g'(x) = \frac{ 2 }{ x^3  } e^{-1/x^2 }.\] Next, we compute \( g"(0)  \) using the definition of the derivative. Then we have 
        \begin{align*}
            g"(0) &= \lim_{ x \to 0  } \frac{ g'(x) - g'(0)  }{ x - 0  }  \\
                  &= \lim_{ x \to 0 }  \frac{ 2 / x^{4 } }{  e^{1 / x^{2}} } \\
        \end{align*}
        Since we have a limit that produces an \( \infty / \infty  \) situation, we can use L'hopital to get 
        \begin{align*}
            \lim_{ x \to 0  } \frac{ 4 }{ x^2 e^{1/x^{2}} } .  \\
        \end{align*}
        But this in itself causes another \( \infty / \infty  \) situation as \( x \to 0 \). Hence, we can use L'Hopital's rule again to get 
        \[  \lim_{ x \to 0 }  \frac{ 4 }{ e^{1/x^2 } } \]
        which goes to zero as \( x \to 0  \). Hence, we have that \( g"(0) = 0  \).
        \end{proof}
    \item[(b)] Compute \( g"(x)  \) and \( g^{(3)}(x)  \) for \( x \neq 0  \). Use these observations and invent whatever notation is needed to give a general description for the \( n \)th derivative \( g^{(n)}(x)  \) at points different from zero.
        \begin{proof}[Solution]
        Since we have \( g^{(1)}(x) = 2 e^{-1 /x^2 } / x^{3} \), we can compute \( g^{(2)}(x)  \) for \( x \neq 0  \) using our differentiation rules. Hence, we have
        \begin{align*}
            g^{(2)}(x) &= \frac{ 2 e^{-1/x^{2}} }{ x^{6} }  ( 2 - 3x^2  ), \\
            g^{(3)}(x) &= \frac{ 2 e^{-1/x^{2}} }{ x^{9} } ( 4 - 18 x^{2} + 12x^{4} ).
        \end{align*}
        For our \( n \)th derivative, we have that for \( x \neq 0  \)
        \[  g^{(n)}(x) = 
        \begin{cases}
            \frac{ 2 e^{-1/x^{2}} }{ x^{3} } &\text{ if } n=1 \\
            \frac{ 2 e^{-1/x^2 } }{ x^{3n} }  \sum_{ i=0 }^{ n  } (-1)^{i} a_i x^{2i} &\text{ if } n > 1 
        \end{cases} \]
        where \( a_n \in \R  \).
        \end{proof}
    \item[(c)] Construct a general statement argument for why \( g^{(n)}(0) = 0  \) for all \( n \in \N  \).
        \begin{proof}[Solution]
        We can use induction to prove that \( g^{(n)}(0) = g^{(n+1)}(0) = 0  \) for all \( n \geq 1  \) to show that \( g^{(n)}(0) = 0  \) for all \( n \in \N  \). We can compute the \( n \)th derivative of \( g  \) by using the description outlined in part (b). Then use the definition of the  derivative to produce a \( \infty  / \infty   \) so that we may use L'Hopital to show that \( g^{(n)}(0) = g^{(n+1)}(0) = 0 \). Hence, \( g^{(n)}(0) = 0  \) for all \( n \in \N   \).
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 6.6.8} Here is a weaker form of Lagrange's Remainder Theorem whose proof is arguably more illuminating than the one for the stronger result.
\begin{enumerate}
    \item[(a)] First establish a lemma: If \( g  \) and \( h  \) are differentiable on \( [0,x] \) with \( g(0) = h(0)  \) and \( g'(t) \leq h'(t) \) for all \( t \in [0,x] \), then \( g(t) \leq h(t)  \) for all \( t \in [0,x]  \).
        \begin{proof}
            Let \( t \in [0,x] \). Since \( g \) and \( h  \) are differentiable on \( [0,x] \), we can use the Mean Value Theorem so that choosing \( c \in (0,t) \) implies 
            \begin{align*}
                g'(c)  &= \frac{ g(t) - g(0) }{ t - 0  },   \\
                h'(c) &= \frac{ h(t) - h(0)  }{ t - 0 }.
            \end{align*}
            Since \( c \in [0,x]  \), we have \( g'(c) \leq h'(c)  \). But we have \( g(0) = h(0) \) so 
            \[  g'(c) \leq h'(c) \iff \frac{ g(t) - g(0)  }{ t  } \leq \frac{ h(t) - h(0)  }{ t  } . \]
            Multiplying through by \( t  \) and adding \( g(0)  \) from both sides (keeping in mind that \( g(0) = h(0) \)), we arrive at \( g(t) \leq h(t) \).
        \end{proof}
    \item[(b)] Let \( f  \), \( S_N  \), and \( E_N  \) be as Theorem 6.6.3, and take \( x \in (0,R ) \). If \( | f^{(N+1)}(t) | \leq M  \) for all \( t \in [0,x ] \), show 
        \[  | E_{N}(x) | \leq \frac{ M x^{N+1} }{ (N+1)! }. \]
        \begin{proof}
            Let \( f  \), \( S_{N} \), and \( E_{N} \) be as Theorem 6.6.3. Let \( x \in (0,R ) \) and let \( x_{N+1} \in [0,x] \). By part (a), we know that 
        \[  \frac{ E_N(x) }{ x^{N+1} } \leq \frac{ E_{N}^{(N+1)}(x_{N+1}) }{ (N+1)! }  \]
        where \( x_{N+1} \in (0, x^{N}) \subseteq \dots \subseteq (0,x)  \) and 
        \[  E_{N}^{(N+1)}(x_{N+1}) = f^{(N+1)}(x_{N+1}) - S_{N}^{(N+1)}(x_{N+1}). \]
        Since \( S_{N}^{(N+1)}(x) = 0  \) past the \( n \)th derivative, we know that 
        \[  E_{N}^{(N+1)}(x_{N+1}) = f^{(N+1)}(x_{N+1}). \] Now set \( x^{N+1} = t  \). Since \( | f^{(N+1)}(t) | \leq M  \) by assumption, we must have
        \[  | E_N(x)  | \leq \frac{ | f^{(N+1)}(x_{N+1}) | x^{N+1} }{ (N+1)! }  \leq \frac{ M x^{N+1} }{ (N+1)! }.\]
        \end{proof}
\end{enumerate}











