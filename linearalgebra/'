\section{Linear Dependence and Linear Independence}

\subsubsection{Exercise 1.5.1} Label the following statements as true or false.

\begin{enumerate}
    \item[(a)] If \( S  \) is a linearly dependent set, then each vector in \( S  \) is a linear combination of other vector in \( S  \).
        \begin{proof}
        \textbf{True}
        \end{proof}
    \item[(b)] Any set containing the zero vector is linearly dependent.
        \begin{proof}
        \textbf{True}
        \end{proof}
    \item[(c)] The empty set is linearly dependent.
        \begin{proof}
        \textbf{False}. It is linearly independent.
        \end{proof}
    \item[(d)] Subsets of linearly dependent sets are linearly dependent. 
        \textbf{True} by Theorem 6.
    \item[(e)] Subsets of linearly independent sets are linearly independent.
        \begin{proof}
        \textbf{True} by corollary to Theorem 6.
        \end{proof}
    \item[(f)] If \( a_{1} x_{1} + a_{2} x_{2} + \cdots + a_{n} x_{n} = 0  \) and \( x_{1}, x_{2}, \dots, x_{n}  \) are linearly independent, then all the scalars \( a_{i}  \) are zero.
        \begin{proof}
       \textbf{True} this is by definition. 
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 1.5.4} In \( F^{n}  \), let \( e_{j}  \) denote the vector whose \( j \)th coordinate is \( 1  \) and whose other coordinates are \( 0  \). Prove that \( \{ e_{1}, e_{2}, \dots, e_{n}  \}  \) is linearly independent.
\begin{proof}
    Choose a finite amount of scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) to create the following linear combination:
    \[  a_{1} e_{1} + a_{2} e_{2} + \cdots + a_{n} e_{n} = (0,0, \dots, 0). \tag{1} \]
    To show that the set \( \{ e_{1} ,e_{2}, \dots, e_{n}  \}  \) is linearly independent, we need to show that the scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) have the trivial representation; that is, \( a_{1} = a_{2} = \cdots = a_{n} = 0  \).
    Since the \( j \)th coordinate of \( e_{j} \) is \( 1  \) but \( 0 \) in all the other entries, we have that 
    \begin{align*}
        &a_{1} (1,0,\dots, 0) + a_{2} (0,1,\dots,0) + \cdots + a_{n} (0,0, \dots, 1 )    \\
        &= (a_{1}, 0, \dots, 0  ) + (0 , a_{2}, \dots, 0 ) + \cdots + (0,0, \dots, a_{n}) \\
        &= (a_{1}, a_{2}, \dots , a_{n}  ).
    \end{align*}
    Hence, we have 
    \[  (a_{1}, a_{2} , \dots, a_{n} ) = (0,0, \dots, 0). \]
    Equating each entry of the left side of the equation above to \( 0 \), we find that \( a_{i} = 0  \) for all \( 1 \leq j \leq n  \). Hence, the set \( \{ e_{1}, e_{2}, \dots, e_{n}  \}  \) is linearly independent.
\end{proof}


\subsubsection{Exercise 1.5.4} Show that the set \( \{ 1,x, x^{2}, \dots, x^{n}  \}  \) is linearly independent in \( P_{n}(F) \).
\begin{proof}
    Just like the prior exercise, we need to show that we can find scalars \( a_{0}, a_{1}, \dots, a_{n} \in F  \)  such that 
    \[  a_{0} + a_{1} x + a_{2} x^{2} + \cdots + a_{n} x^{n} = 0   \]
    where \( a_{i} = 0  \) for all \( 0 \leq i \leq n \). Note that the \( 0  \) polynomial is just 
    \[  0 + 0 x + 0 x^{2} + \cdots + 0 x^{n} = 0.  \]
    Hence, equating coefficients we immediately get that \( a_{i} = 0  \) for all \( 0 \leq i \leq n \). Thus, the set \( \{ 1 , x , x^{2}, \dots, x^{n} \}  \)  is linearly independent. 
\end{proof}

\subsubsection{Exercise 1.5.6} In \( M_{m \times n}(F) \), let \( E^{ij}  \) denote the matrix whose only nonzero entry is \( 1  \) in the \( i \)th row and \( j \)th column. Prove that 
\( \{ E^{ij} : 1 \leq i \leq m , 1 \leq j \leq n  \}  \) is linearly independent.
\begin{proof}
    First, we create a  linear combination of a finite amount vectors in \( E  = \{ E^{ij} : 1 \leq i  \leq m , 1 \leq j \leq n   \}  \) with scalars \( \delta_{k}  \) for \( 1 \leq k \leq N  \) with \( N = mn \) as the number of total entries in each matrix in \( \{ E^{ij} : 1 \leq i \leq m , 1 \leq j \leq n  \}  \). Note that after doing our scalar multiplication and summing up each term, we find that each  \( \delta_{k} E_{ij} = \delta_{k}   \) in our linear combination can equated with a corresponding \( i  \) and  \( j  \) entry in the zero matrix such that \( \delta_{k} =  0  \) for all \( 1 \leq k \leq N  \). Hence, \( E  \) is a linearly independent set.
\end{proof}

\subsubsection{Exercise 1.5.7} Recall from Example 3 in Section 1.3 that the set of diagonal matrices in \( M_{2 \times 2 }(F) \) is a subspace. Find a linearly independent set that generates this subspace.
\begin{proof}
Define \( W  \) as the linearly independent spanning set of the set of diagonal matrices in  \( M_{2 \times 2 } \) where 
\begin{align*}
    W &= \Bigg\{ \begin{pmatrix}
            1 & 0 \\
            0 & 0 
    \end{pmatrix} , \begin{pmatrix}
            0 & 0 \\
            0 & 1 
    \end{pmatrix} \Bigg\}. \\
\end{align*}
To see why \( W  \) is a linearly independent set, choose scalars \( \delta_{1} , \delta_{2} \in F  \) such that 
\[  \delta_{1} \begin{pmatrix}
    1 & 0 \\
    0 & 0 
\end{pmatrix}  + \delta_{2} \begin{pmatrix}
    0 & 0 \\
    0 & 1 
\end{pmatrix} = \begin{pmatrix}
    0 & 0 \\
    0 & 0 
\end{pmatrix}.\]
Performing scalar multiplication and vector addition gives us the following equation 
\[  \begin{pmatrix}
    \delta_1 & 0 \\ 
    0 & \delta_2  
\end{pmatrix}  = \begin{pmatrix}
    0 & 0 \\
    0 & 0 
\end{pmatrix}. \]
Since the zero matrix is a diagonal matrix, we know that equation entries where  \( i = j    \)yields \( \delta_{1} = \delta_{2} = 0 \). Hence, \( W  \) is a linearly independent set that generates the set of diagonal matrices of \( M_{2 \times 2 }(F ) \).
\end{proof}

\subsubsection{Exercise 1.5.8} Let \( S = \{ (1,1,0) , (1,0,1), (0,1,1) \}  \) be a subset of the vector space \( F^{3} \).
\begin{enumerate}
    \item[(a)] Prove that if \( F = \R  \), then \( S  \) is linearly independent.
        \begin{proof}
        
        \end{proof}
    \item[(b)] Prove that if \( F  \) has characteristic two, then \( S  \) is linearly dependent.
        \begin{proof}
        
        \end{proof}
\end{enumerate} 


\subsubsection{Exercise 1.5.9} Let \( u \) and \( v  \) be distinct vectors in a vector space \( V  \). Show that \( \{ u,v  \}  \) is linearly dependent if and only if \( u  \) or \( v  \) is a multiple of the other.
I have written two proofs for this:
\begin{proof}
Let \( u  \) and \( v  \) be distinct vectors in a vector space \( V  \). 

( \(  \Rightarrow \) ) Since \( \{ u,v  \}   \) is a linearly dependent set, we can find scalars \( a_{1} , a_{2} \in F   \) such that 
\[ a_{1} u + a_{2} v = 0 \tag{1} \] 
Suppose \( v  \) is not a multiple of \( u  \) and choose \( a_{1} \neq 0  \) since \( \{ u,v  \}   \) is linearly dependent. We need to show that \( u  \) is a multiple of \( v  \). Solving for \( u  \), we get that
\[  u =  - \frac{ a_{2} }{ a_{1} }   v. \]
Hence, \( u  \) is a multiple of \( v  \). 

( \( \Leftarrow \) ) Suppose \( u  \) or \( v  \) is a scalar multiple of the other. Assume \( u  \) is the scalar multiple of \( v  \). Then for some \( c \neq 0 \in F  \), we have \( u = cv  \).  Hence, we have \( u - cv = 1u - cv = 0  \). This tells us that \( \{ u,v  \}  \) is linearly dependent. 
\end{proof}


\subsubsection{Exercise 1.5.12} Prove Theorem 1.6 and its corollary.
\begin{proof}
See proof in notes.
\end{proof}

\subsubsection{Exercise 1.5.13} Let \( V  \) be a vector space over a field of characteristic not equal to two.
\begin{enumerate}
    \item[(a)] Let \( u  \) and \( v  \) be distinct vectors in \( V  \). Prove that \( \{ u,v  \}   \) is linearly independent if and only if \( \{ u + v , u - v  \}   \) is linearly independent.
        \begin{proof}
        Let \( u \) and \( v  \) be distinct vectors in \( V  \). 

        For the forwards direction, assume \( \{ u,v  \}   \) is a linearly independent set. We need to show that \( \{ u + v , u - v  \}  \) is linearly independent. Hence, we need to find \( a,b \in F  \) such that 
        \[  a(u+v) + b(u-v) = 0. \tag{1} \]
        Note that (1) leads to 
        \begin{align*}
            a(u+v) + b(u-v) &= au + av + bu - bv \\
                            &= au - bv + av + bu. 
        \end{align*}
        Since \( \{ u,v  \}   \) is a linearly independent set, we have that 
        \[  au - bv = 0  \]
        and 
        \[  av + bu = 0  \] for \( a=b = 0 \). Hence, 
        \[  a(u+v) + b(u-v) = 0  \] for \( a = b = 0  \) and so \( \{ u - v , u + v  \}   \) is a linearly independent set.

        For the backwards direction, suppose \( \{ u + v , u - v  \}  \) is linearly independent. We need to show that \( \{ u , v  \}   \) is linearly independent. Note that \( a,b \in F  \) such that 
        \[  a(u+v) + b(u-v) = 0  \]
        for \( a = b = 0  \) since \( \{ u - v, u + v  \}  \) is linearly independent. Note that
        \begin{align*}
            a(u+v) + b(u-v) &= au + av + bu - bv \\
                            &= au - bv + av + bu \\
                            &= 0 + av + bu \\
                            &= 0. 
        \end{align*}
        Thus, \( av + bu = 0  \) where \( a,b  \) both zero. Thus, the set \( \{ u,v  \}   \) is linearly independent.
        \end{proof}
    \item[(b)] Let \( u, v ,  \) and \( w  \) be distinct vectors in \( V  \). Prove that \( \{ u,v,w  \}  \) is linearly independent if and only if \( \{ u+v , u + w , v + w  \}  \) is linearly independent.
        \begin{proof}
        For the forwards direction, suppose \( \{ u,v ,w  \}  \) is linearly independent. Then choose  scalars \( a_{1}, a_{2}, a_{3} \in  F    \) such that 
        \[  a_{1} u + a_{2} v  + a_{3} w = 0   \]
        with \( a_{1} = a_{2} = a_{3} = 0  \). We need to show that \( \{ u+v , u + w , v + w  \}  \) is linearly independent; that is, we need to show that we can find scalars \( a_{1}, a_{2}, a_{3} \in F  \) such that 
        \[  a_{1}(u+v) + a_{2} (u+w) + a_{3} (v+w) = 0 \tag{1} \]
        for \( a_{1} = a_{2} = a_{3} = 0  \). Observe that (1) can be written in the following way 
        \[  ( a_{1} u + a_{3} v + a_{2} w )  + ( a_{1} v + a_{2} u + a_{3}w )  = 0 \tag{2} \]
        Since \( \{ u,v,w \}  \) is linearly independent, we know that \( a_{1} = a_{2} = a_{3} = 0  \). But this also has to mean that \( \{ u+ v, u + w , v + w  \}  \) is a linearly independent set.
        
        For the backwards direction, suppose \( \{ u + v , u + w , v + w  \}  \) is linearly independent. Then choose scalars \( a_{1}, a_{2}, a_{3}  \in F  \) such that 
        \[  a_{1} (u + v ) + a_{2} (u + w) + a_{3} (v + w) = 0 \tag{1}.  \]
        We need to show that \( \{ u,v,w \}  \) is linearly independent. Observe that (1) can be re-written as 
        \begin{align*}
            &(a_{1} u + a_{3} v + a_{2} w )  + (a_{1} v + a_{2} u + a_{3} w ) = 0 \\
            &\implies 0 + (a_{1} v + a_{2} u + a_{3} w ) = 0 \\
            &\implies a_{1} v + a_{2} u + a_{3} w = 0
        \end{align*}
        where \( a_{1} = a_{2} = a_{3} = 0  \). Hence, \( \{ u,v ,w  \}   \) is linearly independent.
        \end{proof}
\end{enumerate}


\subsubsection{Exercise 1.5.14} Prove that a set \( S  \) is linearly dependent if and only if \( S = \{ 0  \}  \) or there exists distinct vectors \( v , u_{1}, u_{2}, \dots, u_{n} \in S  \) such that \( v  \) is a linear combination of \( u_{1}, u_{2}, \dots, u_{n} \).
\begin{proof}
For the forwards direction, suppose \( S  \) is linearly dependent. We need to show that either \( S = \{ 0  \}    \) or there exists distinct vectors \( v, u_{1}, u_{2}, \dots, u_{n} \in S   \) such that \( v  \) is a linear combination of \( u_{1}, u_{2}, \dots, u_{n}   \). Suppose that there does not exists distinct vectors \( v, u_{1}, u_{2}, \dots, u_{n} \in S   \) such that \( v  \) is linear combination of \( u_{1}, u_{2}, \dots, u_{n}  \). We need to show that \( S = \{ 0  \}  \).
\end{proof}


\subsubsection{Exercise 1.5.15} Prove that a set \( S = \{ u_{1} , u_{2}, \dots, u_{n} \}   \) be a finite set of vectors. Prove that \( S  \) is linearly dependent if and only if \( u_{1} = 0  \)  or \( u_{k+1} \in \text{span}(\{ u_{1}, u_{2}, \dots, u_{k } \} ) \) for some \( k  \) where \( 1 \leq k < n  \). 
\begin{proof}
Suppose  \( u_{k+1} \notin \text{span}(\{ u_{1} , u_{2}, \dots, u_{k }  \} ) \) for all \( 1 \leq k \leq n \). Since \( S  \) is linearly dependent, we know that the zero vector is contained in \( S  \). Choose \( k =1  \) such that \( u_{1} = 0  \) and we are done. On the other hand, suppose \( u_{1} \neq 0  \). We need to show that \( u_{k+1} \in \text{span}(\{ u_{1} , u_{2} , \dots, u_{k } \} ) \). Choose \( k = n -1  \). Then clearly \( n = k + 1  \). Since \( S  \) is linear independent, choose scalars \( a_{1} , a_{2}, \dots, a_{n} \in F   \) such that
\[ a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{k } u_{k } + a_{k+1}u_{k+1} = 0.    \tag{1}\]
where \( a_{1}, a_{2} , \dots, a_{k } ,  a_{k+1} \) not all zero. Solving for \( u_{k+1}  \) by subtracting \( a_{k+1} u_{k+1} \) on both sides of (1) and multiplying \( -a^{-1}_{k+1}  \) on both sides of (1), we end up with the following equation:
\[ u_{k+1} = - (a^{-1}_{k+1} a_{1})  u_{1} - ( a^{-1}_{k+1} a_{2} ) u_{2}  - \cdots -  ( a^{-1}_{k+1} a_{k}    ) u_{k}.    \]
This tells us that \( u_{k+1} \) can be written as a linear combination of vectors  \( u_{1}, u_{2}, \dots, u_{k}   \). Hence, \( u_{k+1} \in \text{span}(\{ u_{1}, u_{2}, \dots u_{k} \} ) \).

Conversely, either \( u_{1} = 0  \) or \( u_{k+1} \in \text{span}(u_{1}, u_{2}, \dots, u_{k} ) \). Suppose \( u_{1} = 0  \). Then \( S  \) contains the zero vector so \( S  \) must be linearly dependent. On the other hand, choose \( k = n-1  \) where \( 1 \leq k < n  \) such that  \( u_{k+1} \in \text{span}(\{ u_{1}, u_{2}, \dots, u_{k } \} ) \) implies that there exists scalars \( a_{1}, a_{2}, \dots, a_{k }  \) such that 
\begin{align*} 
    &u_{k+1} = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{k } u_{k } \\ 
    &\implies u_{n} = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n-1} u_{n-1}. \tag{1}
\end{align*}
Subtracting \( u_{n}  \) on both sides of (1) implies that
\[ ( a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n-1} u_{n-1} ) - 1  u_{n} = 0.   \]
Since not all scalars in the linear combination above are zero, we know that \( S  \) must be linearly dependent.

\end{proof}

