\section{Diagonalizability}
Our goals in this section is to: 
\begin{itemize}
    \item Create a simple test to determine whether an operator or a matrix can be diagonalized.
    \item Develop a method for finding a basis of eigenvectors.
\end{itemize}

The next theorem that any constructed set that consists of eigenvectors is linearly independent.

\begin{theorem}\label{Theorem 5.5}
    Let \( T  \) be a linear operator on a vector space \( V  \), and let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) be distinct eigenvalues of \( T  \). If \( {v}_{1}, {v}_{2}, \dots, {v}_{k} \) are eigenvectors of \( T  \) such that \( {\lambda}_{i} \) corresponds to \( {v}_{i} \) (\( 1 \leq i \leq k  \)), then \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{k } \}  \) is linearly independent.
\end{theorem}
\begin{proof}
We proceed via mathematical induction on \( k  \). Suppose that \( k = 1  \). Let \( \lambda_1  \) be an eigenvalue corresponding to \( {v}_{1} \). Since \( {v}_{1} \neq 0  \), we have that \( \{ {v}_{1} \}  \) is linearly independent. Now, assume that the theorem holds for \( k - 1  \) case. Note that \( k - 1 \geq 1  \). Our goal is to show that for some scalars \( {a}_{1}, {a}_{2}, \dots, {a}_{k}  \), we have  
\[  {a}_{1} {v}_{1} + {a}_{2} {v}_{2} + \cdots + {a}_{k} {v}_{k} = 0 \tag{1}  \]
where \( {a}_{1} = {a}_{2} = \cdots = {a}_{k} = 0  \). Applying \( T - {\lambda}_{k} I  \) on both sides of (1), we have
\[ (T - {\lambda}_{k} I) ({a}_{1} {v}_{1} + {a}_{2} {v}_{2} + \cdots + {a}_{k} {v}_{k} ) = 0   \]
implies
\[  {a}_{1} ({\lambda}_{1} - {\lambda}_{k}) {v}_{1} + {a}_{2} ({\lambda}_{2} - {\lambda}_{k}) {v}_{2} + \cdots + {a}_{k-1} ({\lambda}_{k-1} - {\lambda}_{k}) {v}_{k-1} = 0.  \]
Using our induction hypothesis, we have that \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{k-1} \}  \) implies that 
\[  {a}_{1} ({\lambda}_{1} - {\lambda}_{k}) = {a}_{2} ({\lambda}_{2} - {\lambda}_{k})  = \cdots + {a}_{k-1} ( {\lambda}_{k-1} - {\lambda}_{k} ) = 0.\]
Since \( {\lambda}_{i}  \) for \( 1 \leq i \leq k   \) is distinct, we have that \( {\lambda}_{i-1} - \lambda_{i} \neq  0   \) for all \( 1 \leq i \leq k - 1  \). Consequently, this results in \( {a}_{i} = 0  \) for all \( 1 \leq i \leq k - 1 \) which leaves us with \( {a}_{k} {v}_{k} = 0  \). Since \( {v}_{k} \) is an eigenvector, we have \( {v}_{k} \neq  0  \) so \( {a}_{k} = 0  \). Thus, we have \( {a}_{1} = {a}_{2} = \cdots = {a}_{k-1} = {a}_{k} = 0  \) implies that \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{k} \}   \) is a linearly independent set.   
\end{proof}

\begin{corollary}
  Let \( T  \) be a linear operator on an \( n- \)dimensional  vector space \( V  \). If \( T  \) has \( n  \) distinct eigenvalues, then \( T  \) is diagonalizable.  
\end{corollary}
\begin{proof}
Suppose that \( T  \) has \( n  \) distinct eigenvalues \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{n} \). We can choose an eigenvalue \( {\lambda}_{i} \) for each corresponding eigenvector \( {v}_{i} \) for all \( i  \). Note that each \( {\lambda}_{i} \) is distinct. Using {\hyperref[Theorem 5.5]{Theorem 5.5}}, the set \( \{ {v}_{1}, \dots, {v}_{n} \}  \) is linearly independent. Since \( \text{dim}(V) = n  \), this set is a basis for \( V  \). Thus, \( T  \) is diagonalizable via Theorem 5.1. 
\end{proof}

\begin{eg}
   Let  
   \[  A = \begin{pmatrix} 
       1 & 1 \\
       1 & 1 
             \end{pmatrix}  \in {M}_{2 \times 2}(\R). \]
    The characteristic polynomial of \( A  \) (and hence of \( {L}_{A} \)) is
    \[  \text{det}(A - t I ) = \text{det} \begin{pmatrix} 
        1 - t & 1 \\
        1 & 1 - t 
              \end{pmatrix}  = t (t-2). \]
        We can see that the eigenvalues of \( {L}_{A} \) are \( 0  \) and \( 2 \). These eigenvalues correspond to the eigenvectors of \( {L}_{A} \) which form a basis such that \( A  \) is a diagonal matrix. Thus, \( {L}_{A} \) is a linear-operator that is diagonalizable (and hence \( A  \) is also diagonalizable).
\end{eg}

Note that it is not necessarily true that a diagonalizable linear operator contains \( n  \) distinct eigenvalues. A quick counter-example would be the identity operator. Even though \( I  \) is diagonalizable, it only contains one eigenvalue, namely, \( \lambda = 1  \).

This tells us that diagonalizability requires a much stronger condition on the characteristic polynomial.

\begin{definition}[Splits Over]
   A polynomial \( f(t)  \) in \( P(F)  \) \textbf{splits over} \( F  \) if there are scalars \( c, {a}_{1}, {a}_{2}, \dots, {a}_{n} \) (not necessarily distinct) in \( F  \) such that 
   \[  f(t) = c(t- {a}_{1})(t - {a}_{2})\cdots (t - {a}_{n}). \]
\end{definition}

The splitting behavior of a polynomial is different based on which field the polynomial is defined on. For example, we see that \( t^{2} - 1  \) splits over \( \R  \), but \( (t^{2} + 1)(t-2) \) does not since \( t^{2} + 1  \) has no solutions in the real line. However, \( t^{2} + 1  \) can further be split if it was defined over \( \C  \). In this case, \( (t^{2} +1)(t-2) \) does split over \( \C  \), namely, it splits into \( (t+i)(t-i)(t-2) \).

\begin{theorem}
   The characteristic polynomial of any diagonalizable linear operator splits.
\end{theorem}
\begin{proof}
Let \( T  \) be a diagonalizable linear operator on the \( n- \)dimensional vector space \( V  \), and let \( \beta \) be an ordered basis for \( V  \) such that \( [T]_{\beta} = D  \) is a diagonal matrix. Suppose that  
\[ \begin{pmatrix} 
{\lambda}_{1} & 0 & \cdots & 0 \\
0  & {\lambda}_{2} & \cdots & 0 \\
\vdots & \vdots &   & \vdots \\  
0 & 0 & \cdots & {\lambda}_{n}
\end{pmatrix}, \]
and let \( f(t)  \) be the characteristic polynomial of \( T  \). Then
\begin{align*} 
    f(t) = \text{det}(D - tI) = \text{det} \begin{pmatrix} 
{\lambda}_{1} - t  & 0 & \cdots & 0 \\
0  & {\lambda}_{2} - t & \cdots & 0 \\
\vdots & \vdots &   & \vdots \\  
0 & 0 & \cdots & {\lambda}_{n} - t
\end{pmatrix} \\  
= ({\lambda}_{1} - t)({\lambda}_{2} -t)\cdots ({\lambda}_{n} -t) = (-1)^{n}(t - {\lambda}_{1})(t - {\lambda}_{2})\cdots(t- {\lambda}_{n}). 
    \end{align*}
\end{proof}
\begin{itemize}
    \item If \( T  \) is a diagonalizable linear operator but fails to have distinct eigenvalues, then the characteristic polynomial of \( T  \) must have repeated zeros.
    \item The converse of the theorem above is not true since not every characteristic polynomial of a linear operator of \( T  \) guarantees that \( T  \) be diagonalizable.
\end{itemize}

\begin{definition}[Algebraic Multiplicity]
    Let \( \lambda  \) be an eigenvalue of a linear operator or matrix with characteristic polynomial \( f(t) \). The \textbf{(algebraic) multiplicity} of \( \lambda  \) is the largest positive integer \( k  \) for which \( (t- \lambda)^{k }  \) is a factor of \( f(t) \).
\end{definition}

Recall that a diagonalizable linear operator \( T  \) that is defined over a finite-dimensional vector space \( V  \) contains an ordered basis \( \beta \) for \( V \) consisting of eigenvectors of \( T  \). By Theorem 5.1, \( [T]_{\beta} \) is a diagonal matrix in which the diagonal entries are the eigenvalues of \( T  \). Remember that each eigenvalue of \( T  \) corresponds to the diagonal entry of \( [T]_{\beta} \) as many times as its multiplicity permits. 

We can investigate the exact amount of independent eigenvectors that are associated with a given eigenvalue. A way we can do this is to look at the null space of \( T - \lambda I  \).

\begin{definition}[Eigenspace]
    Let \( T  \) be a linear operator on a vector space \( V  \) ,and let \( \lambda  \) be an eigenvalue of \( T  \). Define \( {E}_{\lambda} = \{ x \in V : T(x) = \lambda x \}  =  N(T - \lambda {I}_{V}) \). The set \( {E}_{\lambda} \) is called the \textbf{eigenspace} of \( T  \) corresponding to the eigenvalue \( \lambda  \). Analogously, we define the \textbf{eigenspace} of a square matrix \( A  \) to be the eigenspace of \( {L}_{A} \).
\end{definition}

It can easily be proven that \( {E}_{\lambda} \) is a subspace of \( V  \). The maximum number of linearly independent eigenvectors that correspond to a given eigenvalue can therefore be seen by taking the dimension of the given eigenspace.

\begin{theorem}\label{Theorem 5.7}
    Let \( T  \) be a linear operator on a finite-dimensional vector space \( V  \), and let \( \lambda  \) be an eigenvalue of \( T  \) having multiplicity \(  m  \). Then \( 1 \leq \text{dim}({E}_{\lambda}) \leq  m \).
\end{theorem} 
\begin{proof}
    Choose an ordered basis \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{p} \}   \) for \( {E}_{\lambda} \) and extend this basis into \( \beta = \{ {v}_{1}, {v}_{2}, \dots ,{v}_{p}, {v}_{p+1}, \dots, {v}_{n} \}   \) for \( V  \), and let \( A = [T]_{\beta} \). Observe that \( {v}_{i} \ (1 \leq i \leq p ) \) is an eigenvector of \( T  \) corresponding to \( \lambda  \), and therefore 
    \[  A = \begin{pmatrix} 
        \lambda {I}_{p} & B \\
        O & C 
              \end{pmatrix}.   \]
By Exercise 21 of Section 4.3, the characteristic polynomial of \( T  \) is
\begin{align*}
    f(t) = \text{det}(A - t {I}_{n}) &= \text{det}\begin{pmatrix} 
        (\lambda - t){I}_{p} & B  \\
        O & C - t {I}_{n-p}
              \end{pmatrix}  \\
              &= \text{det}((\lambda -t){I}_{p}) \text{det}(C - {tI}_{n-p}) \\
              &= (\lambda - t)^{p} g(t)
\end{align*}
where \( g(t) = \text{det}(C - {tI}_{n-p}) \) is a polynomial. Thus \( (\lambda - t)^{p} \) is a factor of \( f(t)  \), and hence the multiplicity of \( \lambda  \) is at least \( p  \). However, \( \text{dim}({E}_{\lambda}) = p  \), and so \( \text{dim}({E}_{\lambda}) \leq m \).
\end{proof}

\begin{lemma}
    Let \( T  \) be a linear operator, and let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) be distinct eigenvalues of \( T  \). For each \( i = 1,2,\dots,k  \), and let \( {v}_{i} \in {E}_{{\lambda}_{i}}  \), the eigenspace corresponding to \( {\lambda}_{i} \). If
    \[  {v}_{1} + {v}_{2} + \cdots + {v}_{k} = 0, \]
    then \( {v}_{i} = 0  \) for all \( i \).
\end{lemma}

The last two examples illustrate that operators whose characteristic polynomial splits is diagonalizable if and only if the multiplicity of \( \lambda \) is equal to the dimension its corresponding eigenspace \( {E}_{\lambda} \).

\begin{proof}
Suppose otherwise. By renumbering if necessary, suppose that, for \( 1 \leq m \leq k  \), we have \( {v}_{i} \neq 0  \) for \( 1 \leq i \leq m \), and \( {v}_{i} = 0  \) for \( i > m  \). Then, for each \( i \leq m  \), \( {v}_{i} \) is an eigenvector of \( T \) corresponding to \( {\lambda}_{i} \) and  
\[ {v}_{1} + {v}_{2} + \cdots + {v}_{m} = 0. \]
But this contradicts Theorem 5.5, which states that all \( {v}_{i} \)'s are linearly independent. Thus, we should have \( {v}_{i} = 0  \) for all \( i \).
\end{proof}

\begin{theorem}
   Let \( T \) be a linear operator on a vector space \( V  \), and let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k } \) be distinct eigenvalues of \( T  \). For each \( i = 1,2, \dots, k  \), let \( {S}_{i} \) be a finite linearly independent subset of the eigenspace \( {E}_{{\lambda}_{i}}  \). Then \( S = {S}_{1} \cup {S}_{2} \cup \cdots \cup {S}_{k} \) is a linearly independent subset of \( V  \). 
\end{theorem}
\begin{proof}
Suppose that for each \( i  \) 
\[  {S}_{i} = \{ {v}_{i1}, {v}_{i2}, \dots, {v}_{i {n}_{i}} \}. \]
Then \( S = \{ {v}_{ij} : 1 \leq j \leq {n}_{i}, \ \text{and} \ 1 \leq i \leq k  \}. \) Consider any scalars \( \{ {a}_{ij} \}  \) such that 
\[  \sum_{ i=1 }^{ k  } \sum_{ j=1 }^{ {n}_{i} } {a}_{ij } {v}_{ij} = 0.  \]
For each \( i  \), let 
\[  {w}_{i} = \sum_{ j=1  }^{ {n}_{i} } {a}_{ij} {v}_{ij}. \]
Then \( {w}_{i} \in {E}_{\lambda}  \), for each \( i \), and 
\[  {w}_{1} + {w}_{2} + \cdots + {w}_{k} = 0. \]
Thus, we must have \( {w}_{i} = 0  \) for all \( i  \) by the lemma. Since each \( {S}_{i} \) is linearly independent, we must have that \( {a}_{ij} = 0  \) for each \( 1 \leq j \leq {n}_{i}  \). Thus, \( S  \) is linearly independent.
\end{proof}

The consequence of this theorem is that it creates a procedure for constructing a linearly independent subset of eigenvectors via collecting bases for individual eigenspaces. The following theorem allows us to determine when the resulting set becomes a basis for the entire space.

\begin{theorem}\label{Theorem 5.9}
    Let \( T  \) be a linear operator on a finite-dimensional vector space \( V  \) such that the characteristic polynomial of \( T  \) splits. Let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) be the distinct eigenvalues of \( T  \). Then
    \begin{enumerate}
        \item[(a)] \( T  \) is diagonalizable if and only if the multiplicity of \( {\lambda}_{i} \) is equal to \( \text{dim}({E}_{{\lambda}_{i}}) \) for all \( i  \).
        \item[(b)] If \( T  \) is diagonalizable and \( {\beta}_{i} \) is an ordered basis for \( {E}_{{\lambda}_{i}} \) for each \( i  \), then \( \beta = {\beta}_{1} \cup {\beta}_{2} \cup \cdots \cup {\beta}_{k} \) is an ordered basis for \( V  \) consisting of eigenvectors of \( T  \).
    \end{enumerate}
\end{theorem}
\begin{proof}
For each \( i \), let \( {m}_{i} \) denote the multiplicity of \( {\lambda}_{i} \), \( {d}_{i} = \text{dim}({E}_{{\lambda}_{i}}) \), and \( n = \text{dim}(V) \). 

Suppose that \( T  \) is diagonalizable. Let \( \beta \) be a basis for \( V  \) consisting of eigenvectors of \( T  \). For each \( i \), let 
\[  {\beta}_{i} = \beta \cap {E}_{{\lambda}_{i}} \] be the set of vectors in \( \beta  \) that are eigenvectors corresponding to \( \lambda_{i} \), and let \( {n}_{i} \) denote the number of vectors in \( {\beta}_{i} \). Since each \( {\beta}_{i} \subseteq {E}_{{\lambda}_{i}} \) and that each \( {\beta}_{i} \) is linearly independent (Note that each \( {E}_{{\lambda}_{i}} \) is a subspace), we have \( {n}_{i} \leq {d}_{i} \). By {\hyperref[Theorem 5.7]{Theorem 5.7}}, we have \( {d}_{i} \leq {m}_{i} \). 

Notice how all the \( {n}_{i} \)'s sum to \( n  \) because \( \beta \) contains \( n  \) vectors. At the same time, all the \( {m}_{i} \)'s must also sum up to \( n  \) since the degree of the characteristic polynomial of \( T  \) must be \( n  \) which is also equal to the sum of all the multiplicities of the eigenvalues. Thus, we must have
\[  n = \sum_{ i=1 }^{ k  } {n}_{i} \leq \sum_{ i=1 }^{ k  } {d}_{i} \leq \sum_{ i=1 }^{ k  } {m}_{i} = n \]
which implies that
\[  \sum_{ i=1 }^{ k  } {n}_{i} - {m}_{i} = 0. \]
Since \( {m}_{i} - {d}_{i} \geq 0  \) for all \( i  \), we must have that \( {m}_{i} = {d}_{i} \) for all \( i \).

Conversely, suppose that \( {d}_{i} = {m}_{i} \) for all \( i  \). We will show that \( T  \) is diagonalizable and prove (b). For each \( {\beta}_{i} \) is an ordered basis for \( {E}_{{\lambda}_{i}} \) and let \( \beta = {\beta}_{1} \cup {\beta}_{2} \cup \cdots \cup {\beta}_{k } \). Using Theorem 5.8, we can see that \( \beta \) is linearly independent. Since \( {d}_{i} = {m}_{i} \) for all \( i \), \( \beta \) contains
\[  \sum_{ i=1 }^{ k  } {d}_{i} = \sum_{ i=1 }^{ k  } = n \]
vectors. Since \( \beta \) is a linearly independent subset of \( V  \) that contains equals \(  n  \) vectors, we must have that \( \beta \) spans \( V  \) via {\hyperref[2nd Corollary to RT]{2nd Corollary to the Replacement Theorem}} and thus \( \beta \) is an ordered basis for \( V  \) consisting of eigenvectors of \( T  \) corresponding to each \( {\lambda}_{i} \). But this also means that \( T  \) is diagonalizable by {\hyperref[Theorem 5.1]{Theorem 5.1}}.
\end{proof}

\subsection{Test for Diagonalization}

\begin{prop}
    Let \( T  \) be a linear operator on an \( n- \)dimensional vector space \( V  \). Then \( T  \) is diagonalizable if and only if both of the following conditions hold:
    \begin{enumerate}
        \item[(a)] The characteristic polynomial of \( T  \) splits.
        \item[(b)] For each eigenvalue \( \lambda  \) of \( T  \), the multiplicity of \( \lambda  \) equals \( n - \text{rank}(T - \lambda I ) \).
    \end{enumerate}
\end{prop}
\begin{itemize}
    \item We can also apply these conditions to any square matrix \( A  \) since testing whether \( A  \) is diagonalizable is just a matter of determining if the linear operator \( {L}_{A} \) is diagonalizable.
    \item If \( T  \) is a diagonalizable operator and \( {\beta}_{1}, {\beta}_{2}, \dots, {\beta}_{k}  \) are ordered bases each eigenspace of \( T  \), then the union \( \beta = {\beta}_{1} \cup {\beta}_{2} \cup \cdots \cup {\beta}_{k } \) is an ordered basis for \( V  \) consisting of eigenvectors of \( T  \), and hence \( [T]_{\beta} \) is a diagonal matrix via Theorem 5.1.
    \item To check whether a linear operator is diagonalizable, choose a suitable basis \( \alpha \) and work with \( B = [T]_{\alpha} \).
    \item Then check whether its characteristic polynomial splits and then check the second condition for each \textit{repeated} eigenvalue of \( B  \). If a given eigenvalue \( \lambda    \) contains a multiplicity of \( 1  \) then the second condition is automatically satisfied. Thus, only check for multiplicity that is greater than \( 1  \). 
    \item Given an \( n \times n \) diagonalizable matrix, the corollary to Theorem 2.23 allows us to find an invertible \( n \times n  \) matrix \( Q  \) and a diagonal \( n \times n  \) matrix \( D  \) such that  
        \[  Q^{-1}A Q = D.  \]
    \item The matrix \( Q  \) contains the basis of eigenvectors of \( A  \), while \( D  \) contains the eigenvalue corresponding to the \( j \)th column of \( Q  \).
\end{itemize}

\subsection{Direct Sums}

\begin{definition}[Sum of Subspaces]
    Let \( {W}_{1}, {W}_{2}, \dots, {W}_{k} \) be subspaces of a vector space \( V  \). We define the \textbf{sum} of these subspaces to be the set 
    \[  \{ {v}_{1} + {v}_{2} + \cdots + {v}_{k } : {v}_{i} \in {W}_{i} \ \text{for} \ 1 \leq i \leq k  \}, \]
    which we denote by \( {W}_{1} + {W}_{2} + \cdots + {W}_{k} \) or \( \sum_{ i=1 }^{ k  } {W}_{i} \).
\end{definition}

\begin{prop}
    The sum of subspaces of a vector space \( V  \) is also subspace.
\end{prop}

\begin{proof}
Exercise
\end{proof}

\begin{definition}[Direct Sum of Subspaces]
    Let \( {W}_{1}, {W}_{2}, \dots, {W}_{k} \) be subspaces of a vector space \( V  \). We call \( V  \) the \textbf{direct sum} of the subspaces \( {W}_{1}, {W}_{2}, \dots, {W}_{k } \) and write \( V = {W}_{1} \oplus {W}_{2} \oplus \cdots \oplus {W}_{k} \), if 
    \[  V = \sum_{ i=1  }^{  k  } {W}_{i} \]
    and 
    \[  {w}_{j} \cap \sum_{ i \neq j  }^{  }{W}_{i} = \{ 0  \} \ \text{for} \ (1 \leq j \leq k ). \]
\end{definition}

\begin{theorem}
    Let \( {W}_{1}, {W}_{2}, \dots, {W}_{k} \) be subspaces of a finite-dimensional vector space \( V  \). The following conditions are equivalent. 
    \begin{enumerate}
        \item[(a)] \( V = {W}_{1} \oplus {W}_{2} \oplus \cdots \oplus {W}_{k}  \).
        \item[(b)] \( V = \sum_{ i = 1  }^{ k  } {W}_{i}  \) and, for any vectors       \( {v}_{1}, {v}_{2}, \dots, {v}_{k } \) such that \( {v}_{i} \in {W}_{i} \) for \( 1 \leq i \leq k  \), if \( {v}_{1} + {v}_{2} + \cdots + {v}_{k } = 0  \), then \( {v}_{i} = 0  \) for all \( i \).
        \item[(c)] Each vector \( v \in V  \) can be uniquely written as \( v = {v}_{1} + {v}_{2} + \cdots + {v}_{k } \), where \( {v}_{i} \in {W}_{i} \).
        \item[(d)] If \( {\gamma}_{i} \) is an ordered basis for \( {W}_{i}  \) for \( 1 \leq i \leq k  \), then \( {\gamma}_{1} \cup {\gamma}_{2} \cup \cdots \cup {\gamma}_{k}  \) is an ordered basis for \( V  \).
        \item[(e)] For each \( i = 1, 2 , \dots, k  \), there exists an ordered basis \({\gamma}_{i} \) for \( {W}_{i} \) such that \( {\gamma}_{1} \cup {\gamma}_{2} \cup \cdots \cup {\gamma}_{k} \) is an ordered basis for \( V  \).
    \end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
    \item[(b)] Suppose that \( V = {W}_{1} \oplus {W}_{2} \oplus \cdots \oplus {W}_{k}   \). Let \( {v}_{1}, {v}_{2}, \dots, {v}_{k } \) such that \( {v}_{i} \in {W}_{i} \) for \( 1 \leq i \leq k  \). Suppose 
        \[  \sum_{ i=1  }^{ k  } {v}_{i} = 0. \]
         Choose \( i \neq j  \) such that 
        \[  -{v}_{j} = \sum_{ i \neq j }^{  } {v}_{i} \in {W}_{i}.  \]
        But notice that \( -{v}_{j} \in {W}_{j} \) and so we must have
        \[  {-v}_{j} \in {W}_{j} \cap \sum_{ i \neq j  }^{  }{W}_{i} = \{ 0  \}. \]
        Thus, \( {v}_{j} = 0  \) which subsequently proves that \( {v}_{i} = 0  \) for all \( 1 \leq i \leq k  \).
    \item[(c)] Assume (b). Let \( v \in V  \). Then there exists vectors \( {v}_{1}, {v}_{2}, \dots, {v}_{k} \) such that 
        \[  v = \sum_{ i=1  }^{ k  } {v}_{i}. \tag{1} \]
    Suppose that there also exists vectors \( {w}_{1}, {w}_{2}, \dots, {w}_{k } \) such that
    \[  v = \sum_{ i=1  }^{ k  } {w}_{i}. \tag{2} \]
    Then equating (1) and (2) and subtracting each \( {w}_{i}  \) on both sides, we obtain
    \[  \sum_{ i=1  }^{ k  } ({v}_{i} - {w}_{i}) = 0.   \]
    By part (b), we must have \( {v}_{i} - {w}_{i} = 0  \) which subsequently leads to \( {v}_{i} = {w}_{i} \) for all \( 1 \leq i \leq k  \). Thus, the representation \( v  \) is unique.
    \item[(d)] Suppose (c). For each \( i  \), let \( {\gamma}_{i} \) be an ordered basis for \( {W}_{i} \). Since  
        \[  V = \sum_{ i=1 }^{ k  }{W}_{i} \]
        by (c), it follows that \( {\gamma}_{1} \cup {\gamma}_{2} \cup \cdots \cup {\gamma}_{k} \) generates \( V  \). Now, we will show that
        \[  \gamma =  {\gamma}_{1} \cup {\gamma}_{2} \cup \cdots \cup {\gamma}_{k}  \]
        is linearly independent. Thus, consider vectors \( {v}_{ij} \in {\gamma}_{i} \) where \( 1 \leq j \leq {m}_{i} \) (the number of elements in each \( {\gamma}_{i} \)) and scalars \( {a}_{ij} \) such that
        \[  \sum_{ i,j }^{  } {a}_{ij} {v}_{ij} = 0. \]
        For each \( i  \), set
        \[  {w}_{i} = \sum_{ j=1 }^{ {m}_{i} } {a}_{ij} {v}_{ij}. \]
        Since each \( {\gamma}_{i}  \) is a basis for each \( {W}_{i}  \), we have \( {w}_{i} \in \text{span}({\gamma}_{i}) \) and that 
        \[  {w}_{1} + {w}_{2} + \cdots + {w}_{k} = \sum_{ i,j }^{  } {a}_{ij} {v}_{ij} = 0. \]
        Since each \( 0 \in {W}_{i} \), for each \( i  \) and 
        \[  0 + 0 + \cdots + 0 = {w}_{1} + {w}_{2} + \cdots {w}_{k}, \]
        (b) and (c) implies that \( {w}_{i} = 0  \) for all \( i  \). Thus,  
        \[  0 = {w}_{i} = \sum_{ j=1 }^{ {m}_{i} } {a}_{ij} {v}_{ij} \]
        for each \( i  \). Since each \( {\gamma}_{i} \) is linearly independent, we must have each \( {a}_{ij} = 0  \) for all \( i  \) and \( j  \). Thus, \( \gamma \) is linearly independent and therefore is a basis for \( V  \).
    \item[(e)] This follows immediately from (d).
    \item[(a)] Assume (e). For each \( i \), let \( {\gamma}_{i} \) be an ordered basis for \( {W}_{i} \) such that    
        \[  \gamma = {\gamma}_{1} \cup {\gamma}_{2} \cup \cdots \cup {\gamma}_{k} \]
        is an ordered basis for \( V  \). Thus, we have \( \text{span}(\gamma) = V  \). By repeated applications of Exercise 14 from Section 1.4, we get that
        \begin{align*}
            V &= \text{span}({\gamma}_{1} \cup {\gamma}_{2} \cup \cdots \cup {\gamma}_{k}) \\
              &= \text{span}({\gamma}_{1}) + \text{span}({\gamma}_{2}) + \cdots + \text{span}({\gamma}_{k}) \\
              &=  {W}_{1} + {W}_{2} + \cdots + {W}_{k}  \\
              &= \sum_{ i=1 }^{ k  }{W}_{i}.
        \end{align*}
        Now, suppose for sake of contradiction that we fix some \( j  \) in \( 1 \leq j \leq k  \), such that for some nonzero vector \( v \in V  \), we have
        \[  v \in {W}_{j} \cap \sum_{ i \neq j  }^{  } {W}_{i}. \]
        Then we have both
        \[  v \in {W}_{j} = \text{span}({\gamma}_{j}) \ \ \text{and} \ \ v \in \sum_{ i \neq j  }^{ {W}_{i} } = \text{span}\Big( \bigcup_{ i \neq j  }^{  }  {\gamma}_{i} \Big). \]
        But this implies that \( v  \) can be written in a non-unique way, contradicting the result found in {\hyperref[Unique combinations from bases]{Theorem 1.8}}. Thus, we must have  
        \[  {W}_{j} \cap \sum_{ i \neq j  }^{  }{W}_{i} = \{ 0  \} \]
        thereby showing that 
        \[ V = {\bigoplus}_{i=1}^{k} {W}_{i}.  \]
\end{enumerate}
\end{proof}

\begin{theorem}
    A linear operator \( T  \) on a finite-dimensional vector space \( V  \) is diagonalizable if and only if \( V  \) is the direct sum of the eigenspaces of \( T  \).
\end{theorem}
\begin{proof}
    Let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) be the distinct eigenvalues of \( T  \).
    
    For the forwards direction, suppose that \( T  \) is diagonalizable. For each \( i  \), choose an ordered basis \( {\gamma}_{i} \) for each eigenspace \( {E}_{{\lambda}_{i}} \). Using {\hyperref[Theorem 5.9]{Theorem 5.9}}, we know that 
    \[  \gamma = \bigcup_{ i=1  }^{ k  }  {\gamma}_{i}   \]
    is a basis for for \( V  \), and hence \( V  \) is a direct sum of each eigenspace \( {E}_{{\lambda}_{i}}  \) of \( T  \) by Theorem 5.10.

    For the backwards direction, suppose that \( V  \) is a direct sum of the eigenspaces of \( T  \). For each \( i  \), choose an ordered basis \( {\gamma}_{i}  \) for each \( {E}_{{\lambda}_{i}}  \). Using Theorem 5.10,  
    we have that
    \[  \gamma = \bigcup_{ i=1  }^{ k  }  {\gamma}_{i}   \]
    is a basis for \( V  \) consisting of eigenvectors of \( T  \). Thus, \( T  \) is diagonalizable by {\hyperref[Theorem 5.1]{Theorem 5.1}} .
\end{proof}
