\section{Diagonalizability}
Our goals in this section is to: 
\begin{itemize}
    \item Create a simple test to determine whether an operator or a matrix can be diagonalized.
    \item Develop a method for finding a basis of eigenvectors.
\end{itemize}

The next theorem that any constructed set that consists of eigenvectors is linearly independent.

\begin{theorem}\label{Theorem 5.5}
    Let \( T  \) be a linear operator on a vector space \( V  \), and let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) be distinct eigenvalues of \( T  \). If \( {v}_{1}, {v}_{2}, \dots, {v}_{k} \) are eigenvectors of \( T  \) such that \( {\lambda}_{i} \) corresponds to \( {v}_{i} \) (\( 1 \leq i \leq k  \)), then \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{k } \}  \) is linearly independent.
\end{theorem}
\begin{proof}
We proceed via mathematical induction on \( k  \). Suppose that \( k = 1  \). Let \( \lambda_1  \) be an eigenvalue corresponding to \( {v}_{1} \). Since \( {v}_{1} \neq 0  \), we have that \( \{ {v}_{1} \}  \) is linearly independent. Now, assume that the theorem holds for \( k - 1  \) case. Note that \( k - 1 \geq 1  \). Our goal is to show that for some scalars \( {a}_{1}, {a}_{2}, \dots, {a}_{k}  \), we have  
\[  {a}_{1} {v}_{1} + {a}_{2} {v}_{2} + \cdots + {a}_{k} {v}_{k} = 0 \tag{1}  \]
where \( {a}_{1} = {a}_{2} = \cdots = {a}_{k} = 0  \). Applying \( T - {\lambda}_{k} I  \) on both sides of (1), we have
\[ (T - {\lambda}_{k} I) ({a}_{1} {v}_{1} + {a}_{2} {v}_{2} + \cdots + {a}_{k} {v}_{k} ) = 0   \]
implies
\[  {a}_{1} ({\lambda}_{1} - {\lambda}_{k}) {v}_{1} + {a}_{2} ({\lambda}_{2} - {\lambda}_{k}) {v}_{2} + \cdots + {a}_{k-1} ({\lambda}_{k-1} - {\lambda}_{k}) {v}_{k-1} = 0.  \]
Using our induction hypothesis, we have that \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{k-1} \}  \) implies that 
\[  {a}_{1} ({\lambda}_{1} - {\lambda}_{k}) = {a}_{2} ({\lambda}_{2} - {\lambda}_{k})  = \cdots + {a}_{k-1} ( {\lambda}_{k-1} - {\lambda}_{k} ) = 0.\]
Since \( {\lambda}_{i}  \) for \( 1 \leq i \leq k   \) is distinct, we have that \( {\lambda}_{i-1} - \lambda_{i} \neq  0   \) for all \( 1 \leq i \leq k - 1  \). Consequently, this results in \( {a}_{i} = 0  \) for all \( 1 \leq i \leq k - 1 \) which leaves us with \( {a}_{k} {v}_{k} = 0  \). Since \( {v}_{k} \) is an eigenvector, we have \( {v}_{k} \neq  0  \) so \( {a}_{k} = 0  \). Thus, we have \( {a}_{1} = {a}_{2} = \cdots = {a}_{k-1} = {a}_{k} = 0  \) implies that \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{k} \}   \) is a linearly independent set.   
\end{proof}

\begin{corollary}
  Let \( T  \) be a linear operator on an \( n- \)dimensional  vector space \( V  \). If \( T  \) has \( n  \) distinct eigenvalues, then \( T  \) is diagonalizable.  
\end{corollary}
\begin{proof}
Suppose that \( T  \) has \( n  \) distinct eigenvalues \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{n} \). We can choose an eigenvalue \( {\lambda}_{i} \) for each corresponding eigenvector \( {v}_{i} \) for all \( i  \). Note that each \( {\lambda}_{i} \) is distinct. Using {\hyperref[Theorem 5.5]{Theorem 5.5}}, the set \( \{ {v}_{1}, \dots, {v}_{n} \}  \) is linearly independent. Since \( \text{dim}(V) = n  \), this set is a basis for \( V  \). Thus, \( T  \) is diagonalizable via Theorem 5.1. 
\end{proof}

\begin{eg}
   Let  
   \[  A = \begin{pmatrix} 
       1 & 1 \\
       1 & 1 
             \end{pmatrix}  \in {M}_{2 \times 2}(\R). \]
    The characteristic polynomial of \( A  \) (and hence of \( {L}_{A} \)) is
    \[  \text{det}(A - t I ) = \text{det} \begin{pmatrix} 
        1 - t & 1 \\
        1 & 1 - t 
              \end{pmatrix}  = t (t-2). \]
        We can see that the eigenvalues of \( {L}_{A} \) are \( 0  \) and \( 2 \). These eigenvalues correspond to the eigenvectors of \( {L}_{A} \) which form a basis such that \( A  \) is a diagonal matrix. Thus, \( {L}_{A} \) is a linear-operator that is diagonalizable (and hence \( A  \) is also diagonalizable).
\end{eg}

Note that it is not necessarily true that a diagonalizable linear operator contains \( n  \) distinct eigenvalues. A quick counter-example would be the identity operator. Even though \( I  \) is diagonalizable, it only contains one eigenvalue, namely, \( \lambda = 1  \).

This tells us that diagonalizability requires a much stronger condition on the characteristic polynomial.

\begin{definition}[Splits Over]
   A polynomial \( f(t)  \) in \( P(F)  \) \textbf{splits over} \( F  \) if there are scalars \( c, {a}_{1}, {a}_{2}, \dots, {a}_{n} \) (not necessarily distinct) in \( F  \) such that 
   \[  f(t) = c(t- {a}_{1})(t - {a}_{2})\cdots (t - {a}_{n}). \]
\end{definition}

The splitting behavior of a polynomial is different based on which field the polynomial is defined on. For example, we see that \( t^{2} - 1  \) splits over \( \R  \), but \( (t^{2} + 1)(t-2) \) does not since \( t^{2} + 1  \) has no solutions in the real line. However, \( t^{2} + 1  \) can further be split if it was defined over \( \C  \). In this case, \( (t^{2} +1)(t-2) \) does split over \( \C  \), namely, it splits into \( (t+i)(t-i)(t-2) \).

\begin{theorem}
   The characteristic polynomial of any diagonalizable linear operator splits.
\end{theorem}
\begin{proof}
Let \( T  \) be a diagonalizable linear operator on the \( n- \)dimensional vector space \( V  \), and let \( \beta \) be an ordered basis for \( V  \) such that \( [T]_{\beta} = D  \) is a diagonal matrix. Suppose that  
\[ \begin{pmatrix} 
{\lambda}_{1} & 0 & \cdots & 0 \\
0  & {\lambda}_{2} & \cdots & 0 \\
\vdots & \vdots &   & \vdots \\  
0 & 0 & \cdots & {\lambda}_{n}
\end{pmatrix}, \]
and let \( f(t)  \) be the characteristic polynomial of \( T  \). Then
\begin{align*} 
    f(t) = \text{det}(D - tI) = \text{det} \begin{pmatrix} 
{\lambda}_{1} - t  & 0 & \cdots & 0 \\
0  & {\lambda}_{2} - t & \cdots & 0 \\
\vdots & \vdots &   & \vdots \\  
0 & 0 & \cdots & {\lambda}_{n} - t
\end{pmatrix} \\  
= ({\lambda}_{1} - t)({\lambda}_{2} -t)\cdots ({\lambda}_{n} -t) = (-1)^{n}(t - {\lambda}_{1})(t - {\lambda}_{2})\cdots(t- {\lambda}_{n}). 
    \end{align*}
\end{proof}
\begin{itemize}
    \item If \( T  \) is a diagonalizable linear operator but fails to have distinct eigenvalues, then the characteristic polynomial of \( T  \) must have repeated zeros.
    \item The converse of the theorem above is not true since not every characteristic polynomial of a linear operator of \( T  \) guarantees that \( T  \) be diagonalizable.
\end{itemize}

\begin{definition}[Algebraic Multiplicity]
    Let \( \lambda  \) be an eigenvalue of a linear operator or matrix with characteristic polynomial \( f(t) \). The \textbf{(algebraic) multiplicity} of \( \lambda  \) is the largest positive integer \( k  \) for which \( (t- \lambda)^{k }  \) is a factor of \( f(t) \).
\end{definition}

Recall that a diagonalizable linear operator \( T  \) that is defined over a finite-dimensional vector space \( V  \) contains an ordered basis \( \beta \) for \( V \) consisting of eigenvectors of \( T  \). By Theorem 5.1, \( [T]_{\beta} \) is a diagonal matrix in which the diagonal entries are the eigenvalues of \( T  \). Remember that each eigenvalue of \( T  \) corresponds to the diagonal entry of \( [T]_{\beta} \) as many times as its multiplicity permits. 

We can investigate the exact amount of independent eigenvectors that are associated with a given eigenvalue. A way we can do this is to look at the null space of \( T - \lambda I  \).

\begin{definition}[Eigenspace]
    Let \( T  \) be a linear operator on a vector space \( V  \) ,and let \( \lambda  \) be an eigenvalue of \( T  \). Define \( {E}_{\lambda} = \{ x \in V : T(x) = \lambda x \}  =  N(T - \lambda {I}_{V}) \). The set \( {E}_{\lambda} \) is called the \textbf{eigenspace} of \( T  \) corresponding to the eigenvalue \( \lambda  \). Analogously, we define the \textbf{eigenspace} of a square matrix \( A  \) to be the eigenspace of \( {L}_{A} \).
\end{definition}

It can easily be proven that \( {E}_{\lambda} \) is a subspace of \( V  \). The maximum number of linearly independent eigenvectors that correspond to a given eigenvalue can therefore be seen by taking the dimension of the given eigenspace.

\begin{theorem}
    Let \( T  \) be a linear operator on a finite-dimensional vector space \( V  \), and let \( \lambda  \) be an eigenvalue of \( T  \) having multiplicity \(  m  \). Then \( 1 \leq \text{dim}({E}_{\lambda}) \leq  m \).
\end{theorem} 
\begin{proof}
    Choose an ordered basis \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{p} \}   \) for \( {E}_{\lambda} \) and extend this basis into \( \beta = \{ {v}_{1}, {v}_{2}, \dots ,{v}_{p}, {v}_{p+1}, \dots, {v}_{n} \}   \) for \( V  \), and let \( A = [T]_{\beta} \). Observe that \( {v}_{i} \ (1 \leq i \leq p ) \) is an eigenvector of \( T  \) corresponding to \( \lambda  \), and therefore 
    \[  A = \begin{pmatrix} 
        \lambda {I}_{p} & B \\
        O & C 
              \end{pmatrix}.   \]
By Exercise 21 of Section 4.3, the characteristic polynomial of \( T  \) is
\begin{align*}
    f(t) = \text{det}(A - t {I}_{n}) &= \text{det}\begin{pmatrix} 
        (\lambda - t){I}_{p} & B  \\
        O & C - t {I}_{n-p}
              \end{pmatrix}  \\
              &= \text{det}((\lambda -t){I}_{p}) \text{det}(C - {tI}_{n-p}) \\
              &= (\lambda - t)^{p} g(t)
\end{align*}
where \( g(t) = \text{det}(C - {tI}_{n-p}) \) is a polynomial. Thus \( (\lambda - t)^{p} \) is a factor of \( f(t)  \), and hence the multiplicity of \( \lambda  \) is at least \( p  \). However, \( \text{dim}({E}_{\lambda}) = p  \), and so \( \text{dim}({E}_{\lambda}) \leq m \).
\end{proof}

\begin{lemma}
    Let \( T  \) be a linear operator, and let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) be distinct eigenvalues of \( T  \). For each \( i = 1,2,\dots,k  \), and let \( {v}_{i} \in {E}_{{\lambda}_{i}}  \), the eigenspace corresponding to \( {\lambda}_{i} \). If
    \[  {v}_{1} + {v}_{2} + \cdots + {v}_{k} = 0, \]
    then \( {v}_{i} = 0  \) for all \( i \).
\end{lemma}

The last two examples illustrate that operators whose characteristic polynomial splits is diagonalizable if and only if the multiplicity of \( \lambda \) is equal to the dimension its corresponding eigenspace \( {E}_{\lambda} \).

\begin{proof}
Suppose otherwise. By renumbering if necessary, suppose that, for \( 1 \leq m \leq k  \), we have \( {v}_{i} \neq 0  \) for \( 1 \leq i \leq m \), and \( {v}_{i} = 0  \) for \( i > m  \). Then, for each \( i \leq m  \), \( {v}_{i} \) is an eigenvector of \( T \) corresponding to \( {\lambda}_{i} \) and  
\[ {v}_{1} + {v}_{2} + \cdots + {v}_{m} = 0. \]
But this contradicts Theorem 5.5, which states that all \( {v}_{i} \)'s are linearly independent. Thus, we should have \( {v}_{i} = 0  \) for all \( i \).
\end{proof}

\begin{theorem}
   Let \( T \) be a linear operator on a vector space \( V  \), and let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k } \) be distinct eigenvalues of \( T  \). For each \( i = 1,2, \dots, k  \), let \( {S}_{i} \) be a finite linearly independent subset of the eigenspace \( {E}_{{\lambda}_{i}}  \). Then \( S = {S}_{1} \cup {S}_{2} \cup \cdots \cup {S}_{k} \) is a linearly independent subset of \( V  \). 
\end{theorem}
\begin{proof}
Suppose that for each \( i  \) 
\[  {S}_{i} = \{ {v}_{i1}, {v}_{i2}, \dots, {v}_{i {n}_{i}} \}. \]
Then \( S = \{ {v}_{ij} : 1 \leq j \leq {n}_{i}, \ \text{and} \ 1 \leq i \leq k  \}. \) Consider any scalars \( \{ {a}_{ij} \}  \) such that 
\[  \sum_{ i=1 }^{ k  } \sum_{ j=1 }^{ {n}_{i} } {a}_{ij } {v}_{ij} = 0.  \]
For each \( i  \), let 
\end{proof}
