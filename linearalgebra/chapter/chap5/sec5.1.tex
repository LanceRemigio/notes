\section{Eigenvalues and Eigenvectors}

Our goal in this section is to find a basis \( \beta'  \) for which the matrix representation of a linear operator \( T   \) is a diagonal matrix.

\begin{definition}[Diagonalization]
    A linear operator \( T  \) on a finite-dimensional vector space \( V  \) is called \textbf{diagonalizable} if there is an ordered basis \( \beta  \) for \( V  \) such that \( [T]_{\beta} \) is a diagonal matrix. A square matrix \( A  \) is called \textbf{diagonalizable} if \( {L}_{A} \) is diagonalizable.
\end{definition}

Given a finite-dimensional vector space \( V  \), we can find an ordered basis \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) for \( V  \) such that the linear operator \( T \) acting on \( V  \) contains a matrix representation that is diagonal. If this is accomplished, then \( D = [T]_{\beta} \) is a diagonal matrix where for each \( {v}_{j} \in \beta \), we have
\[  T({v}_{j}) = \sum_{ i=1 }^{ n } {D}_{ij} {v}_{i} = {D}_{jj} = {\lambda}_{j} {v}_{j}. \]
Conversely, the ordered basis \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}   \) is an ordered basis for \( V  \) where \( T({v}_{j}) = {\lambda}_{j} {v}_{j} \) for some scalars \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{n}  \). Thus, the matrix representation is  
\[ [T]_{\beta} = \begin{pmatrix} 
    {\lambda}_{1} & 0 & \cdots & 0 \\ 
    0 & {\lambda}_{2} & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & {\lambda}_{n}
          \end{pmatrix}. \]

Note that each vector \( v \in \beta  \) satisfies the condition that \( T(v) = \lambda v  \) for some scalar \( \lambda \). Since \( v \in \beta  \), we also get that \( v \neq 0  \).

\begin{definition}[Eigenvectors and Eigenvalues]
    Let \( T  \) be a linear operator on a vector space \( V  \). A nonzero vector \( v \in V  \) is called an \textbf{eigenvector} of \( T  \) if there exists a scalar \( \lambda  \) such that \( T(v) = \lambda v  \). The scalar \( \lambda  \) is called the \textbf{eigenvalue} corresponding to the eigenvector \( v  \). 

    Let \( A \in {M}_{n \times n}(F)  \). A nonzero vector \( v \in F^{n} \) is called an \textbf{eigenvector} of \( A  \) if \( v  \) is an eigenvector of \( {L}_{A} \); that is, if \( Av = \lambda v  \) for some scalar \( \lambda \). The scalar \( \lambda  \) is called the \textbf{eigenvalue} of \( A  \) corresponding to the eigenvector \( v  \).
\end{definition}

\begin{theorem}
    A linear operator \( T  \) on a finite-dimensional vector space \( V  \) is diagonalizable if and only if there exists an ordered basis \( \beta  \) for \( V  \) consisting of eigenvectors of \( T  \). Furthermore, if \( T  \) is diagonalizable, \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) is an ordered basis of eigenvectors of \( T  \), and \( D = [T]_{\beta} \), then \( D  \) is a diagonal matrix and \( {D}_{jj}  \) is the eigenvalue corresponding to \( {v}_{j} \) for \( 1 \leq j \leq n \).
\end{theorem}

\begin{corollary}
    A matrix \( A \in {M}_{n \times n}(F) \) is diagonalizable if and only if there exists an ordered basis for \( F^{n} \) consisting of eigenvectors of \( A  \). Furthermore, if \( \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}   \) is an ordered basis for \( F^{n} \) consisting of eigenvectors of \( A  \) and \( Q  \) is the \( n \times n \) matrix whose \( j \)th column is \( {v}_{j} \) for \( j = 1,2, \dots, n,  \), then \( D = Q^{-1} A Q  \) is a diagonal matrix such that \( {D}_{jj }  \) is the eigenvalue of \( A  \) corresponding to \( {v}_{j} \). Hence, \( A  \) is diagonalizable if and only if it is similar to a diagonal matrix.  
\end{corollary}

\begin{eg}[Left-multiplication Operators]
    Let 
    \[  A = \begin{pmatrix} 
        1 & 3 \\
        4 & 2 
              \end{pmatrix}, \ \ {v}_{1} = \begin{pmatrix} 
        1 \\
        -1 
    \end{pmatrix}, \ \ \text{and} \ \ {v}_{2} = \begin{pmatrix} 
               3 \\
               4
              \end{pmatrix}. \]
    Observe that
        \[ {L}_{A}({v}_{1}) = \begin{pmatrix} 
            1 & 3 \\
            4 & 2 
                  \end{pmatrix} \begin{pmatrix} 
                             1 \\
                             -1 
                            \end{pmatrix}  = \begin{pmatrix} 
                                       -2 \\
                                       2 
                                      \end{pmatrix}  = -2 \begin{pmatrix} 
                                                 1 \\
                                                 -1 
                                                \end{pmatrix} = -2 {v}_{1}. \] 
    Thus, we have that \( {v}_{1} \) is an eigenvector of \( {L}_{A} \), and thus of \( A  \) as well. Using the left-multiplication transformation again, we can write
    \[  {L}_{A}({v}_{1}) = \begin{pmatrix} 
        1 & 3 \\
        4 & 2 
              \end{pmatrix} \begin{pmatrix} 
                         3 \\
                         4
                        \end{pmatrix}  = \begin{pmatrix} 
                                   15 \\
                                   20 
                                  \end{pmatrix} = 5 \begin{pmatrix} 
                                             3 \\
                                             4 
                                            \end{pmatrix}  = 5 {v}_{2}. \] This means that \( {v}_{2} \) is an eigenvector of \( {L}_{A} \), and thus is an eigenvector of \( A  \). We can see that \( \beta = \{ {v}_{1}, {v}_{2} \}  \) is an ordered basis for \( \R^{2} \) consisting of eigenvectors of both \( A  \) and \( {L}_{A} \). By the Corollary above, we can state that \( A  \) and \( {L}_{A} \) are diagonalizable. Using Theorem 5.1 and its corollary, if  
                    \[ Q = \begin{pmatrix} 
                        1 & 3 \\
                        -1 & 4 
                              \end{pmatrix},   \]
                              then 
            \[  Q^{-1} A Q = [{L}_{A}]_{\beta} = \begin{pmatrix} 
                -2 & 0 \\
                0 & 5 
                      \end{pmatrix}. \]
\end{eg}

\begin{theorem}[Eigenvalues and determinants]
    Let \( A \in {M}_{n \times n}(F) \). Then a scalar \( \lambda  \) is an eigenvalue of \( A  \) if and only if \( \text{det}(A - \lambda {I}_{n}) = 0  \).
\end{theorem}
\begin{proof}
A scalar \( \lambda  \) is an eigenvalue if and only if there exists a nonzero \( v \in F^{n} \) such that \( {L}_{A}(v) = Av = \lambda v  \). This is true if an only if
\[  (A - \lambda {I}_{n})(v) = 0.  \]
This is also true if and only if 
\[  A - \lambda {I}_{n} = 0 \tag{1}  \] where \( 0   \) is the zero matrix. Since the \( 0  \) matrix is not invertible, we know that the matrix on the left side of (1) has
\[  \text{det}(A - \lambda {I}_{n}) = 0. \] Observe that the converse of this argument proves the other direction. Hence, we are done.
\end{proof}

\begin{definition}[Characteristic Polynomial of a Matrix]
    Let \( A \in {M}_{n \times n}(F) \). The polynomial \( f(t) = \text{det}(A - t {I}_{n})   \) is called the \textbf{characteristic polynomial} of \( A  \).
\end{definition}
\begin{remark}
    We can see that the eigenvalues of a matrix are the roots of its characteristic polynomial. 
\end{remark}

\begin{definition}[Characteristic Polynomial and Determinant of a Transformation]
    Let \( T  \) be a linear operator on a finite-dimensional vector space \( V  \). Choose any ordered basis \( \beta  \) for \( V  \). We define the \textbf{determinant} of \( T  \), denoted \( \text{det}(T) \), to be the determinant of \( A = [T]_{\beta}  \), and the \textbf{characteristic} polynomial \( f(t) \) of \( T  \) to be the characteristic polynomial of \( A  \). That is, 
    \[  f(t) = \text{det}(A - t {I}_{n}). \]
\end{definition}

\begin{theorem}
   Let \( A \in {M}_{n \times n}(F)  \).  
   \begin{enumerate}
       \item[(a)] The characteristic polynomial of \( A  \) is a polynomial of degree \( n  \) with leading coefficient \( (-1)^{n} \).
        \item[(b)] A has at most \( n  \) distinct eigenvalues.
   \end{enumerate}
\end{theorem}
\begin{proof}

\end{proof}

\begin{theorem}[Determining Eigenvectors via Eigenvalues]
    Let \( T  \) be a linear operator on a vector space \( V  \), and let \( \lambda  \) be an eigenvalue of \( T  \). A vector \( v \in V  \) is an eigenvector of \( T  \) corresponding to \( \lambda  \) if and only if \( v \neq 0  \) and \( v \in N(T - \lambda I ) \).
\end{theorem}
\begin{proof}
Let \( v \in V  \) be an eigenvector of \( T  \) corresponding to \( \lambda  \). Since \( \lambda  \) is an eigenvalue of \( T \), we know that \( T(v) = \lambda v  \) where \( v \neq 0  \). Hence,  
\begin{align*}
    T(v) = \lambda v &\iff T(v) - \lambda v = 0  \\
                     &\iff (T - \lambda I)(v) = 0. 
\end{align*}
Note that this is true if and only if \( v \in N(T - \lambda I)  \). We can reverse this argument to show that \( v \in V  \) is an eigenvector of \( T  \) corresponding to \( \lambda  \).
\end{proof}

\begin{eg}
    It can be easily shown that the eigenvalues of 
   \[  A = \begin{pmatrix} 
       1 & 1 \\
       4 & 1
             \end{pmatrix}  \] 
             is \( {\lambda}_{1} = 3  \) and \( {\lambda}_{2} = - 1 \). Let 
             \[  {B}_{1} = A - {\lambda}_{1} I = \begin{pmatrix} 
                 1 & 1 \\
                 4 & 1 
                       \end{pmatrix}  - \begin{pmatrix} 
                 3 & 0 \\
                 0 & 3 
                                 \end{pmatrix} = \begin{pmatrix} 
                 -2 & 1 \\
                 4 & -2 
                                           \end{pmatrix}. \]
                To find the associated eigenvector with the eigenvalue \( {\lambda}_{1} = 3  \), we need to find a given nonzero 
                \[  x = \begin{pmatrix} 
                           {x}_{1} \\
                           {x}_{2}
                          \end{pmatrix}  \in \R^{2} \]
                          such that \( x \in N(T- 3I) \). This can be done setting
                          \[  \begin{pmatrix} 
                              -2 & 1 \\
                              4 & -2 
                                    \end{pmatrix} \begin{pmatrix} 
                                               {x}_{1} \\
                                               {x}_{2}
                                              \end{pmatrix} = \begin{pmatrix} 
                              {-2x}_{1}+ {x}_{2} \\
                              {4x}_{1}- {2x}_{2}
                                                        \end{pmatrix} = \begin{pmatrix} 
                                                                   0 \\
                                                                   0
                                                                  \end{pmatrix} \]
            for which all we need to do is solve the following system of equations
            \begin{align*}
                {-2x}_{1} + {x}_{2} &= 0  \\
                {4x}_{1} - {2x}_{2} &= 0 
            \end{align*}
            for which it can be easily seen that \( {x}_{1} = 1  \) and \( {x}_{2} = 2  \). Thus, the set of all solutions for the above can be represented by
            \[  \Big\{ t \begin{pmatrix} 
                       1 \\
                       2 
                      \end{pmatrix} : t \in \R \Big\}. \]
        Hence, we have that \( x  \) is an eigenvector corresponding to \( {\lambda}_{1} = 3  \) if and only if
        \[  x = t \begin{pmatrix} 
                   1 \\
                   2 
               \end{pmatrix} \ \ \text{for some} t \neq 0. \]
\end{eg}



