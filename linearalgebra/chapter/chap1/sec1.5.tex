\section{Linear Dependence and Linear Independence}

\subsection{Motivation}
Suppose \( V  \) is a vector space over a field \( F  \) and that \( W \subseteq V  \) is a subspace of \( V  \). 
\begin{itemize}
    \item Suppose we have a set \( S  \) that will generate every vector in \( V  \) as a linear combination of vectors in \( S  \).
    \item It is desirable to find a subset of \( S  \) that is as small as possible or rather just enough to generate each vector in \( V  \).
    \item The reason for this is to reduce the amount of computations done to represent a vector in \( V  \). 
    \item Finding this subset is a matter of expressing one of the vectors in the spanning set as a linear combination of the other vectors in \( S  \).
    \item This will naturally lead to a system of linear equations for which we have to solve for the coefficients.
    \item The answer to the question is whether this system of equations leads to a solution or not.
    \item An easier way to answer this question is to find coefficients (not all zero) for which the zero vector can be written as a linear combination of vectors in \( S  \).
    \item Doing this, guarantees that we can write any one of the vectors in \( S  \) as a linear combination of the others.
\end{itemize}


\begin{definition}[Linearly Dependence]
   A subset \( S  \) of a vector space \( V  \) is called \textbf{linearly dependent} if there exists a finite number of distinct vectors in \( u_{1}, u_{2} , \dots , u_{n} \in S  \) and scalars \( a_{1}, a_{2}, \dots, a_{n}  \) not all zero, such that  
   \[  a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n} \]
   In this case, we also say that the vectors of \( S  \) are linearly dependent.
\end{definition}

\begin{itemize}
    \item The \textbf{trivial representation} of \( 0  \) is a linear combination of scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) and distinct vectors \( v_{1}, v_{2}, \dots, v_{n}   \in S   \) where for all \( 1 \leq i \leq n  \), we have \( a_{i} = 0  \). 
    \item This tells us that our definition of \textbf{Linear Dependence} implies that \( 0  \) is a non-trivial linear combination.
    \item Any subset of \( V  \) that contains the zero vector is subsequently linearly dependent since \(  0  \) can be written as non-trivial representation; that is, \( 1 \cdot 0 = 0  \).
\end{itemize}

\subsection{Examples of Linearly Dependent Sets}

\begin{eg}
   Consider a subset in \( \R^{4} \) defined by 
   \[  S = \{ (1,3,-4,2), (2,2,-4,0) , (1, -3, 2 ,-4), (-1, 0, 1, 0) \}.\] To show that \( S \) is a linear dependent set, we need to find scalars \( a_{1}, a_{2}, a_{3}, a_{4} \in \R  \) such that \( a_{i} \) for all \( 1 \leq i \leq 4 \) not all zero such that  
   \[  a_{1} (1,3,-3,2) + a_{2} (2,2,-4,0) + a_{3} (1,-3,2,-4) + a_{4} (-1 , 0, 1, 0) = 0.  \]
    We can represent this as a system of linear equations with each equation being set equal to zero and solving for the scalars. Solving this system of equations leads to the coefficients \( a_{1} = 4 , a_{2} = -3 , a_{3} = 2,   \) and \( a_{4} = 0  \). Thus, we have that \( S  \) is linearly dependent subset of \( \R^{4} \) and hence we can write any vector in \( S  \) as a linear combination of the other vectors contained in \( S  \).
\end{eg}

\begin{eg}
    Define a subset of \( M_{2 \times 2}(\R) \)
    \[ S = \Bigg\{ \begin{pmatrix}
            1 & - 3 & 2 \\
            -4 & 0 & 5 
    \end{pmatrix} \}, \begin{pmatrix}
            - 3 & 7 & 4 \\
            6 & -2 & -7
    \end{pmatrix}, \begin{pmatrix}
            -2 & 3 & 11 \\
            -1 & -3 & 2 
    \end{pmatrix}  \Bigg\}.  \]
    This set is \textbf{linearly dependent} because we can find coefficients \( a_{1} , a_{2}, a_{3}  \) such that \( a_{1} = 5 , a_{2} = 3 ,  \) and \( a_{3} = - 2  \) where 
    \begin{align*}
        5 \begin{pmatrix}
            1 & -3 & 2 \\
            -4 & 0 & 5 
        \end{pmatrix} + 3 \begin{pmatrix}
            -3 & 7 & 4 \\
            6 & -2 & -7
        \end{pmatrix} - 2 \begin{pmatrix}
            -2 & 3 & 11 \\
            -1 & -3 & 2 
        \end{pmatrix} = \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 
        \end{pmatrix}  \\
    \end{align*}
\end{eg}

\begin{definition}[Linear Independence]
   A subset \( S  \) of a vector space that is not linearly dependent is called \textbf{linearly independent}. As before, we also say that the vectors of \( S  \) are linearly independent. 
\end{definition}

Whereas linear dependence requires solutions that are not all zero, linear independence \textbf{requires} all the solutions to be zero. This provides us an easy way to determine if a finite set is linearly independent.

\subsection{Examples of Linear Independent Sets}

Some facts about linearly independent sets include:
\begin{itemize}
    \item The empty set is linearly independent since we don't have any vectors to take linear combinations of.
    \item A set consisting of only one non-zero vector; that is, \( \{ v  \}  \) is linearly independent.
    \item If \( \{ v  \}  \) is linearly dependent, then it the singleton has to be the zero vector \( 0 \). This is because \( au = 0  \) 
        \[  u = 1 \cdot u =  (a^{-1} a ) u = a^{-1} (au) = a^{-1} \cdot 0 = 0.    \]
\end{itemize}

\begin{eg}
     It can be shown that the set
    \[  S = \{ (1,0,0,-1) , (0,1,0,-1) , (0,0,1,-1), (0,0,0,1) \}  \] is linearly independent by showing that all the coefficients of the linear combination of 0 represent the trivial representation; that is, we have scalars \( a_{1} = a_{2} = a_{3} = a_{4} = 0  \).
\end{eg}


