\section{Linear Dependence and Linear Independence}

\subsection{Motivation}
Suppose \( V  \) is a vector space over a field \( F  \) and that \( W \subseteq V  \) is a subspace of \( V  \). 
\begin{itemize}
    \item Suppose we have a set \( S  \) that will generate every vector in \( V  \) as a linear combination of vectors in \( S  \).
    \item It is desirable to find a subset of \( S  \) that is as small as possible or rather just enough to generate each vector in \( V  \).
    \item The reason for this is to reduce the amount of computations done to represent a vector in \( V  \). 
    \item Finding this subset is a matter of expressing one of the vectors in the spanning set as a linear combination of the other vectors in \( S  \).
    \item This will naturally lead to a system of linear equations for which we have to solve for the coefficients.
    \item The answer to the question is whether this system of equations leads to a solution or not.
    \item An easier way to answer this question is to find coefficients (not all zero) for which the zero vector can be written as a linear combination of vectors in \( S  \).
    \item Doing this, guarantees that we can write any one of the vectors in \( S  \) as a linear combination of the others.
\end{itemize}


\begin{definition}[Linearly Dependence]\label{linear dependence}
   A subset \( S  \) of a vector space \( V  \) is called \textbf{linearly dependent} if there exists a finite number of distinct vectors in \( u_{1}, u_{2} , \dots , u_{n} \in S  \) and scalars \( a_{1}, a_{2}, \dots, a_{n}  \) not all zero, such that  
   \[  a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n} = 0.  \]
   In this case, we also say that the vectors of \( S  \) are linearly dependent.
\end{definition}

\begin{itemize}
    \item The \textbf{trivial representation} of \( 0  \) is a linear combination of scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) and distinct vectors \( v_{1}, v_{2}, \dots, v_{n}   \in S   \) where for all \( 1 \leq i \leq n  \), we have \( a_{i} = 0  \). 
    \item This tells us that our definition of {\hyperref[linear dependence]{\textbf{linear dependence}}} implies that \( 0  \) is a non-trivial linear combination.
    \item Any subset of \( V  \) that contains the zero vector is subsequently linearly dependent since \(  0  \) can be written as non-trivial representation; that is, \( 1 \cdot 0 = 0  \).
\end{itemize}

\subsection{Examples of Linearly Dependent Sets}

\begin{eg}
   Consider a subset in \( \R^{4} \) defined by 
   \[  S = \{ (1,3,-4,2), (2,2,-4,0) , (1, -3, 2 ,-4), (-1, 0, 1, 0) \}.\] To show that \( S \) is a linear dependent set, we need to find scalars \( a_{1}, a_{2}, a_{3}, a_{4} \in \R  \) such that \( a_{i} \) for all \( 1 \leq i \leq 4 \) not all zero such that  
   \[  a_{1} (1,3,-3,2) + a_{2} (2,2,-4,0) + a_{3} (1,-3,2,-4) + a_{4} (-1 , 0, 1, 0) = 0.  \]
    We can represent this as a system of linear equations with each equation being set equal to zero and solving for the scalars. Solving this system of equations leads to the coefficients \( a_{1} = 4 , a_{2} = -3 , a_{3} = 2,   \) and \( a_{4} = 0  \). Thus, we have that \( S  \) is linearly dependent subset of \( \R^{4} \) and hence we can write any vector in \( S  \) as a linear combination of the other vectors contained in \( S  \).
\end{eg}

\begin{eg}
    Define a subset of \( M_{2 \times 2}(\R) \)
    \[ S = \Bigg\{ \begin{pmatrix}
            1 & - 3 & 2 \\
            -4 & 0 & 5 
    \end{pmatrix} \}, \begin{pmatrix}
            - 3 & 7 & 4 \\
            6 & -2 & -7
    \end{pmatrix}, \begin{pmatrix}
            -2 & 3 & 11 \\
            -1 & -3 & 2 
    \end{pmatrix}  \Bigg\}.  \]
    This set is \textbf{linearly dependent} because we can find coefficients \( a_{1} , a_{2}, a_{3}  \) such that \( a_{1} = 5 , a_{2} = 3 ,  \) and \( a_{3} = - 2  \) where 
    \begin{align*}
        5 \begin{pmatrix}
            1 & -3 & 2 \\
            -4 & 0 & 5 
        \end{pmatrix} + 3 \begin{pmatrix}
            -3 & 7 & 4 \\
            6 & -2 & -7
        \end{pmatrix} - 2 \begin{pmatrix}
            -2 & 3 & 11 \\
            -1 & -3 & 2 
        \end{pmatrix} = \begin{pmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 
        \end{pmatrix}  \\
    \end{align*}
\end{eg}

\begin{definition}[Linear Independence]\label{linear independence}
   A subset \( S  \) of a vector space that is not linearly dependent is called \textbf{linearly independent}. As before, we also say that the vectors of \( S  \) are linearly independent. 
\end{definition}

Whereas linear dependence requires solutions that are not all zero, linear independence \textbf{requires} all the solutions to be zero. This provides us an easy way to determine if a finite set is linearly independent.

\subsection{Examples of Linear Independent Sets}

Some facts about linearly independent sets include:
\begin{itemize}
    \item The empty set is linearly independent since we don't have any vectors to take linear combinations of.
    \item A set consisting of only one non-zero vector; that is, \( \{ v  \}  \) is linearly independent.
    \item If \( \{ v  \}  \) is linearly dependent, then it the singleton has to be the zero vector \( 0 \). This is because \( au = 0  \) 
        \[  u = 1 \cdot u =  (a^{-1} a ) u = a^{-1} (au) = a^{-1} \cdot 0 = 0.    \]
\end{itemize}

\begin{eg}[A Set of Vectors in \( \R^4 \)]\label{A set of vectors in R4}
     It can be shown that the set
    \[  S = \{ (1,0,0,-1) , (0,1,0,-1) , (0,0,1,-1), (0,0,0,1) \}  \] is linearly independent by showing that all the coefficients of the linear combination of 0 represent the trivial representation; that is, we have scalars \( a_{1} = a_{2} = a_{3} = a_{4} = 0  \).
\end{eg}

\begin{eg}\label{Linearly Independent Set of Polys}
    For \( k = 0,1, \dots, n  \), let \( p_{k} = x^{k } + x^{k+1}  + \cdots + x^{n}  \). The set 
    \[  \{ p_{0}(x), p_{1}(x), \dots p_{n}(x) \}  \] is linearly independent in \( P_{n}(F) \).
    It can be shown that for some scalars \( a_{0}, a_{1} , \dots , a_{n} \in F   \),  the following equation 
    \[  a_{0} + (a_{0} + a_{1} + a_{2})x^{2} + \cdots + (a_{0} + a_{1} + \cdots + a_{n})x^{n} = 0  \]
    has the trivial-representation; that is, \( a_{i} = 0  \) for all \( 1 \leq i \leq n \).
\end{eg}

\begin{theorem}
    Let \( V  \) be a vector space, and let \( S_{1} \subseteq S_{2} \subseteq V  \). If \( S_{1}  \) is linearly dependent, then \( S_{2}  \) is linearly dependent.
\end{theorem}
\begin{proof}
Let \( V  \) be a vector space. Suppose \( S_{1}  \) is linearly dependent. Then there exists a finite number of distinct vectors \( v_{1}, v_{2}, \dots, v_{n} \in S_{1}  \) and scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) not all zero such that 
\[  a_{1} v_{1} + a_{2} v_{2} + \cdots + a_{n} v_{n} = 0.  \]
Since \( S_{1} \subseteq S_{2}  \), we must have \( v_{1} , v_{2}, \dots, v_{n} \in S_{2} \) as well. If these vectors are linearly dependent, then these vectors are also linearly dependent in \( S_{2} \); that is, we have scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) not all zero such that  
\[  a_{1} v_{1} + a_{2} v_{2} + \cdots + a_{n} v_{n} = 0.  \]
Hence, \( S_{2}  \) is linearly dependent.
\end{proof}

\begin{corollary}\label{Corollary to Theorem 1.6}
   Let \( V  \) be a vector space, and let \( S_{1} \subseteq S_{2} \subseteq V  \). If \( S_{2}  \) is linearly independent, then \( S_{1}  \) is linearly independent. 
\end{corollary}
\begin{proof}
    Note that this corollary is just the contrapositive of the theorem before it. Hence, \( S_{1}  \) is linearly independent.
\end{proof}

\begin{itemize}
    \item Determining whether there exists a minimal generating set for the span of \( S  \) is related to solving the problem of finding whether some vector in \( S  \) that can be written in terms of a linear combination of other vectors in \( S  \). 
    \item We can see that in a given subset of \( \R^{3}  \) defined by 
        \[  S = \{ u_{1}, u_{2}, u_{3}, u_{4}  \}  \] where \( u_{1} = (2,-1,4) , u_{2} = (1,-1, 3), u_{3} = (1,1,-1) \), and \( u_{4} = (1,-2,1) \). Note that this \( S  \) is linearly dependent. This tells us that any vector in \( S  \) can be written as a linear combination of the others. Suppose we pick \( u_{3}  \) and write as a linear combination of the vectors \( u_{1}, u_{2},  \) and \( u_{4} \). We would find that the span of these three vectors (denote this set as \( S' \)) generates the same spanning set \( S  \) but with a cardinality one less than \( S  \); that is, \( \text{span}(S) = \text{span}(S') \). 
    \item If we find that there does not exist a proper subset that is equivalent to the span of \( S  \), then \( S  \) must be a linearly independent set.
\end{itemize}

\begin{theorem}\label{Theorem 1.7}

    Let \( S  \) be a linearly independent subset of a vector space \( V  \), and let \( v \in V  \) but not in \( S  \). Then \( S \cup \{ v \}  \) is linearly dependent if and only if \( v \in \text{span}(S) \).
\end{theorem} 
\begin{proof}
    ( \( \Rightarrow \) ) Suppose \( S \cup \{ v \}  \) is a linearly dependent set. Then there exists a finite number of scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) and vectors \( u_{1} , u_{2} , \dots, u_{n} \in S \cup \{ v  \}  \) such that 
\[  a_{1} u_{1} + a_{2} u_{2} + \dots + a_{n} u_{n} = 0. \]
Since \( S  \) is a linearly independent set, we must have \( u_{i} = v   \) for some \( 1 \leq i \leq n  \). Choose \( i = 1  \) (any choice of \( i \) will do) such that 
\[ a_{1} v + a_{2} u_{2} + \cdots + a_{n} u_{n} = 0.  \]
Solving for \( v  \) by subtracting \( a_{1} v   \) on both sides, multiplying by \( a^{-1}_{1}  \) on both sides of the equation, and distributing by \( a^{-1}_{1} \) yields the following:
\[  v = (- a_{1}^{-1} a_{2})u_{2} + (- a_{1}^{-1} a_{3}) u_{3} + \cdots + (- a^{-1}_{1} a_{n}  ) u_{n}.     \]
Since \( v  \) is a linear combination of vectors \( u_{1} , u_{2}, \dots, u_{n} \in S  \), we know that \( v \in \text{span}(S) \). 


    ( \( \Leftarrow  \) ) Conversely, suppose \( v \in \text{span}(S) \). This implies that there exists a finite amount of scalars \( a_{1} , a_{2} , \dots, a_{n} \in F  \) and \( u_{1} , u_{2} , \dots, u_{n} \in S  \) such that   
    \[  v = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n}. \]
    Subtracting \( v  \) from both sides of the equation above gives us the following equation:
    \[  a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n} - v = 0.  \]
    Note that \( v  \) is not contained in \( S  \), so \( v \neq u_{i}  \) for all \( 1 \leq i \leq n \). Since \( S  \) is a linearly independent set, we know that \( a_{i} = 0  \) for all \( 1 \leq i \leq n \). This implies that the only coefficient that is non-zero is with \( -v = -1v  \). Hence, the set of vectors \( S' = \{ u_{1}, u_{2} , \dots , u_{n} , v  \}  \) is linearly dependent. Since \( S' \subseteq S \cup \{ v \}  \), we know that \( S \cup \{ v \}  \) is also linearly independent by Theorem 6. 
\end{proof}



