\section{Bases And Dimension}

\subsection{Bases}

\begin{itemize}
    \item Recall that \( S  \) is a generating set for a subspace \( W  \) and no proper subset of \( S  \) is a generating set for \( W  \), then \( S  \) must be linearly independent. 
    \item Linearly independent sets possess the unique property that every vector that its spanning set generates is unique.
    \item This is property is what allows generating sets to be the building blocks of vector spaces.
\end{itemize}

\begin{definition}[Basis]
    A \textbf{basis} \( \beta \) for a vector space \( V  \) is linearly independent subset of \( V  \) that generates \( V  \). If \( \beta  \) is a basis for \( V  \), we also say that the vectors of \( \beta \) form a basis for \( V  \).
\end{definition}

\begin{eg}
    \begin{itemize}
        \item     Recall that the empty set \( \emptyset \) is linearly independent and that \( \text{span}(\emptyset) = \{ 0 \}  \). The empty set \( \emptyset \) in this case is the basis for the zero vector space.
        \item Note that in \( F^{n}  \), the vectors \( e_{1} = (1,0, \dots, 0 ) \), \( e_{2} = (0,1,0, \dots, 0 ) \dots, e_{n} = (0,0, \dots, 0, 1 ) \) form a basis for \( F^{n} \).
        \item The basis for \( M_{m \times n}(F ) \) is the set of matrices \( E^{ij} \) such that the only nonzero entry is a 1 in the \( i \)th and \( j \)th column.
        \item As we have seen in the last section, the set \( \{ 1, x , x^{2}, \dots, x^{n} \}  \) is a basis for \( P_{n}(F) \).
        \item In \( P(F) \), the set \( \{ 1,x ,x,  x^{2}, \dots \}  \) is a basis. \textit{Bases are not limited to finite sets. They can be infinite.} 
\end{itemize}
\end{eg}

\begin{theorem}[ ]
    Let \( V  \) be a vector space and \( u_{1}, u_{2}, \dots, u_{n}  \) be distinct vectors in \( V  \). Then \( \beta = \{ u_{1}, u_{2},\dots , u_{n}  \}   \) is a basis for \( V  \) if and only if each \( v \in V  \) can be unique expressed as a linear combination of vectors in \( \beta \), that is, expressed in the form 
    \[  v = a_{1} v_{1} + a_{2} v_{2} + \cdots + a_{n} v_{n}  \] for unique scalars \( a_{1}, a_{2}, \dots, a_{n}  \).
\end{theorem}
\begin{proof}
    (\( \Rightarrow \)) Suppose \( \beta = \{ u_{1}, u_{2}, \dots, u_{n} \}  \) is a basis for \( V  \). Then \( \text{span}(\beta) = V  \). If \( v \in V  \), then \( v \in \text{span}(\beta) \). Hence, we can write \( v  \) as a linear combination of vectors in \( \beta   \) such that choosing scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) leads to 
\[ v = \sum_{ i=1 }^{ n } a_{i} u_{i}.   \]
Suppose there exists another representation of \( v \in V  \) such that 
\[  v = \sum_{ i=1 }^{ n  } b_{i} x_{i} \]
Hence, observe that 
\begin{align*}  &\sum_{ i=1  }^{ n } a_{i} x_{i} = \sum_{ i=1 }^{ n } b_{i} y_{i}  \\
    &\implies \sum_{ i=1 }^{ n } (a_{i} - b_{i}) x_{i} = 0.
\end{align*}
Since \( \beta \) is linearly independent, we know that \( a_{i} - b_{i} = 0  \) which implies \( a_{i} = b_{i}  \) for all \( 1 \leq i \leq n \). Hence, \( v  \) can be expressed as a unique linear combination of vectors in \( \beta \).

(\( \Leftarrow \)) Conversely, let \( v \in V  \) be expressed as a unique linear combination of vectors in \( \beta \) such that 
\[  v = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n} \tag{1}  \]
for unique scalars \( a_{1}, a_{2}, \dots , a_{n}  \). We want to show that \( \beta   \) is a basis for \( V  \); that is, we want to show that \( \beta \) is a spanning set for \( V  \) and \( \beta \) is linearly independent. To show that \( \beta \) is a spanning set for \( V  \), we need to show that \( \text{span}(\beta) \subseteq V  \) and \( V \subseteq \text{span}(\beta) \). Note that \( \beta \subseteq V  \) and \( \text{span}(\beta) \) is a subspace for \( V  \). Hence, \( \text{span}(\beta) \subseteq V  \). On the other hand, \( V \subseteq \text{span}(\beta) \) follows immediately from (1). To show that \( \beta \) is linearly independent, we need to show that for scalars \( \delta_{1}, \delta_{2}, \dots, \delta_{n}  \), we have
\[ \delta_{1} u_{1} + \delta_{2}u_{2} + \cdots + \delta_{n} u_{n} = 0   \] such that  \( u_{i} = 0   \) for all \( 1 \leq i \leq n \). Note that \( v + 0 = v  \). Hence, we can write
\begin{align*}
    \sum_{ i=1 }^{ n } a_{i} u_{i}  + \sum_{ i=1 }^{ n } \delta_{i} u_{i}    &=  \sum_{ i=1 }^{ n } a_{i} u_{i} \\
    \sum_{ i=1 }^{ n } (a_{i} + \delta_{i}) u_{i} &= \sum_{ i=1 }^{ n } a_{i} u_{i} \\
\end{align*} Equating each term in the equation above yields the following equality:
\[ a_{i} + \delta_{i} = a_{i} \implies \delta_{i} = 0    \]
for all \( 1 \leq  i \leq n  \). But this tells us that \( \beta \) is linearly independent and we are done.
\end{proof}

\begin{itemize}
    \item Any vector \(  v \in V  \) can be written as a linear combination of vectors from the basis containing \( u_{1}, u_{2}, \dots, u_{n} \in V  \). 
    \item This determines a unique \( n  \)-tuple of scalars \( (a_{1}, a_{2}, \dots, a_{n}) \) and conversely, each \( n \)-tuple of scalars determines  a unique vector \( v \in V   \) such that each coefficient from the linear combination of \( u_{1}, u_{2}, \dots, u_{n}  \) is an entry from said tuple.
    \item For example, in our vector space \( F^{n} \), \( n  \) is the number of vectors that should be in the basis for \( F^{n} \) which is indeed the case.
    \item In this book, we are only concerned with finite bases.
\end{itemize}

\begin{theorem}[Finite Spanning Set For a Vector Space]\label{Theorem 1.9}
   If a vector space \( V  \) is generated by a finite set \( S  \), then some subset of \( S  \) is a basis for \( V  \). Hence, \( V  \) has a finite basis. 
\end{theorem}
\begin{proof}
    Suppose \( S = \emptyset \) or \( S = \{ 0 \}  \), then \( V = \{  0  \}  \) and \( \emptyset \) is a subset of \( S  \) that is a basis for \( V  \). If \( S  \) neither of these choices, then \( S  \) must contain at least one nonzero vector \( u_{1}  \); that is, \( S = \{ u_{1} \}  \). Since \( u_{1} \) is nonzero, it follows that \( S  \) is a linearly independent set. We can continue this process of adding vectors \( u_{2}, \dots, u_{k } \) into \( S  \) such that \( S  \) is a linearly independent set of \( k  \) vectors. Since \( S  \) is a finite set, we must end with the linearly independent set \( \beta = \{ u_{1} , u_{2}, \dots, u_{n} \}  \). There are two cases for which this occurs, either \( \beta = S  \) or \( \beta \subseteq S  \): 
    \begin{enumerate}
        \item[(i)] Suppose that \( \beta = S  \) (remember that \( \beta \) is a finite set by construction). Then we have \( S  \) is a linearly independent set and spanning set for \( V  \) (since \( S  \) is a finite set that generates \( V  \)). Hence, \( S  \) is a finite basis for \( V  \).
        \item[(ii)] Suppose  \( \beta \subseteq S  \) is a linearly independent set such that adding \( v \in  S  \) where \( v \notin \beta  \) makes a linearly dependent set. We claim that \( \beta  \) is the desired subset of \( S  \) that is a basis for \( V  \). Then we have two cases; that is, either \( v \in \beta  \) or \( v \notin \beta \). Since  \( \beta \subseteq S   \), we know by { \hyperref[Theorem 1.5]{Theorem 1.5} } that \( \text{span}(\beta) \subseteq S  \) (This applies for both cases). It suffices to show that \( S \subseteq \text{span}(\beta) \). If \( v \in \beta  \), then surely \( v \in \text{span}(\beta) \). Hence, we have \( S \subseteq \text{span}(\beta) \).  Suppose \( v \notin \beta  \). Since \( \beta  \) is a linearly independent set, then by { \hyperref[Theorem  1.7]{Theorem 1.7} }, we have that \( \beta \cup \{ v  \}  \) being linearly dependent implies that \( v \in \text{span}(\beta) \). Hence, \( S \subseteq \text{span}(\beta) \). Thus, that both cases implies that \( \beta \) is a spanning set for \( V  \).    
    \end{enumerate}
\end{proof}

This theorem tells us that any spanning set of a vector space \( V  \) can reduced to a finite basis for \( V  \). This is illustrated in the following examples.


\begin{eg}
    Define 
    \[  S = \{ (2,-3,5), (8,-12, 20), (1,0,-2), (0,2,-1), (7,2,0) \} \]
    which can be shown to generate \( \R^{3} \). The idea is to create a proper subset of \( S  \) such that none of the vectors in \( \beta \) are a multiples of each other. In other words, we want a set that is linearly independent; that is, our choices of vectors determine whether a set will be linearly dependent or independent (we want the latter to hold). Say, we pick \( (2,-3,5 ) \) as our first vector in our subset \( \beta \). Right away, we can exclude \( (8,-12,20) \) since it is a multiple of \( (2,-3,5) \). Otherwise, including it would make \( \beta \) linearly dependent (see exercise 9 from section 1.5). Next, add the vectors \( (1,0,-2) \) and \( (0,2,-1) \) since they are not multiples of the other. Narrowing down our set to 
    \[  \beta = \{ (2,-3,5), (1,0,-2), (0,2,-1) \}.  \]
    Note that adding the vector \( (7,2,0) \) makes \( \beta \) linearly dependent, so \( (7,2,0) \) is excluded from the list. Hence, we have arrived at a subset of \( S  \) such that \( \beta \subseteq S  \) is both a linearly independent set and spanning set for \( \R^{3}\).
\end{eg}

The following theorem and its corollaries are the most important results in the Chapter 1.


\subsection{Dimensions}


\begin{theorem}[Replacement Theorem]\label{Replacement Theorem}
   Let \( V  \) be a vector space that is generated by a set \( G  \) containing exactly \( n  \) vectors, and let \( L  \) be a linearly independent subset of \( V  \) containing exactly \( m  \) vectors. Then \( m \leq n  \) and there exists a subset \( H  \) of \( G  \) containing exactly \( n - m  \) vectors such that \( L \cup H  \) generates \( V  \).  
\end{theorem}
\begin{proof}
Let us proceed the proof via induction on \( m  \). Let \( m = 0  \) be our base case. Then we find that \( L = \emptyset  \) is linearly independent set with exactly \( 0  \) vectors. Letting \( H = G   \) gives us the desired result (since \( G  \) contains exactly \( n  \) vectors).
    Now suppose that the theorem is holds for some integer \( m \geq 0  \). We will show that the theorem holds for the \( m + 1  \) case.Let \( L = \{ v_{1}, v_{2}, \dots, v_{m+1} \}  \) be a linearly independent subset of \( V  \) consisting of exactly \( m + 1  \) vectors. By the corollary to { \hyperref[Corollary to Theorem 1.6]{Theorem 1.6} }, we find that \(L' =  \{ v_{1}, v_{2}, \dots, v_{m} \}  \) is a linearly independent set (because \( L' \subseteq L  \) and \( L  \) is linearly independent). Using our induction hypothesis, we can conclude that \( m \leq n  \) and that there exists a subset \( H' =  \{ u_{1}, u_{2}, \dots, u_{n-m} \}  \) of \( G  \) such that \( L' \cup H' \) generates \( V  \). Thus there exists scalars \( a_{1}, a_{2}, \dots a_{m}, b_{1}, b_{2}, \dots, b_{n-m} \) such that  
    \[  v_{m+1} = a_{1} v_{1} + a_{2} v_{2} + \cdots +   a_{m} v_{m} + b_{1} u_{1} + b_{2} u_{2} + \cdots + b_{n-m} u_{n-m} \tag{1}.\] 
    Note that \( n -m > 0  \), \textbf{unless} \( v_{m+1} \) is a linear combination of \( v_{1}, v_{2}, \dots, v_{m} \) which by Theorem 1.7 contradicts the assumption that \( L  \) is a linearly independent set. Hence, \( n > m  \); that is, \( n \geq m + 1  \). Furthermore, some \( b_{i}  \), say \( b_{1}  \) is nonzero, for otherwise we obtain the same contradiction. Solving (1) for \( u_{1}  \), we get 
    \begin{align*}
        u_{1} =  (- b_{1}^{-1} &a_{1}) v_{1} + (- b_{1}^{-1} a_{2}) v_{2} + \cdots + (- b_{1}^{-1} ) v_{m} + (b_{1}^{-1} )v_{m+1}   \\
                               &+ (- b_{1}^{-1} b_{2} ) u_{2} + \cdots + (- b_{1}^{-1} b_{n-m}) u_{n-m}.
    \end{align*}
    Let \( H = \{  u_{2}, u_{3}, \dots, u_{n-m} \}  \). Then \( u_{1} \in \text{span}(L \cup H ) \) and because \( v_{1}, v_{2}, \dots, v_{m} , u_{2}, \dots, u_{n-m}\) are clearly in \( \text{span}(L \cup H ) \), we have that 
    \[ L' \cup H' \subseteq \text{span}(L \cup H).  \]
    
Since \( L' \cup H' \) generates \( V  \) and the fact that \( L' \cup H' \subseteq \text{span}(L \cup H ) \) (note that \( \text{span}(L \cup H)   \) is also a subspace), we know by {\hyperref[Theorem 1.5]{Theorem 1.5}} that \( \text{span}(L' \cup H') \subseteq \text{span}(L \cup H) \). Since \( \text{span}(L' \cup H')  \) generates \( V  \), we know that \( V \subseteq \text{span}(L' \cup H') \subseteq \text{span}(L \cup H ) \). Observe that \( \text{span}(L \cup H ) \subseteq V  \) is true by default. Hence, \( \text{span}(L \cup H ) \) generates \( V  \) and that \( H  \) contains \( (n-m) - 1 = n - (m+1)  \) vectors which concludes our induction proof.
\end{proof}


\begin{corollary}[ ]
   Let \( V  \) be a vector space having a finite basis. Then all bases for \( V  \) are finite, and every basis for \( V  \) contains the same number of vectors. 
\end{corollary}
\begin{proof}
    Let \( \beta \) be a finite basis for \( V  \) that contains exactly \( n  \) vectors. Let \( \gamma \) be any other basis that contains more than \( n  \) vectors. Suppose we pick a subset \( S  \) of \( \gamma  \) such that this subset contains exactly \( n + 1  \) vectors. Since \( \beta \) is a finite basis for \( V  \), we know that \( \beta \) is a linearly independent set. By {\hyperref[Theorem 1.6]{Theorem 1.6}}, we know that \( S  \) is also a linearly independent set. By {\hyperref[Replacement Theorem]{Replacement Theorem}}, we have that \( n + 1 \leq n  \) which is a contradiction. Therefore, we must have \( \gamma  \) is finite, and \( \gamma \) contains exactly \( m  \) amount of vectors in \( \gamma \) which satisfies \( m \leq n \). To show that \( n \leq m  \), we can reverse the roles of \( \beta \) and \( \gamma \) and use the same argument as above. Hence, \( n \leq m \) and thus \( m = n  \). 
\end{proof}

The main takeaway from the corollary above is that the number of vectors in any basis for \( V  \) is an inherent property of \( V  \).  

\begin{definition}[Finite-dimensional]\label{Finite-dimensional}
   A vector space is \textbf{finite-dimensional} if it has a basis consisting of a finite number of vectors.  
\end{definition}

\begin{definition}[Dimension of a Vector Space]\label{Dimension}
    The unique integer \( n \) such that every basis for \( V  \) contains exactly \( n  \) elements is called the \textbf{dimension} of \( V  \) and is denoted by \( \text{dim}(V) \). 
\end{definition}

\begin{definition}[Infinite-dimensional]\label{Infinite-dimensional}
    A vector space that is not finite-dimensional is called \textbf{infinite-dimensional}.
\end{definition}

\begin{eg}
    \begin{itemize}
        \item The vector space \( \{ 0 \}   \) has dimension zero. This is because \( \{ 0 \}  \) is generated by the empty set \( \emptyset \).
        \item The vector space \( F^{n}  \) has dimension \( n \).
        \item The vector space \( M_{m \times n}(F)  \) has dimension \( mn \).
        \item The vector space \( P_{n}(F) \) has dimension \( n + 1  \).
    \end{itemize}
\end{eg}

It turns out that the dimension of a vector space depends on the choice of the field it's defined on.


\begin{eg}
    \begin{itemize}
        \item Over the field of complex numbers, the vector space of complex numbers has dimension 1. (A basis is \( \{ 1 \}  \)).
        \item On the other hand, if \( \C  \) is defined over the real numbers \( \R  \), then the vector space has dimension 2.
    \end{itemize}
\end{eg}

The first conclusion of the replacement theorem states that given a {\hyperref[Finite-dimensional]{finite-dimensional}} vector space, a linearly independent subset of \( V  \) cannot contain no more than \( \text{dim}(V) \) amount of vectors. 

\begin{eg}[Example of an Infinite-Dimensional Vector Space]
    The vector space \( P(F) \) is {\hyperref[Infinite-dimensional]{infinite-dimensional}} since it contains a linearly independent set \( \{ 1,x,x^{2}, \dots \}  \) that is infinite. This is peculiar because all of the results in this section thus far does not guarantee that an infinite-dimensional vector space containing a basis. In fact, it is proven in section 1.7 that infinite-dimensional vector space are guaranteed to have a basis.
\end{eg}

The next corollary from the Replacement Theorem refers to the size of the generating set.

\begin{corollary}\label{2nd Corollary to RT}
    Let \( V  \) be a vector space with dimension \( n \).
    \begin{enumerate}
        \item[(a)] Any finite generating set for \( V  \) contains at least \( n  \) vectors, and a generating set for \( V  \) that contains exactly \( n \) vectors is a basis for \( V  \).
        \item[(b)] Any linearly independent subset of \( V  \) that contains exactly \( n  \) vectors is a basis for \( V  \).
        \item[(c)] Every linearly independent subset of \( V  \) can be extended to a basis for \( V  \), that is, if \( L \) is a linearly independent subset of \( V  \), then there is a basis \( \beta \) of \( V  \) such that \( L \subseteq \beta \).   
    \end{enumerate}
\end{corollary}
\begin{proof}
    Let \( \beta  \) be a basis for \( V  \).
\begin{enumerate}
    \item[(a)] Let \( \beta  \) be a finite generating set for \( V  \). By {\hyperref[Theorem 1.9]{Theorem 1.9}}, \( \beta  \) contains a subset \( S \) such that \( S \) is a finite basis for \( V  \). Since \( V  \) is a vector space with {\hyperref[Dimension]{dimension}} \( n \), we know that \( S \) contains exactly \( n  \) vectors. This means that \( \beta  \) must contain at least \( n  \) vectors since \( S  \subseteq \beta  \). 
    \item[(b)] Let \( L \) be a linearly independent subset of \( V  \) that contains exactly \( n  \) vectors. Since \( \beta  \) is a generating set for \( V  \), the {\hyperref[Replacement Theorem]{Replacement Theorem}} states that we can find a subset of \( \beta \), say \( L' \), such that \( L' \) contains exactly \( n - n = 0   \) vectors implies that \( L' = \emptyset \) and \( L \cup L' = L \cup \emptyset = L  \) generates \( V  \). Since \( L  \) is also linearly independent, we conclude that \( L  \) is a basis for \( V  \). 
    \item[(c)] Let \( L  \) be a linearly independent subset of \( V  \) containing \( m  \) vectors. Then the Replacement Theorem implies that there exists \( H \subseteq \beta  \) containing \( n - m  \), where \( \beta  \) is the generating set of \( V  \) with exactly \( n  \) amount of vectors, such that \( L \cup H    \) generates \( V  \) and is finite. This implies that \( L \cup H  \) must contain at most \( n  \) vectors, but part (a) also tells us that \( L \cup H  \) contains at least \(  n \) vectors, so \( L \cup H  \) must contain exactly \( n \) vectors. Hence, \( L \cup H  \) is a basis for \( V  \). 
\end{enumerate}
\end{proof}


\begin{eg}
    \begin{itemize}
        \item In {\hyperref[Generating Polynomials section 1.4]{Example 1.3.5}} the set   
    \[  \{ x^{2} + 3x - 2, 2x^{2} + 5x - 3,    -x^{2} - 4x + 4    \}  \] is a generating set for \( P_{2}(\R) \) that contains exactly \( 3  \) vectors. Hence, the set above is a basis for \( P_{2}(\R) \) by part (a).
\item It follows from {\hyperref[Generating Matrices section 1.4]{Example 1.3.6}} and (a) of the second corollary to the Replacement Theorem that the set  
    \[ \Big\{ \begin{pmatrix}
            1 & 0 \\
            1 & 0 
        \end{pmatrix},  \begin{pmatrix}
            1 & 1 \\ 
            0 & 1 
        \end{pmatrix},  \begin{pmatrix}
            1 & 0 \\ 
            1 & 1 
        \end{pmatrix},  \begin{pmatrix}
            0 & 1 \\
            1 & 1 
        \end{pmatrix}     \Big\} \] is a basis for \( M_{2 \times 2}(\R) \).
    \item The set in {\hyperref[A set of vectors in R4]{Example 1.4.3}} is a basis for \( \R^{4} \).
    \item See {\hyperref[Linearly Independent Set of Polys]{Example 1.4.4}}. This set is a basis for \( P_{n}(F) \). 
    \end{itemize}
\end{eg}


\subsection{An Overview of Dimension and Its Consequences}
The main takeaways from our results are:
\begin{itemize}
    \item We can reduce a finite generating set into a basis for a vector space \( V  \).
    \item A finite generating set contains at least \( n  \) amount of vectors.
    \item A linearly independent set of vectors contain at most \( n \) amount of vectors.
    \item We can Extend a linearly independent set into a basis for \( V  \).
    \item A finite basis for \( V  \) must contain exactly \( n  \) vectors.
    \item Any basis for \( V  \) contains the same number of vectors. In other words, if a basis for \( V \), say \( \beta \), contains \( n  \) amount of vectors then any other basis must contain the same amount.
    \item The number of vectors in a basis is the dimension of a vector space.
\end{itemize}


\subsection{The Dimension of Subspaces}

We can relate the dimension of a vector space \( V  \) to a subspace of \( V  \).

\begin{theorem}[ ]
    Let \( W  \) be a subspace of a {\hyperref[Finite-dimensional]{finite-dimensional}} vector space \( V  \). Then \( W  \) is a finite-dimensional and \( \text{dim}(W) \leq \text{dim}(V) \). Moreover, if \( \text{dim}(V) = \text{dim}(W) \), then \( V = W  \).
\end{theorem}
\begin{proof}
    Let \( \text{dim}(V) = n  \) since \( V \) is finite-dimensional. Suppose \( W = \{ 0  \}  \). Then \( W  \) contains the empty set \( \emptyset \) such that \( \text{span}(\emptyset) = W  \) and \( \emptyset \) is linearly independent. Hence, \( \emptyset \) is a subset of \( W  \) that is a basis for \( W = \{ 0 \}  \) that contains \( 0  \) vectors. Hence, \( W  \) is finite-dimensional. Otherwise, \( W  \) contains a nonzero vector \( =  x_{1}  \) such that \(W' =  \{ x_{1} \}  \) is a linearly independent set. Continue this process of adding vectors \( x_{2}, x_{3} , \dots, x_{k } \) into \( W' \) such that \( W'  \) is linearly independent. Since no linearly independent subset of \( V  \) cannot contain no more than \( n \) vectors, this process must stop at \( k \leq  n \). Note adding a vector from \(w \in W  \) into \( W' \) such that \(w \notin W' \) will make a linearly dependent set. By {\hyperref[Theorem 1.7]{Theorem 1.4.2}}, we get that \( w \in \text{span}(W') \). Hence, \( W'  \) is a basis for \( W  \) and that \( \text{dim}(W) = k  \leq n = \text{dim}(V) \).
    Suppose \( \text{dim}(W) = n  \), then there exists a basis \( \beta \) for \( W  \)   such that \( \beta \) contains exactly \( n \) amount of vectors. Since \( \beta \) is also linearly independent and contains the same amount of vectors as a basis for \( V  \), then we must have that \( \beta \) must also be a basis for \( V  \) by {\hyperref[2nd Corollary to RT]{Corollary 2}} of the Replacement Theorem. Hence, \( W = \text{span}(\beta) = V  \). 
\end{proof}

\begin{eg}
    \begin{itemize}
        \item Let  
    \[  W = \{ (a_{1}, a_{2}, a_{3}, a_{4}, a_{5}) \in F^{5} : a_{1} + a_{3} + a_{5} = 0 , a_{2} = a_{4} \}. \]
    It can be easily shown that \( W  \) is a subspace of \( F^{5} \) having 
    \[  \{ (-1,0,1,0,0), (-1,0,0,0,1), (0,1,0,1,0) \}  \] as a basis. Hence, \( \text{dim}(W) = 3  \)
    \item The set of diagonal \( n \times n  \) matrices is a subspace \( W  \) of \( M_{n \times n}(F)  \). A basis for \( W  \) is the following set 
        \[  \{ E^{11} , E^{22} , \dots, E^{n n } \},  \]
        where \( E^{ij}  \) is the matrix in which the only nonzero entry is a 1 in the \( i \)th row and \( j \)th column. Thus, we have \( \text{dim}(W) = n  \).
    \end{itemize}
\end{eg}

\begin{corollary}\label{Corollary to Theorem 1.11}
    If \( W  \) is a subspace of a finite-dimensional vector space \( V  \), then any basis for \( W  \) can be extended to a basis for \( V  \).
\end{corollary}
\begin{proof}
Let \( \beta \) be a basis for the subspace \( W    \). Since \( \beta \subseteq W \subseteq V   \) is a linearly independent subset of \( V  \), we know that \( \beta \) can be extended as a basis for \( V  \) by {\hyperref[2nd Corollary to RT]{Corollary 2}} of the Replacement Theorem.
\end{proof}


\subsection{Geometrical View of Subspaces in Euclidean Space}

\begin{itemize}
    \item The subspaces of \( R^{2} \) include \( \{ 0 \}   \) of dimension 0, \( \R  \) of dimension 1, and \( R^{2} \) itself. 
        \begin{itemize}
            \item The set \(\{ 0 \}   \) of zero dimension can be visualized as the origin in \( R^{2} \).
            \item The set \( \R \) is the field itself and can be visualized as the line \( y = cx \) with \( c \in \R  \) where \( x \in \R^{2} \).
            \item The set \( \R^{2} \) is the whole cartesian space itself.
        \end{itemize}
    \item The subspaces of \( \R^{3} \) must have subspaces with dimensions \( 0, 1, 2,  \) or \( 3 \). The same logic can be followed from the three bullet points above.
\end{itemize}

