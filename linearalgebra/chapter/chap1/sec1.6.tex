\section{Bases And Dimension}

\begin{itemize}
    \item Recall that \( S  \) is a generating set for a subspace \( W  \) and no proper subset of \( S  \) is a generating set for \( W  \), then \( S  \) must be linearly independent. 
    \item Linearly independent sets possess the unique property that every vector that its spanning set generates is unique.
    \item This is property is what allows generating sets to be the building blocks of vector spaces.
\end{itemize}

\begin{definition}[Basis]
    A \textbf{basis} \( \beta \) for a vector space \( V  \) is linearly independent subset of \( V  \) that generates \( V  \). If \( \beta  \) is a basis for \( V  \), we also say that the vectors of \( \beta \) form a basis for \( V  \).
\end{definition}

\begin{eg}
    \begin{itemize}
        \item     Recall that the empty set \( \emptyset \) is linearly independent and that \( \text{span}(\emptyset) = \{ 0 \}  \). The empty set \( \emptyset \) in this case is the basis for the zero vector space.
        \item Note that in \( F^{n}  \), the vectors \( e_{1} = (1,0, \dots, 0 ) \), \( e_{2} = (0,1,0, \dots, 0 ) \dots, e_{n} = (0,0, \dots, 0, 1 ) \) form a basis for \( F^{n} \).
        \item The basis for \( M_{m \times n}(F ) \) is the set of matrices \( E^{ij} \) such that the only nonzero entry is a 1 in the \( i \)th and \( j \)th column.
        \item As we have seen in the last section, the set \( \{ 1, x , x^{2}, \dots, x^{n} \}  \) is a basis for \( P_{n}(F) \).
        \item In \( P(F) \), the set \( \{ 1,x ,x x^{2}, \dots \}  \) is a basis. \textit{Bases are not limited to finite sets. They can be infinite.} 
\end{itemize}
\end{eg}

\begin{theorem}[ ]
    Let \( V  \) be a vector space and \( u_{1}, u_{2}, \dots, u_{n}  \) be distinct vectors in \( V  \). Then \( \beta = \{ u_{1}, u_{2},\dots , u_{n}  \}   \) is a basis for \( V  \) if and only if each \( v \in V  \) can be unique expressed as a linear combination of vectors in \( \beta \), that is, expressed in the form 
    \[  v = a_{1} v_{1} + a_{2} v_{2} + \cdots + a_{n} v_{n}  \] for unique scalars \( a_{1}, a_{2}, \dots, a_{n}  \).
\end{theorem}
\begin{proof}
    (\( \Rightarrow \)) Suppose \( \beta = \{ u_{1}, u_{2}, \dots, u_{n} \}  \) is a basis for \( V  \). Then \( \text{span}(\beta) = V  \). If \( v \in V  \), then \( v \in \text{span}(\beta) \). Hence, we can write \( v  \) as a linear combination of vectors in \( \beta   \) such that choosing scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) leads to 
\[ v = \sum_{ i=1 }^{ n } a_{i} u_{i}.   \]
Suppose there exists another representation of \( v \in V  \) such that 
\[  v = \sum_{ i=1 }^{ n  } b_{i} x_{i} \]
Hence, observe that 
\begin{align*}  &\sum_{ i=1  }^{ n } a_{i} x_{i} = \sum_{ i=1 }^{ n } b_{i} y_{i}  \\
    &\implies \sum_{ i=1 }^{ n } (a_{i} - b_{i}) x_{i} = 0.
\end{align*}
Since \( \beta \) is linearly independent, we know that \( a_{i} - b_{i} = 0  \) which implies \( a_{i} = b_{i}  \) for all \( 1 \leq i \leq n \). Hence, \( v  \) can be expressed as a unique linear combination of vectors in \( \beta \).

(\( \Leftarrow \)) Let \( v \in V  \). Then \( v  \) can be uniquely expressed as a linear combination of vectors in \( \beta = \{ u_{1}, u_{2}, \dots, u_{n} \}  \) such that 
\[  v = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n} \tag{1} \]
for unique \( a_{i}  \) for all \(  1 \leq i \leq n \). Suppose for sake of contradiction that \( \beta  \) is \textbf{NOT} a basis for \( V  \). Then either \( \beta   \) is linearly dependent or \( \text{span}(\beta) \neq  V  \). Suppose \( \text{span}(\beta) \neq V  \), then \( v \in V  \) cannot be written a linear combination of vectors in \( \beta \) which is a contradiction. Suppose \( \beta \) is linearly dependent. Then either \( u_{1} = 0  \) or \( u_{k+1} \in \text{span}(\{ u_{1}, u_{2} , \dots, u_{k }  \} ) \) for some \(  1 \leq  k < n  \). If \( u_{1} = 0  \), then (1) can be re-written as
\[ v = a_{1} 0 + a_{2} u_{2} + \cdots + a_{n} u_{n}.   \]
This implies that \( a_{1} \in F  \) can be made arbitrary such that \( v  \) can be written in another representation. But this is a contradiction since we assumed that \( v  \) contains a unique linear combination. Now, suppose \( u_{k+1} \in \text{span}(\{ u_{1} , u_{2}, \dots, u_{k } \} ) \). Then we have 
\[  u_{k+1} = \delta_{1} u_{1} + \delta_{2} u_{2} + \cdots + \delta_{k} u_{k }.  \]
Substituting this equation for \( u_{k+1} \) in (1) produces the following equation:
\begin{align*}
    &v = a_{1} u_{1} +a_{2} u_{2} + \cdots + a_{k} u_{k} + a_{k+1} u_{k+1} + \cdots a_{n} u_{n}  \\
    &\implies v =  a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{k} u_{k } + a_{k+1} ( \delta_{1} u_{1} + \delta_{2} u_{2} + \cdots + \delta_{k} u_{k }  ) \\ 
    &+ \cdots +  a_{n} u_{n} \\
    &\implies v = (a_{1} + a_{k+1} \delta_{1}) u_{1} +  (a_{2} + a_{k+1} \delta_{2}) u_{2} + \cdots + (a_{k } + a_{k+1} \delta_{k } ) u_{k } + \cdots + a_{n} u_{n}. \tag{2} \\
\end{align*}
Since \( v  \) has a unique representation, we must have 
\[  a_{i} = a_{i} + a_{k+1} \delta_{i } \implies  a_{k+1}\delta_{i} = 0 \tag{3}  \] 
for all \( 1 \leq i \leq k  \). Assuming that \( a_{k+1} \neq 0  \), the only way for \( a_{k+1} \delta_{i} = 0  \) is if \( \delta_{i} = 0   \) for all \( 1 \leq i \leq k    \). But note that not all \( \delta_{i}  \) are zero since \( \{ u_{1}, u_{2}, \dots, u_{k } \} \subseteq \beta \) is linearly dependent by Exercise 16. Hence, there exists at least one \( i  \) such that \( a_{k+1} \delta_{i} \neq  0  \) which is a contradiction. Hence, \( \beta \) must be a basis for \( V  \).


\end{proof}

\begin{itemize}
    \item Any vector \(  v \in V  \) can be written as a linear combination of vectors from the basis containing \( u_{1}, u_{2}, \dots, u_{n} \in V  \). 
    \item This determines a unique \( n  \)-tuple of scalars \( (a_{1}, a_{2}, \dots, a_{n}) \) and conversely, each \( n \)-tuple of scalars determines  a unique vector \( v \in V   \) such that each coefficient from the linear combination of \( u_{1}, u_{2}, \dots, u_{n}  \) is an entry from said tuple.
    \item For example, in our vector space \( F^{n} \), \( n  \) is the number of vectors that should be in the basis for \( F^{n} \) which is indeed the case.
    \item In this book, we are only concerned with finite bases.
\end{itemize}

\begin{theorem}[ ]
   If a vector space \( V  \) is generated by a finite set \( S  \), then some subset of \( S  \) is a basis for \( V  \). Hence, \( V  \) has a finite basis. 
\end{theorem}
\begin{proof}

\end{proof}


