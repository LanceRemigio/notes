\section{The Rank Of A Matrix And Matrix Inverses}

In this section, we will
\begin{itemize}
    \item Define the \textit{rank} of a matrix. 
    \item Compute the rank of a matrix and a linear transformation.
    \item Create a procedure for computing the inverse of an invertible matrix.
\end{itemize}

\begin{definition}[Rank]
    If \( A \in {M}_{m \times n}(F)  \), we define the \textbf{rank} of \( A  \), denoted \( \text{rank}(A) \) to be the rank of the linear transformation \( {L}_{A}: F^{n} \to F^{m} \).
\end{definition}

\begin{itemize}
    \item An important result derives from {\hyperref[Fact 3]{fact 3}} and {\hyperref[Corollary 2 to Theorem 2.18]{Corollary 2 to Theorem 2.18}} is that \textit{an \( n \times n  \) matrix is invertible if and only if its rank is \( n \)}.
    \item The rank of a matrix is equivalent to the rank of the linear transformation \( {L}_{A} \).
\end{itemize}

\begin{theorem}
    Let \( T: V \to W  \) be a linear transformation between finite-dimensional vector spaces, and let \( \beta  \) and \( \gamma \) be ordered bases for \( V  \) and \( W  \), respectively. Then \( \text{rank}(T) = \text{rank}([T]_{\beta}^{\gamma} ) \).
\end{theorem}
\begin{proof}
This is a restatement of Exercise 20 of Section 2.4.
\end{proof}

\begin{itemize}
    \item We can see now that finding the rank of a linear transformation can be done by finding the rank of a matrix.
    \item The next theorem will show us a method on how to do this. 
    \item This allows us to do these operations in a rank-preserving way on matrices.
\end{itemize}

\begin{theorem}\label{Theorem 3.4}
    Let \( A  \) be an \( m \times n  \) matrix. If \(  P  \) and \( Q  \) are invertible \( m \times m  \) and \( n \times n  \) matrices, respectively, then 
    \begin{enumerate}
        \item[(a)] \( \text{rank}(AQ) = \text{rank}(A) \),
        \item[(b)] \( \text{rank}(PA) = \text{rank}(A) \),
        \item[(c)] \( \text{rank}(PAQ) = \text{rank}(A) \).
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item[(a)] Let \( A \in {M}_{m \times n }(F) \). 
Observe that
\[  R({L}_{AQ}) = R({L}_{A} {L}_{Q}) = {L}_{A} {L}_{Q} (F^{n}) = {L}_{A} ({L}_{Q}(F^{n})) = {L}_{A}(F^{n}) = R({L}_{A}) \]
where \( {L}_{Q}  \) is surjective. Hence, we have 
\[  \text{rank}(AQ) = \text{dim}(R({L}_{AQ})) = \text{dim}(R({L}_{A})) = \text{rank}(A) \]
and so, \( \text{rank}(AQ) = \text{rank}(A) \). 
    \item[(b)] Note that \( {L}_{A}(F^{n})\) is a subspace of \( F^{m} \). So, we must have \( \text{dim}({L}_{A}(F^{n})) = \text{dim}({L}_{P}({L}_{A}(F^{n}))) \) by Exercise 17. We can apply this result to write that
        \[  R({L}_{PA}) = R({L}_{P}{L}_{A}) = {L}_{P}{L}_{A}(F^{n}) = {L}_{P}({L}_{A}(F^{n})) = {L}_{A}(F^{n}) = R({L}_{A})\]
        which implies that
        \[  \text{rank}(PA) = \text{dim}(R({L}_{PA})) = \text{dim}(R({L}_{A})) = \text{rank}(A) \]
        and so, we have \( \text{rank}(PA) = \text{rank}(A) \).
    \item[(c)] Applying part (a) and (b), we can see that
        \[  \text{rank}(PAQ) = \text{rank}(PA) = \text{rank}(A).  \]
    \end{enumerate}
\end{proof}

\begin{corollary}\label{Corollary to Theorem 3.4}
    Elementary row and column operations on a matrix are rank-preserving.
\end{corollary}
\begin{proof}
If \( B  \) is obtained from a matrix \( A  \) by an elementary row operation, then there exists an elementary matrix \( E  \) such that \( B = EA  \) by {\hyperref[Theorem 3.1]{Theorem 3.1}}. Since \( E  \) is an invertible matrix, we use {\hyperref[Theorem 3.4]{Theorem 3.4}} to conclude that \( \text{rank}(A) = \text{rank}(B) \). Similarly, if \(  B  \) is obtained from a matrix \( A  \) by an elementary column operation, then there exists an elementary matrix \( E  \) such that \( B = A E  \) by Theorem 3.1. Then we have \( \text{rank}(B) = \text{rank}(AE) = \text{rank}(A ) \) by Theorem 3.4 and we are done. 
\end{proof}

This result allows us to preserve the rank of matrices while using elementary operations on them. The next theorem allows us to find out the rank of a transformed matrix.

\begin{theorem}
    The rank of any matrix equals the maximum number of its linearly independent columns; that is, the rank of a matrix is the dimension of the subspace generated by its columns.
\end{theorem}
\begin{proof}
Let \( A \in {M}_{m \times n }(F) \). Then observe that
\[  \text{rank}(A) = \text{rank}({L}_{A}) = \text{dim}(R({L}_{A})). \]
Let \( \beta  = \{ {e}_{1}, {e}_{2}, \dots, {e}_{n} \} \) be the standard ordered basis of \( F^{n} \). Since \( {L}_{A}: F^{n} \to F^{m} \) is linear, we know by {\hyperref[Spanning set for R(T)]{Theorem 2.2}} that 
\[  R({L}_{A}) = \text{span}({L}_{A}(\beta)) = \text{span}(\{ {L}_{A}({e}_{1}), {L}_{A}({e}_{2}), \dots, {L}_{A}({e}_{n}) \} ). \]
Observe that for each \( 1 \leq j \leq n \) that 
\[  {L}_{A}({e}_{j}) = A {e}_{j} = {a}_{j} \] where \( {a}_{j} \) is the \( j \)th column of \( A  \). So, we have that
\[  R({L}_{A}) = \text{span}(\{ {a}_{1}, {a}_{2}, \dots, {a}_{n} \}). \]
\end{proof}

\begin{eg}
   Suppose we have the matrix 
   \[  A = \begin{pmatrix}
       1 & 0 & 1 \\
       0 & 1 & 1 \\
       1 & 0 & 1 
   \end{pmatrix}. \]
   In this example, we can see quite easily that the first two rows are linearly independent and that the third column is a linear combination of the first two. Thus, we can see that the rank of \( A  \) is just \( 2  \). 
\end{eg}

\begin{remark}
    In much less trivial matrices, it is often much easier to perform appropriate elementary row and column operations to convert the matrix in question to a suitable one so that one can easily see the linearly independent columns of said matrix through the accumulation of more zero entries. Remember that the {\hyperref[Corollary to Theorem 3.4]{Corollary to Theorem 3.4}} guarantees that our simplifying operations will preserve the rank of our matrix.
\end{remark}

\begin{eg}
    Let 
    \[  A = \begin{pmatrix} 
        1 & 2 & 1 \\
        1 & 0 & 3 \\
        1 & 1 & 2 
              \end{pmatrix}. \]
\end{eg}

\begin{theorem}
    Let \( A  \) be an \( n \times n  \) matrix of rank \( r  \). Then \( r \leq m, r \leq n  \), and, y means of a finite number of elementary row and column operations, \( A  \) can be transformed into the matrix 
    \[  D = \begin{pmatrix}
        {I}_{r} & {O}_{1} \\
        {O}_{2} & {O}_{3}
    \end{pmatrix} \]
    where \( {O}_{1}, {O}_{2}, \dots, {O}_{3} \) are zero matrices. Thus, \( {D}_{ii} = 1  \) for \( i \leq r  \) and \( {D}_{ij} = 0  \) otherwise.
\end{theorem}



