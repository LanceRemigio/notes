\section{The Rank Of A Matrix And Matrix Inverses}

\subsection{Defining the Rank of a Matrix}
In this section, we will
\begin{itemize}
    \item Define the \textit{rank} of a matrix. 
    \item Compute the rank of a matrix and a linear transformation.
    \item Create a procedure for computing the inverse of an invertible matrix.
\end{itemize}

\begin{definition}[Rank]
    If \( A \in {M}_{m \times n}(F)  \), we define the \textbf{rank} of \( A  \), denoted \( \text{rank}(A) \) to be the rank of the linear transformation \( {L}_{A}: F^{n} \to F^{m} \).
\end{definition}

\begin{itemize}
    \item An important result derives from {\hyperref[Fact 3]{fact 3}} and {\hyperref[Corollary 2 to Theorem 2.18]{Corollary 2 to Theorem 2.18}} is that \textit{an \( n \times n  \) matrix is invertible if and only if its rank is \( n \)}.
    \item The rank of a matrix is equivalent to the rank of the linear transformation \( {L}_{A} \).
\end{itemize}

\begin{theorem}
    Let \( T: V \to W  \) be a linear transformation between finite-dimensional vector spaces, and let \( \beta  \) and \( \gamma \) be ordered bases for \( V  \) and \( W  \), respectively. Then \( \text{rank}(T) = \text{rank}([T]_{\beta}^{\gamma} ) \).
\end{theorem}
\begin{proof}
This is a restatement of Exercise 20 of Section 2.4.
\end{proof}

\begin{itemize}
    \item We can see now that finding the rank of a linear transformation can be done by finding the rank of a matrix.
    \item The next theorem will show us a method on how to do this. 
    \item This allows us to do these operations in a rank-preserving way on matrices.
\end{itemize}

\begin{theorem}\label{Theorem 3.4}
    Let \( A  \) be an \( m \times n  \) matrix. If \(  P  \) and \( Q  \) are invertible \( m \times m  \) and \( n \times n  \) matrices, respectively, then 
    \begin{enumerate}
        \item[(a)] \( \text{rank}(AQ) = \text{rank}(A) \),
        \item[(b)] \( \text{rank}(PA) = \text{rank}(A) \),
        \item[(c)] \( \text{rank}(PAQ) = \text{rank}(A) \).
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \item[(a)] Let \( A \in {M}_{m \times n }(F) \). 
Observe that
\[  R({L}_{AQ}) = R({L}_{A} {L}_{Q}) = {L}_{A} {L}_{Q} (F^{n}) = {L}_{A} ({L}_{Q}(F^{n})) = {L}_{A}(F^{n}) = R({L}_{A}) \]
where \( {L}_{Q}  \) is surjective. Hence, we have 
\[  \text{rank}(AQ) = \text{dim}(R({L}_{AQ})) = \text{dim}(R({L}_{A})) = \text{rank}(A) \]
and so, \( \text{rank}(AQ) = \text{rank}(A) \). 
    \item[(b)] Note that \( {L}_{A}(F^{n})\) is a subspace of \( F^{m} \). So, we must have \( \text{dim}({L}_{A}(F^{n})) = \text{dim}({L}_{P}({L}_{A}(F^{n}))) \) by Exercise 17. We can apply this result to write that
        \[  R({L}_{PA}) = R({L}_{P}{L}_{A}) = {L}_{P}{L}_{A}(F^{n}) = {L}_{P}({L}_{A}(F^{n})) = {L}_{A}(F^{n}) = R({L}_{A})\]
        which implies that
        \[  \text{rank}(PA) = \text{dim}(R({L}_{PA})) = \text{dim}(R({L}_{A})) = \text{rank}(A) \]
        and so, we have \( \text{rank}(PA) = \text{rank}(A) \).
    \item[(c)] Applying part (a) and (b), we can see that
        \[  \text{rank}(PAQ) = \text{rank}(PA) = \text{rank}(A).  \]
    \end{enumerate}
\end{proof}

\begin{corollary}\label{Corollary to Theorem 3.4}
    Elementary row and column operations on a matrix are rank-preserving.
\end{corollary}
\begin{proof}
If \( B  \) is obtained from a matrix \( A  \) by an elementary row operation, then there exists an elementary matrix \( E  \) such that \( B = EA  \) by {\hyperref[Theorem 3.1]{Theorem 3.1}}. Since \( E  \) is an invertible matrix, we use {\hyperref[Theorem 3.4]{Theorem 3.4}} to conclude that \( \text{rank}(A) = \text{rank}(B) \). Similarly, if \(  B  \) is obtained from a matrix \( A  \) by an elementary column operation, then there exists an elementary matrix \( E  \) such that \( B = A E  \) by Theorem 3.1. Then we have \( \text{rank}(B) = \text{rank}(AE) = \text{rank}(A ) \) by Theorem 3.4 and we are done. 
\end{proof}

This result allows us to preserve the rank of matrices while using elementary operations on them. The next theorem allows us to find out the rank of a transformed matrix.

\begin{theorem}
    The rank of any matrix equals the maximum number of its linearly independent columns; that is, the rank of a matrix is the dimension of the subspace generated by its columns.
\end{theorem}
\begin{proof}
Let \( A \in {M}_{m \times n }(F) \). Then observe that
\[  \text{rank}(A) = \text{rank}({L}_{A}) = \text{dim}(R({L}_{A})). \]
Let \( \beta  = \{ {e}_{1}, {e}_{2}, \dots, {e}_{n} \} \) be the standard ordered basis of \( F^{n} \). Since \( {L}_{A}: F^{n} \to F^{m} \) is linear, we know by {\hyperref[Spanning set for R(T)]{Theorem 2.2}} that 
\[  R({L}_{A}) = \text{span}({L}_{A}(\beta)) = \text{span}(\{ {L}_{A}({e}_{1}), {L}_{A}({e}_{2}), \dots, {L}_{A}({e}_{n}) \} ). \]
Observe that for each \( 1 \leq j \leq n \) that 
\[  {L}_{A}({e}_{j}) = A {e}_{j} = {a}_{j} \] where \( {a}_{j} \) is the \( j \)th column of \( A  \). So, we have that
\[  R({L}_{A}) = \text{span}(\{ {a}_{1}, {a}_{2}, \dots, {a}_{n} \}). \]
\end{proof}

\begin{eg}
   Suppose we have the matrix 
   \[  A = \begin{pmatrix}
       1 & 0 & 1 \\
       0 & 1 & 1 \\
       1 & 0 & 1 
   \end{pmatrix}. \]
   In this example, we can see quite easily that the first two rows are linearly independent and that the third column is a linear combination of the first two. Thus, we can see that the rank of \( A  \) is just \( 2  \). 
\end{eg}

\begin{remark}
    In much less trivial matrices, it is often much easier to perform appropriate elementary row and column operations to convert the matrix in question to a suitable one so that one can easily see the linearly independent columns of said matrix through the accumulation of more zero entries. Remember that the {\hyperref[Corollary to Theorem 3.4]{Corollary to Theorem 3.4}} guarantees that our simplifying operations will preserve the rank of our matrix.
\end{remark}

\begin{eg}
    Let 
    \[  A = \begin{pmatrix} 
        1 & 2 & 1 \\
        1 & 0 & 3 \\
        1 & 1 & 2 
              \end{pmatrix}. \]
              Observe that
              \begin{align*}
                  A = \begin{pmatrix}
                      1 & 2 & 1 \\
                      1 & 0 & 3 \\
                      1 & 1 & 2 
                      \end{pmatrix} &\xrightarrow{{R}_{2} - R_{1} \text{ and } {R}_{3} - {R}_{1}  } \begin{pmatrix}
                  1 & 2 & 1 \\
                  0 & -2 & 2 \\
                  0 & -1 & 1 
                  \end{pmatrix}. \\
              \end{align*}
              Then we have
              \begin{align*}
                  \begin{pmatrix} 
                      1 & 2 & 1 \\
                      0 & -2 & 2 \\
                      0 & -1 & 1 
                  \end{pmatrix} &\xrightarrow{{R}_{2} - {2R}_{1} \text{ and } {R}_{3} - {2R}_{1}} \begin{pmatrix}
                  1 &  0 & 0  \\
                  0 & -2 & 2 \\
                  0 & -1 & 1 
                  \end{pmatrix}
              \end{align*}
              for which we can see that the maximum number of linearly independent columns of \( A  \) is \( 2  \) (again this fact is possible since our operations are rank preserving). Hence, the rank of \( A  \) is \( 2  \).
\end{eg}

These set of operations to convert a given matrix into a simpler form is used to prove the following fact.

\begin{theorem}[Echelon Row (Column) Reduction]\label{Theorem 3.6}
    Let \( A  \) be an \( m \times n  \) matrix of rank \( r  \). Then \( r \leq m, r \leq n  \), and, y means of a finite number of elementary row and column operations, \( A  \) can be transformed into the matrix 
    \[  D = \begin{pmatrix}
        {I}_{r} & {O}_{1} \\
        {O}_{2} & {O}_{3}
    \end{pmatrix} \]
    where \( {O}_{1}, {O}_{2}, \dots, {O}_{3} \) are zero matrices. Thus, \( {D}_{ii} = 1  \) for \( i \leq r  \) and \( {D}_{ij} = 0  \) otherwise.
\end{theorem}
\begin{proof}
If \( A  \) is the zero matrix, then we have \( r = 0  \) by Exercise 3. Hence, we have \( D = A  \). 
    Otherwise, suppose \( A \neq  O  \) and \( r = \text{rank}(A ) \). So, \( r > 0  \). Let us proceed with induction on \(  m  \), the number of rows of \( A  \). Let \( m = 1  \) be our base case. Using at most one type 1 column operation and at most one type 2 column operation, \( A  \) can be transformed into a matrix with a \( 1  \) in the \( 1,1  \) position. By means of at most \( n - 1  \) type 3 column operations, \( A  \) can be turned into the following matrix
    \[  \begin{pmatrix} 
        1 & 0 & \cdots & 0 
              \end{pmatrix}. \]
              Thus, we can see that the first column is a (the only) linearly independent column in \( D  \). Since the rank of \( A  \) is preserved, we get that \( \text{rank}(D) = \text{rank}(A) = 1  \) by Theorem 3.4. Hence, this ends our base case.

              Assume that this theorem holds for any matrix with at most \( m - 1  \) rows (that is, for some \( m > 0  \)). Our goal is to show that the theorem holds for any matrix with \( m  \) rows.

    Suppose that \( A  \) is any \( m \times n  \) matrix. Suppose \( n = 1  \). Then Theorem 3.6 can be shown in similarly as we have done wen we fixed \(  m =1  \). Otherwise, let \(  n > 1  \). Since \( A \neq O  \), we must have that \( {A}_{ij} \neq 0  \) for some \( i,j  \). 

    Utilizing both at most one elementary row and at most elementary column operation (both of each are type 1 operations), we can move the nonzero entry to the \( 1,1 \) position. Now, an additional type 2 operation can ensure a \( 1  \) in the \( 1,1  \) position. This has an effect of creating the following matrix
    \[  B = \begin{pmatrix} 
        {I}_{r} & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & B' &   \\
        0 &    &
              \end{pmatrix} \]
where \( B'  \) has a rank of one less than \( B  \). Since \( \text{rank}(A ) = \text{rank}(B)  \), we mus have \( \text{rank}(B') = r - 1  \). Hence, \( r -1 \leq m - 1  \) and \( r - 1 \leq n - 1  \) by the induction hypothesis. Thus, we have \( r \leq  m  \) and \( r \leq n \).
    
Using the induction hypothesis again, \( B' \) can be transformed into 
\[  D' = \begin{pmatrix} 
    {I}_{r-1} & {O}_{4} \\
    {O}_{5} & {O}_{6}
          \end{pmatrix}  \] (an \( (m-1) \times (n-1) \) matrix) via a finite number of elementary row and column operations. Note that \( {O}_{4}, {O}_{5},  \) and \( {O}_{6} \) are zero matrices and that \( D'  \) contains \( r - 1  \) diagonal entries containing \( 1  \) and everywhere else containing \( 0  \). Thus, we have 
          \[  D = \begin{pmatrix} 
        {I}_{r} & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & D' &   \\
        0 &    &
                    \end{pmatrix}.   \]
                    We can show through a repeated set of applications involving elementary row and column operations that \( D  \) can be obtained from \( B  \) (Exercise 12). Since \( A  \) can be transformed into \( B  \) and \( B  \) can be transformed into \( D  \) through a finite number of elementary operations, \( A  \) can be transformed into \( D  \) by a finite number of elementary operations.

    Since \( D'  \) contains ones in its first \( r - 1  \) diagonal entries, we can see that \( D  \) contains ones in its first \( r  \) diagonal entires and zeros elsewhere. This establishes the theorem.
\end{proof}

\begin{corollary}
    Let \( A  \) be an \( m \times n  \) matrix of \( \text{rank}(A) = r \). Then there exist invertible matrices \( B  \) and \( C  \) of sizes \( m \times m  \) and \( n \times n  \), respectively, such that \( D = BAC  \), where 
    \[  D = \begin{pmatrix} 
        {I}_{r} & {O}_{1} \\
        {O}_{2} & {O}_{3}
              \end{pmatrix}  \] is the \( m \times n  \) matrix in which \( {O}_{1}, {O}_{2},  \) and \( {O}_{3}  \) are zero matrices.
          \end{corollary}\label{Corollary 1 to Theorem 3.6}
\begin{proof}
By Theorem 3.6, \( A  \) can be transformed into \( D  \) via a finite number of elementary row and elementary column operations. We can use {\hyperref[Theorem 3.1]{Theorem 3.1}} to state that there exist elementary \( m \times m  \) matrices \( {E}_{1}, {E}_{2}, \dots, {E}_{p} \) and elementary \( n \times n  \) matrices \( {G}_{1}, {G}_{2}, \dots, {G}_{q} \) such that
    \[  D = {E}_{p} {E}_{p-1} \cdots {E}_{2} {E}_{1} A {G}_{1} {G}_{2} \cdots {G}_{q} \]
for each elementary row and column operation done on \( A  \) to convert it into \( D  \). By {\hyperref[Theorem 3.2]{Theorem 3.2}}, we can see that each \( {E}_{j}  \) and \( {G}_{j} \) is invertible. So, let \( B ={E}_{p} {E}_{p-1} \cdots {E}_{2} {E}_{1} \) and let \( C = {G}_{1} {G}_{2} \cdots {G}_{q} \). Using exercise 4 of section 2.4, we can see that \( B  \) and \( C  \) are invertible and thus \( D = BAC \).
\end{proof}

\begin{corollary}\label{Corollary 2 to Theorem 3.6}
    Let \( A  \) be an \( m \times n  \) matrix. Then  
    \begin{enumerate}
        \item[(a)] \( \text{rank}(A^{t}) = \text{rank}(A) \).
        \item[(b)] The rank of any matrix equals the maximum number of its linearly independent rows; that is, the rank of a matrix is the dimension of the subspace generated by its rows.
        \item[(c)] The rows and columns of any matrix generate subspaces of the same dimension, numerically equal to the rank of the matrix.
    \end{enumerate}
\end{corollary}    
\begin{proof}
\begin{enumerate}
    \item[(a)] Using {\hyperref[Corollary 1 to Theorem 3.6]{Corollary 1}}, there exists invertible matrices \( B  \) and \( C  \) such that \( D = BAC \), where 
        \[  D = \begin{pmatrix} 
            {I}_{r} & {O}_{1} \\
            {O}_{2} & {O}_{3}
                  \end{pmatrix}  \] is an \( m \times n  \) matrix with the conditions satisfied in the Corollary. Taking the transpose of both sides of \( D = BAC \), we get that
                  \[  D^{t} = (BAC)^{t} = {C}^{t} A^{t} B^{t}. \]
                  Note that \( B^{t}  \) and \( C^{t} \) are invertible by Exercise 5 of Section 2.4. So, {\hyperref[Theorem 3.4]{Theorem 3.4}} implies that  
                  \[  \text{rank}(A^{t}) = \text{rank}(C^{t} A^{t} B^{t}) = \text{rank}(D^{t}). \]
                  Since \( \text{rank}(A) = r  \), we know that \( D^{t} \) is an \( n \times n  \) matrix with the form found as in \( D  \) in Corollary 1, and thus \( \text{rank}(D^{t})  \) must also have the same rank by Theorem 3.5. Hence, we have
                  \[  \text{rank}(A^{t}) = \text{rank}(D^{t}) = r = \text{rank}(A). \]
        \item[(b)] Left as an exercise. \textit{Similar process used to prove Theorem 3.5} 
        \item[(c)] Left as an exercise.
\end{enumerate}
\end{proof}

\begin{corollary}\label{Corollary 3 to Theorem 3.6}
   Every invertible matrix is a product of elementary matrices. 
\end{corollary}
\begin{proof}
Let \( A \in {M}_{m \times n }(F)  \) be an invertible matrix. Thus, \( \text{rank}(A) = n  \) and then by {\hyperref[Corollary 1 to Theorem 3.6]{Corollary 1}}, there exists \( m \times m  \) and \( n \times n  \) invertible matrices \( B  \) and \( C  \), respectively, such that  \( {I}_{n} = BAC \) where \( B = {E}_{p} {E}_{p-1} \cdots {E}_{2} {E}_{1} \) and \( C =  {G}_{q} {G}_{q-1} \cdots {G}_{2} {G}_{1}\). Thus, we have that 
\[  A = B^{-1}{I}_{n}C^{-1} = {B}^{-1}C^{-1}\]
where \(B^{-1} = {E}_{1} {E}_{2} \cdots {E}_{p-1}{E}_{p} \) and \( C^{-1} = {G}_{1} {G}_{2} \cdots {G}_{q-1} {G}_{q}  \). Note that that each \( {E}_{j}  \) and \( {G}_{j}   \) making up the product of \( B^{-1}  \) and \( C^{-1} \) are elementary matrices by {\hyperref[Theorem 3.2]{Theorem 3.2}}.
\end{proof}

\begin{theorem}
    Let \( T: V \to W  \) and \( U: W \to Z  \) be linear transformations on finite-dimensional vector spaces \( V,W,  \) and \( Z  \), and let \( A  \) and \( B  \) matrices such that the product \( AB  \) is defined. Then
    \begin{enumerate}
        \item[(a)] \( \text{rank}(UT) \leq \text{rank}(U) \).
        \item[(b)] \( \text{rank}(UT) \leq \text{rank}(T) \).
        \item[(c)] \( \text{rank}(AB) \leq \text{rank}(A) \).
        \item[(d)] \( \text{rank}(AB) \leq \text{rank}(B) \).
    \end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
    \item[(a)] Note that \( R(T) \subseteq W  \). So, we must have
        \[  R(UT) = UT(V) = U(T(V)) = U(R(T)) \subseteq U(W) = R(U). \]
        We can see by Theorem 1.11 that \( R(UT) \subseteq R(U)  \) implies that  \( \text{rank}(UT) \leq \text{rank}(U) \).
    \item[(c)] Observe that
        \begin{align*}
            \text{rank}(AB) = \text{rank}({L}_{AB}) &= \text{rank}({L}_{A} {L}_{B}) \\
                                                    &\leq  \text{rank}({L}_{A}) \tag{By part (a)} \\
                                                    &= \text{rank}(A).
        \end{align*}
        Thus, \( \text{rank}(AB) \leq \text{rank}(A) \).
    \item[(d)] Observe that \( (AB)^{t} = B^{t} A^{t} \). So, we have
        \[  \text{rank}((AB)^{t}) = \text{rank}(B^{t} A^{t}) \leq \text{rank}(B^{t}). \]
By {\hyperref[Corollary 3 to Theorem 3.6]{Corollary 3 to Theorem 3.6}}, we must have \( \text{rank}(B^{t}) = \text{rank}(B) \) and that \( \text{rank}((AB)^{t}) = \text{rank}(AB) \). Thus, we have that \( \text{rank}(AB) \leq \text{rank}(B) \).
    \item[(b)] Let \( \alpha, \beta,  \) and \( \gamma \) be ordered bases for \( V, W  \) and \( Z  \), respectively. Thus, \( [T]_{\alpha}^{\beta}, [U]_{\beta}^{\gamma} \) and \( [UT]_{\alpha}^{\gamma}   \) are defined. Since these are just matrices, we can see that
        \begin{align*}
            \text{rank}(UT) &= \text{rank}([UT]_{\alpha}^{\gamma} ) \\
                            &= \text{rank}([U]_{\beta}^{\gamma}  [T]_{\alpha}^{\beta} ) \tag{{\hyperref[Theorem 2.11]{Theorem 2.11}}} \\
                            &\leq \text{rank}([T]_{\alpha}^{\beta}) \tag{part (d)} \\
                            &= \text{rank}(T).
        \end{align*}
        Hence, we have \( \text{rank}(UT) \leq \text{rank}(T) \).
\end{enumerate}
\end{proof}
\begin{eg}
    
\end{eg}

\subsection{The Inverse of a Matrix}

\begin{definition}[Augmented Matrices]
   Let \( A  \) and \( B  \) be \( m \times n  \) and \( m \times p  \) matrices, respectively. By the \textbf{augmented matrix} \( (A|B)  \), we mean the \( m \times (n + p)  \) matrix \( (A|B)  \), that is, the matrix who first \( n  \) columns are the columns of \( A  \), and whose last \( p  \) columns are the columns of \( B  \). 
\end{definition}

