\section{Composition of Linear Transformations and Matrix Multiplication}
\subsection{Compositions}
In the last section, we learned that
\begin{itemize}
    \item Linear Transformations as well as their operations such as sums and scalar multiplications can represented in terms of matrices.
    \item The set of all linear transformations from \( V  \) to \( W  \) forms a vector space over some field \( F   \).
\end{itemize}

The main goal in this section is to describe how to represent the multiplication of two linear transformations and thus the multiplication of two matrices. The notation \( UT \) is used in replacement of \( U \circ T  \) for the composite of linear transformations of \( U  \) and \( T  \).

Our first goal is to prove that the composite of linear transformations is linear.

\begin{theorem}\label{Composition is Linear}
   Let \( V, W,   \) and \( Z  \) be vector spaces over the same field \( F  \), and let \( T: V \to W  \) and \( U: W \to Z  \) be linear. Then \( UT: V \to Z  \) is linear.
\end{theorem}
\begin{proof}
    Suppose \( T: V \to W   \) and \( U: W \to Z  \) are linear. Let \( cx + y \in V  \). Note that \( T(cx+y) \in W  \) and \( U(T(cx+y)) \in Z \). So, 
    \begin{align*}
        (UT)(cx+y) &= U(T(cx+y)) \tag{Definition of Composition} \\
                   &= U(cT(x) + T(y)) \tag{\( T \) linear} \\
                   &= c U(T(x)) + U(T(y)) \tag{\( U \) linear} \\
                   &=  c (UT)(x) + (UT)(y). 
    \end{align*}
    Hence, \( UT  \) is linear.
\end{proof} 

The following are a list of properties of the composition of linear transformations.

\begin{theorem}\label{Properties of Compositions}
    Let \( V  \) be a vector space. Let \( T, {U}_{1}, {U}_{2} \in \mathcal{L}(V) \). Then 
    \begin{enumerate}
        \item[(a)] \( T({U}_{1} + {U}_{2}) = {TU}_{1} + {TU}_{2}  \) and \( ({U}_{1} + {U}_{2}) T = {U}_{1} T + {U}_{2} T  \).
        \item[(b)] \( T({U}_{1} {U}_{2}) = ({TU}_{1}){U}_{2} \).
        \item[(c)] \( TI = IT = T  \).
        \item[(d)] \( a({U}_{1}{U}_{2}) = ({aU}_{1}) {U}_{2} = {U}_{1} ({aU}_{2}) \) for all scalars \( a \). 
    \end{enumerate}
\end{theorem}
\begin{proof}
    Let \( T, {U}_{1}, {U}_{2} \in \mathcal{V} \) with vector space \( V  \). 
    \begin{enumerate}
        \item[(a)] Then for \( x \in V  \), we have
    \begin{align*}
        T({U}_{1} + {U}_{2})(x) &= T \Big( ({U}_{1} + {U}_{2})(x) \Big) \tag{Def of Composition}   \\
                             &= T \Big( {U}_{1}(x) + {U}_{2}(x) \Big) \tag{\( \mathcal{L}(V) \) V.S} \\
                             &= T \Big( {U}_{1}(x) \Big) + T \Big( {U}_{2}(x) \Big) \tag{\( T \) is linear} \\  
                             &= ({TU}_{1})(x) +  ({TU}_{2})(x) \tag{Def of Composition} \\
    \end{align*}
    Hence, \( T({U}_{1} + {U}_{2}) = {TU}_{1} + {TU}_{2} \).
        
    Let \( x \in V  \) again. Then
    \begin{align*}
        \Big( ({U}_{1} + {U}_{2})T \Big) (x)  &= ({U}_{1} + {U}_{2})(T(x))   \tag{Def of Composition}  \\
                                              &= {U}_{1}(T(x)) + {U}_{2}(T(x)) \tag{\( \mathcal{L}(V) \) V.S} \\
                                              &= ({U}_{1}T)(x) + ({U}_{2}T)(x). \tag{Def of Composition}
    \end{align*}
    Hence, \( ({U}_{1}  + {U}_{2})T = ({U}_{1} T)  + {U}_{2}T  \).
    \item[(b)] Let \( x \in V  \). Then using the definition of composition, we have
    \begin{align*}
        T({U}_{1}{U}_{2})(x) &= T \Big( ({U}_{1} {U}_{2})(x) \Big) \\
                             &=  T \Big( {U}_{1} ({U}_{2}(x)) \Big) \\
                             &= ({TU}_{1})({U}_{2}(x)) \\
                             &= ({TU}_{1}){U}_{2}(x).
    \end{align*}
    Hence, \( T({U}_{1}{U}_{2}) = {TU}_{1} \).
    \item[(c)] Let \( x \in V  \). Using the definition of composition, we get
        \[  (TI)(x) = T(I(x)) = T(x) = I(T(x)) = (IT)(x). \]
        Hence, \( TI = IT = T \).
    \item[(d)] Let \( a \in F  \) and \( v \in V \). Then using the definition of composition and operations of \( \mathcal{L}(V) \), we must have
        \begin{align*}
            a({U}_{1} {U}_{2})(x) &= a {U}_{1}({U}_{2}(x)) \\
                                  &= ({aU}_{1})({U}_{2}(x)) \\
                                  &= ({U}_{1}a)({U}_{2}(x)) \\
                                  &= {U}_{1}( {aU}_{2}(x)) \\
                                  &= {U}_{1} ({aU}_{2})(x).
        \end{align*}
        Hence, we have \( a({U}_{1}{U}_{2}) = {U}_{1} ({aU}_{2}) \).
    \end{enumerate}
\end{proof}

We can also prove a more general result when \( T: V \to W   \) where  \( \text{dim}(V) \neq \text{dim}(W) \).

\subsection{Matrix Products}

\begin{definition}[Matrix Representation of the Composition]
    Let \( T: V \to W  \) and \( U: W \to Z  \) be linear transformations and let \( A = [U]_{\beta}^{\gamma}  \) and \( B = [T]_{\alpha}^{\beta}  \) where \( \alpha = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n}  \}, \beta = \{ {w}_{1}, {w}_{2}, \dots, {w}_{m} \},   \) and \( \gamma = \{ {z}_{1}, {z}_{2}, \dots,  {z}_{p} \}  \) are ordered bases for \( V, W  \) and \( Z  \), respectively. Define the product \( AB \) of two matrices so that \( AB = [UT]_{\alpha}^{\gamma}  \). 
\end{definition}

\subsection{Summation Formula for Matrix Representation of Composition}
For \( 1 \leq j \leq n \), we have
\begin{align*}
    (UT)({v}_{j}) &= U(T({v}_{j})) \tag{Definition of Composition} \\
              &= U \Big( \sum_{ k=1 }^{ m } {B}_{kj} {w}_{k } \Big) \tag{\( [T]_{\alpha}^{\beta}\) for \( 1 \leq j \leq n  \)} \\
              &= \sum_{ k=1 }^{ m } {B}_{kj} U({w}_{k }) \tag{Linearity of \( U \)}  \\
              &= \sum_{ k=1 }^{ m } {B}_{k j} \Big( \sum_{ i=1 }^{ p } {A}_{i k } {z}_{i} \Big) \tag{\( [U]_{\beta}^{\gamma}  \) for \( 1 \leq k \leq m  \)} \\  
              &= \sum_{ i=1 }^{ p  } \Big( \sum_{ k=1 }^{ m } {A}_{ik } {B}_{kj} \Big) {z}_{i} \tag{Finite Sums are Interchangeable} \\
              &= \sum_{ i=1  }^{ p } {C}_{ij} {z}_{i}
\end{align*}
where 
\[  {C}_{ij} = \sum_{ k=1 }^{ m } {A}_{ik } {B}_{kj}. \]

\begin{definition}[Product of Two Matrices]
    Let \( A  \) be an \( m \times n  \) matrix and \( B  \) be an \( n \times p  \) matrix. We define the \textbf{product} of \( A  \) and \( B  \), denoted \( AB  \), to be the \( m \times p  \) matrix such that 
    \[  {(AB)}_{ij} = \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj } \ \text{ for } \ 1 \leq i \leq m , \  1 \leq j \leq p   \]
\end{definition}

\begin{itemize}
    \item In order for the product \( AB  \) to be defined, where \( A  \) is an \( m \times n  \) matrix and \( B  \) is a \( n \times  p  \), the two inner dimensions must be equal (in this case \( n  \)).
    \item Subsequently, the two outer dimensions (namely, \( m \) and \( p \)) determine the size of the resulting matrix.
\end{itemize}

\begin{eg}
    We have
    \[  \begin{pmatrix}
        1 & 2 & 1 \\
        0 & 4 & -1
    \end{pmatrix} \begin{pmatrix}
        4 \\
        2 \\
        5 
    \end{pmatrix} = \begin{pmatrix}
        1 \cdot 4 + 2 \cdot 2 + 1 \cdot 5 \\
        0 \cdot 4 + 4 \cdot 2 + (-1) \cdot 5
    \end{pmatrix} = \begin{pmatrix}
        13 \\
        3
    \end{pmatrix}. \]
    Notice how the number of columns of the first matrix matches the number of rows on the second.
\end{eg}

 Matrix multiplication is not commutative; that is, it is not always the case that \( AB =  BA  \).If \( A  \) and \( B  \) have corresponding inner dimensions; that is, if \( A  \) is an \( m \times n  \) matrix and \( B  \) is an \( n \times p  \) matrix, then \( (AB)^{t} = B^{t} A^{t} \)since
 \[  (AB)^{t}_{ij} = (AB)_{ji} = \sum_{ k=1 }^{ n } {A}_{jk } {B}_{ki} \] 
 and 
 \[  {(B^{t}A^{t})}_{ij} = \sum_{ k=1 }^{ n } (B^{t})_{ik} {(A)^{t}}_{kj } = \sum_{ k=1 }^{ n } {B}_{ki} {A}_{jk }.  \]

\begin{theorem}
    Let \( V,W,   \) and \( Z  \) be finite-dimensional vector spaces with ordered bases \( \alpha , \beta ,  \) and \( \gamma \) respectively. Let \( T: V \to W  \) and \( U : W \to Z  \) be linear transformations. Then
    \[  [UT]_{\alpha}^{\gamma} = [U]_{\beta}^{\gamma} [T]_{\alpha}^{\beta}. \]
\end{theorem} 
\begin{proof}
This fact follows immediately from our definition of matrix products.
\end{proof}

\begin{corollary}
    Let \( V  \) be a finite-dimensional vector space with an ordered basis \( \beta  \). Let \( T,U \in \mathcal{L}(V) \). Then \( [UT]_{\beta}^{}  = [U]_{\beta}^{}  [T]_{\beta}^{}  \).
\end{corollary}
 

