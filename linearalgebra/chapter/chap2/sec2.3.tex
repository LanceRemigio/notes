\section{Composition of Linear Transformations and Matrix Multiplication}
\subsection{Compositions}
In the last section, we learned that
\begin{itemize}
    \item Linear Transformations as well as their operations such as sums and scalar multiplications can represented in terms of matrices.
    \item The set of all linear transformations from \( V  \) to \( W  \) forms a vector space over some field \( F   \).
\end{itemize}

The main goal in this section is to describe how to represent the multiplication of two linear transformations and thus the multiplication of two matrices. The notation \( UT \) is used in replacement of \( U \circ T  \) for the composite of linear transformations of \( U  \) and \( T  \).

Our first goal is to prove that the composite of linear transformations is linear.

\begin{theorem}\label{Composition is Linear}
   Let \( V, W,   \) and \( Z  \) be vector spaces over the same field \( F  \), and let \( T: V \to W  \) and \( U: W \to Z  \) be linear. Then \( UT: V \to Z  \) is linear.
\end{theorem}
\begin{proof}
    Suppose \( T: V \to W   \) and \( U: W \to Z  \) are linear. Let \( cx + y \in V  \). Note that \( T(cx+y) \in W  \) and \( U(T(cx+y)) \in Z \). So, 
    \begin{align*}
        (UT)(cx+y) &= U(T(cx+y)) \tag{Definition of Composition} \\
                   &= U(cT(x) + T(y)) \tag{\( T \) linear} \\
                   &= c U(T(x)) + U(T(y)) \tag{\( U \) linear} \\
                   &=  c (UT)(x) + (UT)(y). 
    \end{align*}
    Hence, \( UT  \) is linear.
\end{proof} 

The following are a list of properties of the composition of linear transformations.

\begin{theorem}\label{Properties of Compositions}
    Let \( V  \) be a vector space. Let \( T, {U}_{1}, {U}_{2} \in \mathcal{L}(V) \). Then 
    \begin{enumerate}
        \item[(a)] \( T({U}_{1} + {U}_{2}) = {TU}_{1} + {TU}_{2}  \) and \( ({U}_{1} + {U}_{2}) T = {U}_{1} T + {U}_{2} T  \).
        \item[(b)] \( T({U}_{1} {U}_{2}) = ({TU}_{1}){U}_{2} \).
        \item[(c)] \( TI = IT = T  \).
        \item[(d)] \( a({U}_{1}{U}_{2}) = ({aU}_{1}) {U}_{2} = {U}_{1} ({aU}_{2}) \) for all scalars \( a \). 
    \end{enumerate}
\end{theorem}
\begin{proof}
    Let \( T, {U}_{1}, {U}_{2} \in \mathcal{V} \) with vector space \( V  \). 
    \begin{enumerate}
        \item[(a)] Then for \( x \in V  \), we have
    \begin{align*}
        T({U}_{1} + {U}_{2})(x) &= T \Big( ({U}_{1} + {U}_{2})(x) \Big) \tag{Def of Composition}   \\
                             &= T \Big( {U}_{1}(x) + {U}_{2}(x) \Big) \tag{\( \mathcal{L}(V) \) V.S} \\
                             &= T \Big( {U}_{1}(x) \Big) + T \Big( {U}_{2}(x) \Big) \tag{\( T \) is linear} \\  
                             &= ({TU}_{1})(x) +  ({TU}_{2})(x) \tag{Def of Composition} \\
    \end{align*}
    Hence, \( T({U}_{1} + {U}_{2}) = {TU}_{1} + {TU}_{2} \).
        
    Let \( x \in V  \) again. Then
    \begin{align*}
        \Big( ({U}_{1} + {U}_{2})T \Big) (x)  &= ({U}_{1} + {U}_{2})(T(x))   \tag{Def of Composition}  \\
                                              &= {U}_{1}(T(x)) + {U}_{2}(T(x)) \tag{\( \mathcal{L}(V) \) V.S} \\
                                              &= ({U}_{1}T)(x) + ({U}_{2}T)(x). \tag{Def of Composition}
    \end{align*}
    Hence, \( ({U}_{1}  + {U}_{2})T = ({U}_{1} T)  + {U}_{2}T  \).
    \item[(b)] Let \( x \in V  \). Then using the definition of composition, we have
    \begin{align*}
        T({U}_{1}{U}_{2})(x) &= T \Big( ({U}_{1} {U}_{2})(x) \Big) \\
                             &=  T \Big( {U}_{1} ({U}_{2}(x)) \Big) \\
                             &= ({TU}_{1})({U}_{2}(x)) \\
                             &= ({TU}_{1}){U}_{2}(x).
    \end{align*}
    Hence, \( T({U}_{1}{U}_{2}) = {TU}_{1} \).
    \item[(c)] Let \( x \in V  \). Using the definition of composition, we get
        \[  (TI)(x) = T(I(x)) = T(x) = I(T(x)) = (IT)(x). \]
        Hence, \( TI = IT = T \).
    \item[(d)] Let \( a \in F  \) and \( v \in V \). Then using the definition of composition and operations of \( \mathcal{L}(V) \), we must have
        \begin{align*}
            a({U}_{1} {U}_{2})(x) &= a {U}_{1}({U}_{2}(x)) \\
                                  &= ({aU}_{1})({U}_{2}(x)) \\
                                  &= ({U}_{1}a)({U}_{2}(x)) \\
                                  &= {U}_{1}( {aU}_{2}(x)) \\
                                  &= {U}_{1} ({aU}_{2})(x).
        \end{align*}
        Hence, we have \( a({U}_{1}{U}_{2}) = {U}_{1} ({aU}_{2}) \).
    \end{enumerate}
\end{proof}

We can also prove a more general result when \( T: V \to W   \) where  \( \text{dim}(V) \neq \text{dim}(W) \).

\subsection{Matrix Products}

\begin{definition}[Matrix Representation of the Composition]
    Let \( T: V \to W  \) and \( U: W \to Z  \) be linear transformations and let \( A = [U]_{\beta}^{\gamma}  \) and \( B = [T]_{\alpha}^{\beta}  \) where \( \alpha = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n}  \}, \beta = \{ {w}_{1}, {w}_{2}, \dots, {w}_{m} \},   \) and \( \gamma = \{ {z}_{1}, {z}_{2}, \dots,  {z}_{p} \}  \) are ordered bases for \( V, W  \) and \( Z  \), respectively. Define the product \( AB \) of two matrices so that \( AB = [UT]_{\alpha}^{\gamma}  \). 
\end{definition}

\subsection{Summation Formula for Matrix Representation of Composition}
For \( 1 \leq j \leq n \), we have
\begin{align*}
    (UT)({v}_{j}) &= U(T({v}_{j})) \tag{Definition of Composition} \\
              &= U \Big( \sum_{ k=1 }^{ m } {B}_{kj} {w}_{k } \Big) \tag{\( [T]_{\alpha}^{\beta}\) for \( 1 \leq j \leq n  \)} \\
              &= \sum_{ k=1 }^{ m } {B}_{kj} U({w}_{k }) \tag{Linearity of \( U \)}  \\
              &= \sum_{ k=1 }^{ m } {B}_{k j} \Big( \sum_{ i=1 }^{ p } {A}_{i k } {z}_{i} \Big) \tag{\( [U]_{\beta}^{\gamma}  \) for \( 1 \leq k \leq m  \)} \\  
              &= \sum_{ i=1 }^{ p  } \Big( \sum_{ k=1 }^{ m } {A}_{ik } {B}_{kj} \Big) {z}_{i} \tag{Finite Sums are Interchangeable} \\
              &= \sum_{ i=1  }^{ p } {C}_{ij} {z}_{i}
\end{align*}
where 
\[  {C}_{ij} = \sum_{ k=1 }^{ m } {A}_{ik } {B}_{kj}. \]

\begin{definition}[Product of Two Matrices]
    Let \( A  \) be an \( m \times n  \) matrix and \( B  \) be an \( n \times p  \) matrix. We define the \textbf{product} of \( A  \) and \( B  \), denoted \( AB  \), to be the \( m \times p  \) matrix such that 
    \[  {(AB)}_{ij} = \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj } \ \text{ for } \ 1 \leq i \leq m , \  1 \leq j \leq p   \]
\end{definition}

\begin{itemize}
    \item In order for the product \( AB  \) to be defined, where \( A  \) is an \( m \times n  \) matrix and \( B  \) is a \( n \times  p  \), the two inner dimensions must be equal (in this case \( n  \)).
    \item Subsequently, the two outer dimensions (namely, \( m \) and \( p \)) determine the size of the resulting matrix.
\end{itemize}

\begin{eg}
    We have
    \[  \begin{pmatrix}
        1 & 2 & 1 \\
        0 & 4 & -1
    \end{pmatrix} \begin{pmatrix}
        4 \\
        2 \\
        5 
    \end{pmatrix} = \begin{pmatrix}
        1 \cdot 4 + 2 \cdot 2 + 1 \cdot 5 \\
        0 \cdot 4 + 4 \cdot 2 + (-1) \cdot 5
    \end{pmatrix} = \begin{pmatrix}
        13 \\
        3
    \end{pmatrix}. \]
    Notice how the number of columns of the first matrix matches the number of rows on the second.
\end{eg}

 Matrix multiplication is not commutative; that is, it is not always the case that \( AB =  BA  \).If \( A  \) and \( B  \) have corresponding inner dimensions; that is, if \( A  \) is an \( m \times n  \) matrix and \( B  \) is an \( n \times p  \) matrix, then \( (AB)^{t} = B^{t} A^{t} \)since
 \[  (AB)^{t}_{ij} = (AB)_{ji} = \sum_{ k=1 }^{ n } {A}_{jk } {B}_{ki} \] 
 and 
 \[  {(B^{t}A^{t})}_{ij} = \sum_{ k=1 }^{ n } (B^{t})_{ik} {(A)^{t}}_{kj } = \sum_{ k=1 }^{ n } {B}_{ki} {A}_{jk }.  \]

\begin{theorem}
    Let \( V,W,   \) and \( Z  \) be finite-dimensional vector spaces with ordered bases \( \alpha , \beta ,  \) and \( \gamma \) respectively. Let \( T: V \to W  \) and \( U : W \to Z  \) be linear transformations. Then
    \[  [UT]_{\alpha}^{\gamma} = [U]_{\beta}^{\gamma} [T]_{\alpha}^{\beta}. \]
\end{theorem} 
\begin{proof}
This fact follows immediately from our definition of matrix products.
\end{proof}

\begin{corollary}
    Let \( V  \) be a finite-dimensional vector space with an ordered basis \( \beta  \). Let \( T,U \in \mathcal{L}(V) \). Then \( [UT]_{\beta}^{}  = [U]_{\beta}^{}  [T]_{\beta}^{}  \).
\end{corollary}

\begin{eg}
    Let \( U: {P}_{3}(\R) \to {P}_{2}(\R) \) and \( T: {P}_{2}(\R) \to {P}_{3}(\R) \) be the linear transformations respectively defined by
    \[ U(f(x)) = f'(x)  \ \text{ and } \ T(f(x)) = \int_{ 0 }^{ x } f(t) \ dt.  \]
    Let \( \alpha  \) and \( \beta  \) be the standard ordered bases of \( {P}_{3}(\R) \) and \( {P}_{2}(\R) \), respectively. We claim that \( UT = I \). To see why this is the case, observe that
    \[ [UT]_{\beta}^{}  = [U]_{\alpha}^{\beta}  [T]_{\beta}^{\alpha} = \begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 3
        \end{pmatrix} \begin{pmatrix}
            0 & 0 & 0 \\
            1 & 0 & 0 \\
            0 & \frac{ 1 }{ 2 } & 0 \\
            0 & 0 & \frac{ 1 }{ 3 }  
            \end{pmatrix}  = \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1 
    \end{pmatrix} = [I]_{\beta}^{}.  \]
\end{eg}


\begin{definition}[Kronecker Delta]
    We define the \textbf{Kronecker delta} \( {\delta}_{ij} \) by \( {\delta}_{ij} = 1  \) if \( i = j  \) and \( {\delta}_{ij} = 0  \) if \( i \neq j  \). 
\end{definition}

\begin{definition}[Identity Matrix]
   The \( n \times n  \) \textbf{identity matrix} \( {I}_{n} \) is defined by \( {({I}_{n})}_{ij} = {\delta}_{ij} \). 
\end{definition}

In other words, the identity matrix is made up of the Kronecker delta constants where entries along the diagonal are equal to 1 and 0 otherwise.

\begin{theorem}[Properties of Matrices]
    Let \( A  \) be an \( m \times n  \) matrix, \( B  \) and \(  C  \) be \( n \times p  \) matrices, and \( D  \) and \( E  \) be \( q \times  m  \) matrices. Then
    \begin{enumerate}
        \item[(a)] \( A(B+C) = AB + AC  \) and \( (D+E) A = DA + EA  \).
        \item[(b)] \( a(AB) = (aA)B  = A(aB)\) for any scalar \( a  \).
        \item[(c)] \( {I}_{m} A = A = A {I}_{n} \).
        \item[(d)] If \( V  \) is an \( n- \)dimensional vector space with an ordered basis \( \beta  \), then \( [{I}_{V}]_{\beta}^{}  = {I}_{n} \).
    \end{enumerate}
\end{theorem}
\begin{proof}
    Let \( A  \) be an \( m \times n  \) matrix, \( B  \) and \(  C  \) be \( n \times p  \) matrices, and \( D  \) and \( E  \) be \( q \times  m  \) matrices. Then
\begin{enumerate}
    \item[(a)] Let \( 1 \leq i \leq m  \) and \( 1 \leq j \leq p  \). By definition of the product of two matrices, we have 
        \begin{align*}
        A(B+C) &= \sum_{ k=1 }^{ n } {A}_{ik } {(B+C)}_{kj} \\ 
               &= \sum_{ k=1 }^{ n } {A}_{ik } ({B}_{kj} + {C}_{kj} ) \\
               &= \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj} + \sum_{ k=1 }^{ n } {A}_{ik} {C}_{kj} \\ 
               &= AB + AC.    
        \end{align*}
        Hence, \( A(B+C) = AB + AC  \).
        
        Now, let \( 1 \leq i \leq q  \) and \( 1 \leq j \leq n  \). For the second formula, we can use the same definition to write
    \begin{align*}
        (D+E) A &= \sum_{ k=1 }^{ m } {(D+E)}_{ik} {A}_{kj}    \\
                &= \sum_{ k=1 }^{ m } ({D}_{ik } + {E}_{ik }) {A}_{kj} \\
                &= \sum_{ k=1 }^{ m } {D}_{ik} {A}_{kj } + \sum_{ i=1 }^{ m   } {E}_{ik } {A}_{kj} \\
                &= DA + EA.
    \end{align*}
    Hence, \( (D+E) A = DA + EA \).
    \item[(b)] Let \(  1 \leq i \leq m  \) and \( 1 \leq j \leq p  \). Let \( a \in F  \). Then using the definition of the product once again, we have
        \begin{align*}
            a (AB) &= a \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj} \\
                   &= \sum_{ k=1 }^{ n } a ({A}_{ik } {B}_{kj }) \\
                   &= \sum_{ k=1 }^{ n } ({aA}_{ik}) {B}_{kj } \\
                   &= \sum_{ k=1 }^{ n } {(aA)}_{ik } {B}_{kj} \\
                   &= (aA)B.
        \end{align*}
    Then observe that 
    \begin{align*}  
        (aA)B &= \sum_{ k = 1  }^{ n  } (a {A}_{ik } ) {B}_{kj} \\ 
    &= \sum_{ k=1 }^{ n } ({A}_{ik } a ) {B}_{kj}  \\
    &= \sum_{ k=1 }^{ n } {A}_{ik } (a {B}_{kj})  \\
    &= \sum_{ k=1 }^{ n } {A}_{ik } {(aB)}_{kj} \\
    &= A (aB).
    \end{align*}
    Hence, \( (aA)B = A(aB) \). Thus, we conclude that
    \[ a(AB) = (aA)B = A(aB).  \]
\item[(c)] Let \(   1 \leq i \leq m  \) and \(  1 \leq j \leq n  \). Since \( {\delta}_{ik} = 1   \) only when \( i = k  \) and \( 0  \) otherwise, we must have
    \begin{align*}
        ({I}_{m} A)_{ij} &= \sum_{ k=1  }^{ m  } {\delta}_{ik } {A}_{kj}  = {A}_{ij }. \\
    \end{align*}
    Likewise, 
    \[  {(A {I}_{n} )}_{ij } = \sum_{ k= 1  }^{ n  } {A}_{ik } {\delta}_{k j } = {A}_{ij }  \] by the same reasoning. Hence, we have
    \[  {I}_{m} A = A = A {I}_{n}. \]
    \item[(d)] Let \( V  \) be an \( n- \)dimensional vector space with \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) as an ordered basis. Since \( {I}_{V}({v}_{j}) = {v}_{j }  \) for all \( 1 \leq j \leq n  \), we must have
        \[  {v}_{j} = {I}_{V}({v}_{j}) = \sum_{ i=1 }^{ n } {a}_{ij} {v}_{i}   \]
        which holds only if \(  {a}_{ij} = 1  \) for all \( i = j  \) and \( 0  \) otherwise. But this means that \( {a}_{ij} = {\delta}_{ij}  \), so \( [{I}_{V}]_{\beta}^{}  = {I}_{n} \).
        
\end{enumerate}
\end{proof}

\begin{corollary}
    Let \( A  \) be an \( m \times n  \) matrix, \( {B}_{1}, {B}_{2}, \dots, {B}_{k } \) be \( n \times  p  \) matrices, \( {C}_{1}, {C}_{2}, \dots ,{C}_{k }  \) be \( q \times  m  \) matrices, and \( {a}_{1}, {a}_{2}, \dots, {a}_{k }  \) be scalars. Then 
    \[  A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i}  \Big) = \sum_{ i=1  }^{  k  } {a}_{i} {AB}_{i} \]
    and
    \[ \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i}  \Big) A = \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i} A. \]
\end{corollary}

\begin{proof}
Let \( A  \) be an \( m \times n  \) matrix and \( {B}_{1}, {B}_{2}, \dots, {B}_{k } \) be \( n \times p  \) matrices. Let \( 1 \leq \ell \leq m  \) and \( 1 \leq s  \leq p  \). Then
\begin{align*}
    \Big[ A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i} \Big) \Big]_{\ell s } &= \sum_{ \lambda = 1  }^{ n } {A}_{ \ell \lambda }  \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i} \Big)_{\lambda s }  \\
                                                                        &= \sum_{ \lambda = 1  }^{ n  } {A}_{ \ell \lambda } \Big(  \sum_{ i=1  }^{  k  } {({a}_{i} {B}_{i})}_{\lambda s } \Big)  \\
                                                                        &=  \sum_{ \lambda = 1  }^{ n  } {A}_{ \ell \lambda } \Big( \sum_{ i=1 }^{ k  } {a}_{i} ({B}_{i})_{\lambda s } \Big) \\
                                                                        &= \sum_{ i=1  }^{ k  } {a}_{i} \Big( \sum_{ \lambda =1  }^{ n  } {A}_{ \ell \lambda } ({B}_{i})_{\lambda s } \Big) \tag{part (a) of Theorem 2.12}  .  
\end{align*}
Hence, we have
\[  A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i}  \Big) = \sum_{ i=1  }^{  k  } {a}_{i} {AB}_{i}.\]

To show the second formula, let \( 1 \leq \ell \leq q  \) and \( 1 \leq  s  \leq n   \). Then
\begin{align*}
    \Big[ \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i}  \Big) A  \Big]_{\ell \lambda }  &= \sum_{ \lambda = 1  }^{ n   } \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i} \Big)_{\ell \lambda } {A}_{\lambda s }  \\
                                                                                     &= \sum_{ \lambda = 1  }^{ n } \Big( \sum_{ i=1 }^{  k   } ({a}_{i} {C}_{i})_{\ell \lambda }  \Big) {A}_{\lambda s } \\
                                                                                     &= \sum_{ \lambda = 1  }^{ n } \Big( \sum_{ i=1 }^{  k   } {a}_{i} ({C}_{i})_{\ell \lambda }  \Big) {A}_{\lambda s } \\
                                                                                     &= \sum_{ i =1  }^{ k  } {a}_{i} \Big( \sum_{ i=1  }^{ k  } {({C}_{i})}_{\ell \lambda } {A}_{\lambda s }  \Big) \tag{part (a) of Theorem 2.12}  .
\end{align*}
Hence, we have
\[  \Big( \sum_{ i=1  }^{  k  } {a}_{i} {C}_{i}  \Big) A = \sum_{ i=1  }^{ k  } {a}_{i} {C}_{i} A. \]
\end{proof}



\begin{theorem} 
   Let \( A  \) be an \( m \times n  \) matrix and \( B  \) be an \( n \times p  \) matrix. For each \( j  \) (\( 1 \leq j \leq p  \)) let \( {u}_{j}  \) and \(  {v}_{j}  \) denote the \( j \)th columns of \( AB  \) and \( B  \) respectively. Then
    \begin{enumerate}
        \item[(a)]  \( {u}_{j} = A {v}_{j}  \)
        \item[(b)] \( {v}_{j} = B {e}_{j }  \), where \( {e}_{j}  \) is the \( j \)th standard vector of \( F^{p} \).
    \end{enumerate}
\end{theorem}


