\section{Composition of Linear Transformations and Matrix Multiplication}
\subsection{Compositions}
In the last section, we learned that
\begin{itemize}
    \item Linear Transformations as well as their operations such as sums and scalar multiplications can represented in terms of matrices.
    \item The set of all linear transformations from \( V  \) to \( W  \) forms a vector space over some field \( F   \).
\end{itemize}

The main goal in this section is to describe how to represent the multiplication of two linear transformations and thus the multiplication of two matrices. The notation \( UT \) is used in replacement of \( U \circ T  \) for the composite of linear transformations of \( U  \) and \( T  \).

Our first goal is to prove that the composite of linear transformations is linear.

\begin{theorem}\label{Composition is Linear}
   Let \( V, W,   \) and \( Z  \) be vector spaces over the same field \( F  \), and let \( T: V \to W  \) and \( U: W \to Z  \) be linear. Then \( UT: V \to Z  \) is linear.
\end{theorem}
\begin{proof}
    Suppose \( T: V \to W   \) and \( U: W \to Z  \) are linear. Let \( cx + y \in V  \). Note that \( T(cx+y) \in W  \) and \( U(T(cx+y)) \in Z \). So, 
    \begin{align*}
        (UT)(cx+y) &= U(T(cx+y)) \tag{Definition of Composition} \\
                   &= U(cT(x) + T(y)) \tag{\( T \) linear} \\
                   &= c U(T(x)) + U(T(y)) \tag{\( U \) linear} \\
                   &=  c (UT)(x) + (UT)(y). 
    \end{align*}
    Hence, \( UT  \) is linear.
\end{proof} 

The following are a list of properties of the composition of linear transformations.

\begin{theorem}\label{Properties of Compositions}
    Let \( V  \) be a vector space. Let \( T, {U}_{1}, {U}_{2} \in \mathcal{L}(V) \). Then 
    \begin{enumerate}
        \item[(a)] \( T({U}_{1} + {U}_{2}) = {TU}_{1} + {TU}_{2}  \) and \( ({U}_{1} + {U}_{2}) T = {U}_{1} T + {U}_{2} T  \).
        \item[(b)] \( T({U}_{1} {U}_{2}) = ({TU}_{1}){U}_{2} \).
        \item[(c)] \( TI = IT = T  \).
        \item[(d)] \( a({U}_{1}{U}_{2}) = ({aU}_{1}) {U}_{2} = {U}_{1} ({aU}_{2}) \) for all scalars \( a \). 
    \end{enumerate}
\end{theorem}
\begin{proof}
    Let \( T, {U}_{1}, {U}_{2} \in \mathcal{V} \) with vector space \( V  \). 
    \begin{enumerate}
        \item[(a)] Then for \( x \in V  \), we have
    \begin{align*}
        T({U}_{1} + {U}_{2})(x) &= T \Big( ({U}_{1} + {U}_{2})(x) \Big) \tag{Def of Composition}   \\
                             &= T \Big( {U}_{1}(x) + {U}_{2}(x) \Big) \tag{\( \mathcal{L}(V) \) V.S} \\
                             &= T \Big( {U}_{1}(x) \Big) + T \Big( {U}_{2}(x) \Big) \tag{\( T \) is linear} \\  
                             &= ({TU}_{1})(x) +  ({TU}_{2})(x) \tag{Def of Composition} \\
    \end{align*}
    Hence, \( T({U}_{1} + {U}_{2}) = {TU}_{1} + {TU}_{2} \).
        
    Let \( x \in V  \) again. Then
    \begin{align*}
        \Big( ({U}_{1} + {U}_{2})T \Big) (x)  &= ({U}_{1} + {U}_{2})(T(x))   \tag{Def of Composition}  \\
                                              &= {U}_{1}(T(x)) + {U}_{2}(T(x)) \tag{\( \mathcal{L}(V) \) V.S} \\
                                              &= ({U}_{1}T)(x) + ({U}_{2}T)(x). \tag{Def of Composition}
    \end{align*}
    Hence, \( ({U}_{1}  + {U}_{2})T = ({U}_{1} T)  + {U}_{2}T  \).
    \item[(b)] Let \( x \in V  \). Then using the definition of composition, we have
    \begin{align*}
        T({U}_{1}{U}_{2})(x) &= T \Big( ({U}_{1} {U}_{2})(x) \Big) \\
                             &=  T \Big( {U}_{1} ({U}_{2}(x)) \Big) \\
                             &= ({TU}_{1})({U}_{2}(x)) \\
                             &= ({TU}_{1}){U}_{2}(x).
    \end{align*}
    Hence, \( T({U}_{1}{U}_{2}) = {TU}_{1} \).
    \item[(c)] Let \( x \in V  \). Using the definition of composition, we get
        \[  (TI)(x) = T(I(x)) = T(x) = I(T(x)) = (IT)(x). \]
        Hence, \( TI = IT = T \).
    \item[(d)] Let \( a \in F  \) and \( v \in V \). Then using the definition of composition and operations of \( \mathcal{L}(V) \), we must have
        \begin{align*}
            a({U}_{1} {U}_{2})(x) &= a {U}_{1}({U}_{2}(x)) \\
                                  &= ({aU}_{1})({U}_{2}(x)) \\
                                  &= ({U}_{1}a)({U}_{2}(x)) \\
                                  &= {U}_{1}( {aU}_{2}(x)) \\
                                  &= {U}_{1} ({aU}_{2})(x).
        \end{align*}
        Hence, we have \( a({U}_{1}{U}_{2}) = {U}_{1} ({aU}_{2}) \).
    \end{enumerate}
\end{proof}

We can also prove a more general result when \( T: V \to W   \) where  \( \text{dim}(V) \neq \text{dim}(W) \).

\subsection{Matrix Products}

\begin{definition}[Matrix Representation of the Composition]
    Let \( T: V \to W  \) and \( U: W \to Z  \) be linear transformations and let \( A = [U]_{\beta}^{\gamma}  \) and \( B = [T]_{\alpha}^{\beta}  \) where \( \alpha = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n}  \}, \beta = \{ {w}_{1}, {w}_{2}, \dots, {w}_{m} \},   \) and \( \gamma = \{ {z}_{1}, {z}_{2}, \dots,  {z}_{p} \}  \) are ordered bases for \( V, W  \) and \( Z  \), respectively. Define the product \( AB \) of two matrices so that \( AB = [UT]_{\alpha}^{\gamma}  \). 
\end{definition}

\subsection{Summation Formula for Matrix Representation of Composition}
For \( 1 \leq j \leq n \), we have
\begin{align*}
    (UT)({v}_{j}) &= U(T({v}_{j})) \tag{Definition of Composition} \\
              &= U \Big( \sum_{ k=1 }^{ m } {B}_{kj} {w}_{k } \Big) \tag{\( [T]_{\alpha}^{\beta}\) for \( 1 \leq j \leq n  \)} \\
              &= \sum_{ k=1 }^{ m } {B}_{kj} U({w}_{k }) \tag{Linearity of \( U \)}  \\
              &= \sum_{ k=1 }^{ m } {B}_{k j} \Big( \sum_{ i=1 }^{ p } {A}_{i k } {z}_{i} \Big) \tag{\( [U]_{\beta}^{\gamma}  \) for \( 1 \leq k \leq m  \)} \\  
              &= \sum_{ i=1 }^{ p  } \Big( \sum_{ k=1 }^{ m } {A}_{ik } {B}_{kj} \Big) {z}_{i} \tag{Finite Sums are Interchangeable} \\
              &= \sum_{ i=1  }^{ p } {C}_{ij} {z}_{i}
\end{align*}
where 
\[  {C}_{ij} = \sum_{ k=1 }^{ m } {A}_{ik } {B}_{kj}. \]

\begin{definition}[Product of Two Matrices]
    Let \( A  \) be an \( m \times n  \) matrix and \( B  \) be an \( n \times p  \) matrix. We define the \textbf{product} of \( A  \) and \( B  \), denoted \( AB  \), to be the \( m \times p  \) matrix such that 
    \[  {(AB)}_{ij} = \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj } \ \text{ for } \ 1 \leq i \leq m , \  1 \leq j \leq p   \]
\end{definition}

\begin{itemize}
    \item In order for the product \( AB  \) to be defined, where \( A  \) is an \( m \times n  \) matrix and \( B  \) is a \( n \times  p  \), the two inner dimensions must be equal (in this case \( n  \)).
    \item Subsequently, the two outer dimensions (namely, \( m \) and \( p \)) determine the size of the resulting matrix.
\end{itemize}

\begin{eg}
    We have
    \[  \begin{pmatrix}
        1 & 2 & 1 \\
        0 & 4 & -1
    \end{pmatrix} \begin{pmatrix}
        4 \\
        2 \\
        5 
    \end{pmatrix} = \begin{pmatrix}
        1 \cdot 4 + 2 \cdot 2 + 1 \cdot 5 \\
        0 \cdot 4 + 4 \cdot 2 + (-1) \cdot 5
    \end{pmatrix} = \begin{pmatrix}
        13 \\
        3
    \end{pmatrix}. \]
    Notice how the number of columns of the first matrix matches the number of rows on the second.
\end{eg}

 Matrix multiplication is not commutative; that is, it is not always the case that \( AB =  BA  \).If \( A  \) and \( B  \) have corresponding inner dimensions; that is, if \( A  \) is an \( m \times n  \) matrix and \( B  \) is an \( n \times p  \) matrix, then \( (AB)^{t} = B^{t} A^{t} \)since
 \[  (AB)^{t}_{ij} = (AB)_{ji} = \sum_{ k=1 }^{ n } {A}_{jk } {B}_{ki} \] 
 and 
 \[  {(B^{t}A^{t})}_{ij} = \sum_{ k=1 }^{ n } (B^{t})_{ik} {(A)^{t}}_{kj } = \sum_{ k=1 }^{ n } {B}_{ki} {A}_{jk }.  \]

\begin{theorem}
    Let \( V,W,   \) and \( Z  \) be finite-dimensional vector spaces with ordered bases \( \alpha , \beta ,  \) and \( \gamma \) respectively. Let \( T: V \to W  \) and \( U : W \to Z  \) be linear transformations. Then
    \[  [UT]_{\alpha}^{\gamma} = [U]_{\beta}^{\gamma} [T]_{\alpha}^{\beta}. \]
\end{theorem} 
\begin{proof}
This fact follows immediately from our definition of matrix products.
\end{proof}

\begin{corollary}
    Let \( V  \) be a finite-dimensional vector space with an ordered basis \( \beta  \). Let \( T,U \in \mathcal{L}(V) \). Then \( [UT]_{\beta}^{}  = [U]_{\beta}^{}  [T]_{\beta}^{}  \).
\end{corollary}

\begin{eg}
    Let \( U: {P}_{3}(\R) \to {P}_{2}(\R) \) and \( T: {P}_{2}(\R) \to {P}_{3}(\R) \) be the linear transformations respectively defined by
    \[ U(f(x)) = f'(x)  \ \text{ and } \ T(f(x)) = \int_{ 0 }^{ x } f(t) \ dt.  \]
    Let \( \alpha  \) and \( \beta  \) be the standard ordered bases of \( {P}_{3}(\R) \) and \( {P}_{2}(\R) \), respectively. We claim that \( UT = I \). To see why this is the case, observe that
    \[ [UT]_{\beta}^{}  = [U]_{\alpha}^{\beta}  [T]_{\beta}^{\alpha} = \begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 3
        \end{pmatrix} \begin{pmatrix}
            0 & 0 & 0 \\
            1 & 0 & 0 \\
            0 & \frac{ 1 }{ 2 } & 0 \\
            0 & 0 & \frac{ 1 }{ 3 }  
            \end{pmatrix}  = \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1 
    \end{pmatrix} = [I]_{\beta}^{}.  \]
\end{eg}


\begin{definition}[Kronecker Delta]
    We define the \textbf{Kronecker delta} \( {\delta}_{ij} \) by \( {\delta}_{ij} = 1  \) if \( i = j  \) and \( {\delta}_{ij} = 0  \) if \( i \neq j  \). 
\end{definition}

\begin{definition}[Identity Matrix]
   The \( n \times n  \) \textbf{identity matrix} \( {I}_{n} \) is defined by \( {({I}_{n})}_{ij} = {\delta}_{ij} \). 
\end{definition}

In other words, the identity matrix is made up of the Kronecker delta constants where entries along the diagonal are equal to 1 and 0 otherwise.

\begin{theorem}[Properties of Matrices]\label{prop of matrices}
    Let \( A  \) be an \( m \times n  \) matrix, \( B  \) and \(  C  \) be \( n \times p  \) matrices, and \( D  \) and \( E  \) be \( q \times  m  \) matrices. Then
    \begin{enumerate}
        \item[(a)] \( A(B+C) = AB + AC  \) and \( (D+E) A = DA + EA  \).
        \item[(b)] \( a(AB) = (aA)B  = A(aB)\) for any scalar \( a  \).
        \item[(c)] \( {I}_{m} A = A = A {I}_{n} \).
        \item[(d)] If \( V  \) is an \( n- \)dimensional vector space with an ordered basis \( \beta  \), then \( [{I}_{V}]_{\beta}^{}  = {I}_{n} \).
    \end{enumerate}
\end{theorem}
\begin{proof}
    Let \( A  \) be an \( m \times n  \) matrix, \( B  \) and \(  C  \) be \( n \times p  \) matrices, and \( D  \) and \( E  \) be \( q \times  m  \) matrices. Then
\begin{enumerate}
    \item[(a)] Let \( 1 \leq i \leq m  \) and \( 1 \leq j \leq p  \). By definition of the product of two matrices, we have 
        \begin{align*}
            (A(B+C))_{ij} &= \sum_{ k=1 }^{ n } {A}_{ik } {(B+C)}_{kj} \\ 
               &= \sum_{ k=1 }^{ n } {A}_{ik } ({B}_{kj} + {C}_{kj} ) \\
               &= \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj} + \sum_{ k=1 }^{ n } {A}_{ik} {C}_{kj} \\ 
               &= (AB)_{ij} + (AC)_{ij}.    
        \end{align*}
        Hence, \( A(B+C) = AB + AC  \).
        
        Now, let \( 1 \leq i \leq q  \) and \( 1 \leq j \leq n  \). For the second formula, we can use the same definition to write
    \begin{align*}
        ((D+E) A)_{ij} &= \sum_{ k=1 }^{ m } {(D+E)}_{ik} {A}_{kj}    \\
                &= \sum_{ k=1 }^{ m } ({D}_{ik } + {E}_{ik }) {A}_{kj} \\
                &= \sum_{ k=1 }^{ m } {D}_{ik} {A}_{kj } + \sum_{ i=1 }^{ m   } {E}_{ik } {A}_{kj} \\
                &= (DA)_{ij} + (EA)_{ij}.
    \end{align*}
    Hence, \( (D+E) A = DA + EA \).
    \item[(b)] Let \(  1 \leq i \leq m  \) and \( 1 \leq j \leq p  \). Let \( a \in F  \). Then using the definition of the product once again, we have
        \begin{align*}
            a (AB)_{ij} &= a \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj} \\
                   &= \sum_{ k=1 }^{ n } a ({A}_{ik } {B}_{kj }) \\
                   &= \sum_{ k=1 }^{ n } ({aA}_{ik}) {B}_{kj } \\
                   &= \sum_{ k=1 }^{ n } {(aA)}_{ik } {B}_{kj} \\
                   &= ((aA)B)_{ij}.
        \end{align*}
    Then observe that 
    \begin{align*}  
        ((aA)B)_{ij} &= \sum_{ k = 1  }^{ n  } (a {A}_{ik } ) {B}_{kj} \\ 
    &= \sum_{ k=1 }^{ n } ({A}_{ik } a ) {B}_{kj}  \\
    &= \sum_{ k=1 }^{ n } {A}_{ik } (a {B}_{kj})  \\
    &= \sum_{ k=1 }^{ n } {A}_{ik } {(aB)}_{kj} \\
    &= (A (aB))_{ij}.
    \end{align*}
    Hence, \( (aA)B = A(aB) \). Thus, we conclude that
    \[ a(AB) = (aA)B = A(aB).  \]
\item[(c)] Let \(   1 \leq i \leq m  \) and \(  1 \leq j \leq n  \). Since \( {\delta}_{ik} = 1   \) only when \( i = k  \) and \( 0  \) otherwise, we must have
    \begin{align*}
        ({I}_{m} A)_{ij} &= \sum_{ k=1  }^{ m  } {\delta}_{ik } {A}_{kj}  = {A}_{ij }. \\
    \end{align*}
    Likewise, 
    \[  {(A {I}_{n} )}_{ij } = \sum_{ k= 1  }^{ n  } {A}_{ik } {\delta}_{k j } = {A}_{ij }  \] by the same reasoning. Hence, we have
    \[  {I}_{m} A = A = A {I}_{n}. \]
    \item[(d)] Let \( V  \) be an \( n- \)dimensional vector space with \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) as an ordered basis. Since \( {I}_{V}({v}_{j}) = {v}_{j }  \) for all \( 1 \leq j \leq n  \), we must have
        \[  {v}_{j} = {I}_{V}({v}_{j}) = \sum_{ i=1 }^{ n } {a}_{ij} {v}_{i}   \]
        which holds only if \(  {a}_{ij} = 1  \) for all \( i = j  \) and \( 0  \) otherwise. But this means that \( {a}_{ij} = {\delta}_{ij}  \), so \( [{I}_{V}]_{\beta}^{}  = {I}_{n} \).
        
\end{enumerate}
\end{proof}

\begin{corollary}
    Let \( A  \) be an \( m \times n  \) matrix, \( {B}_{1}, {B}_{2}, \dots, {B}_{k } \) be \( n \times  p  \) matrices, \( {C}_{1}, {C}_{2}, \dots ,{C}_{k }  \) be \( q \times  m  \) matrices, and \( {a}_{1}, {a}_{2}, \dots, {a}_{k }  \) be scalars. Then 
    \[  A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i}  \Big) = \sum_{ i=1  }^{  k  } {a}_{i} {AB}_{i} \]
    and
    \[ \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i}  \Big) A = \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i} A. \]
\end{corollary}

\begin{proof}
Let \( A  \) be an \( m \times n  \) matrix and \( {B}_{1}, {B}_{2}, \dots, {B}_{k } \) be \( n \times p  \) matrices. Let \( 1 \leq \ell \leq m  \) and \( 1 \leq s  \leq p  \). Then
\begin{align*}
    \Big[ A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i} \Big) \Big]_{\ell s } &= \sum_{ \lambda = 1  }^{ n } {A}_{ \ell \lambda }  \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i} \Big)_{\lambda s }  \\
                                                                        &= \sum_{ \lambda = 1  }^{ n  } {A}_{ \ell \lambda } \Big(  \sum_{ i=1  }^{  k  } {({a}_{i} {B}_{i})}_{\lambda s } \Big)  \\
                                                                        &=  \sum_{ \lambda = 1  }^{ n  } {A}_{ \ell \lambda } \Big( \sum_{ i=1 }^{ k  } {a}_{i} ({B}_{i})_{\lambda s } \Big) \\
                                                                        &= \sum_{ i=1  }^{ k  } {a}_{i} \Big( \sum_{ \lambda =1  }^{ n  } {A}_{ \ell \lambda } ({B}_{i})_{\lambda s } \Big) \tag{part (a) of Theorem 2.12}  \\
                                                                        &= \sum_{ i=1  }^{ k  } {a}_{i}  (A {B}_{i})_{\ell s } .
\end{align*}
Hence, we have
\[  A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i}  \Big) = \sum_{ i=1  }^{  k  } {a}_{i} {AB}_{i}.\]

To show the second formula, let \( 1 \leq \ell \leq q  \) and \( 1 \leq  s  \leq n   \). Then
\begin{align*}
    \Big[ \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i}  \Big) A  \Big]_{\ell s }  &= \sum_{ \lambda = 1  }^{ n   } \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i} \Big)_{\ell \lambda } {A}_{\lambda s }  \\
                                                                                     &= \sum_{ \lambda = 1  }^{ n } \Big( \sum_{ i=1 }^{  k   } ({a}_{i} {C}_{i})_{\ell \lambda }  \Big) {A}_{\lambda s } \\
                                                                                     &= \sum_{ \lambda = 1  }^{ n } \Big( \sum_{ i=1 }^{  k   } {a}_{i} ({C}_{i})_{\ell \lambda }  \Big) {A}_{\lambda s } \\
                                                                                     &= \sum_{ i =1  }^{ k  } {a}_{i} \Big( \sum_{ i=1  }^{ k  } {({C}_{i})}_{\ell \lambda } {A}_{\lambda s }  \Big) \tag{part (a) of Theorem 2.12}   \\
                                                                                     &= \sum_{ i=1  }^{ k  } {a}_{i} ({C}_{i} A  )_{\ell s }.
\end{align*}
Hence, we have
\[  \Big( \sum_{ i=1  }^{  k  } {a}_{i} {C}_{i}  \Big) A = \sum_{ i=1  }^{ k  } {a}_{i} {C}_{i} A. \]
\end{proof}



\begin{theorem}\label{Theorem 2.13} 
   Let \( A  \) be an \( m \times n  \) matrix and \( B  \) be an \( n \times p  \) matrix. For each \( j  \) (\( 1 \leq j \leq p  \)) let \( {u}_{j}  \) and \(  {v}_{j}  \) denote the \( j \)th columns of \( AB  \) and \( B  \) respectively. Then
    \begin{enumerate}
        \item[(a)]  \( {u}_{j} = A {v}_{j}  \)
        \item[(b)] \( {v}_{j} = B {e}_{j }  \), where \( {e}_{j}  \) is the \( j \)th standard vector of \( F^{p} \).
    \end{enumerate}
\end{theorem}

\begin{proof}
Let \( 1 \leq j \leq p  \). Then we have
\begin{align*}
    {u}_{j} &= \begin{pmatrix}
        {(AB)}_{1j } \\
        {(AB)}_{2j } \\
        \vdots \\
        {(AB)}_{mj} 
    \end{pmatrix} = \begin{pmatrix}
        \sum_{ k=1 }^{ n } {A}_{1k} {B}_{kj} \\
        \sum_{ k=1  }^{  n } {A}_{2k } {B}_{kj } \\
        \vdots \\
        \sum_{ k=1 }^{ n } {A}_{mk } {B}_{kj }
    \end{pmatrix} =  A \begin{pmatrix}
        {B}_{1j } \\
        {B}_{2j } \\
        \vdots \\
        {B}_{nj}
    \end{pmatrix} =  A {v}_{j }.
\end{align*}
To show the other equation, we apply part (c) of Theorem 2.12, to write
\begin{align*}
   {v}_{j}  &= \begin{pmatrix}
       {B}_{1j } \\
       {B}_{2j } \\
       \vdots \\
       {B}_{nj } 
   \end{pmatrix} = 
   \begin{pmatrix}
        (B {I}_{p})_{1j} \\  
        {(B{I}_{p})}_{2j} \\
        \vdots \\
        {(B {I}_{p})}_{nj}
   \end{pmatrix} = \begin{pmatrix}
       \sum_{ k=1  }^{ n } {B}_{1k} {\delta}_{kj} \\
       \sum_{ k=1  }^{ n } {B}_{2k} {\delta}_{kj } \\
       \vdots \\
       \sum_{ k=1  }^{ n } {B}_{nk} {\delta}_{kj } 
       \end{pmatrix} = B \begin{pmatrix}
       {\delta}_{1j } \\
       {\delta}_{2j } \\
       \vdots \\
       {\delta}_{pj } 
   \end{pmatrix} = B {e}_{j}
\end{align*}
where \( {\delta}_{1j} \) are the Kronecker delta constants.
\end{proof}

\begin{itemize}
    \item This result gives us the ability to write columns of \( AB  \) (that is, the \( j \)th column of \( AB \)) as a linear combination of the columns of \( A  \) with the coefficients being entries of column \( j  \) of \( B  \).
    \item In the case for rows, the \( i \)th row of \( AB  \) can be written as a linear combination of the rows of \( B  \) with coefficients being entries from \( i \)th row in \( A  \).
\end{itemize}

\begin{theorem}
    Let \( V  \) and \( W  \) be finite-dimensional vector spaces having ordered bases \( \beta  \) and \( \gamma  \), respectively, and let \( T: V \to W  \) be linear. Then, for each \( u \in V  \), we have
    \[  [T(u)]_{\gamma}^{}  = [T]_{\beta}^{\gamma}  [u]_{\beta}^{}. \]
\end{theorem}
\begin{proof}
    Let \( \beta \) and \( \gamma \) be ordered bases for \( V  \) and \( W  \) respectively. Fix \( u \in V  \), and define the linear transformations 
    \begin{center}
        \( f: F \to V  \) by \( f(a) = au  \) and \( g: F \to W  \) by \( g(a) = aT(u) \) for all \( a \in F  \). 
    \end{center}
    Let \( \alpha = \{  1  \}  \) be the standard ordered basis for \( F  \). Note that 
    \[  T(u) = 1 \cdot T(u) = g(1) = g = Tf  \]
    since \( f = f(1)  = 1 \cdot u = u\). Using Theorem 2.11, we can write
    \begin{align*}
        [T(u)]_{\gamma}  = [g(1)]_{\gamma} &= [g]_{\alpha}^{\gamma}  \\
                                                 &= [Tf]_{\alpha}^{\gamma} \\
                                                 &= [T]_{\beta}^{\gamma} [f]_{\alpha}^{\beta} \\
                                                 &= [T]_{\beta}^{\gamma}  [f(1)]_{\beta}  \\
                                                 &= [T]_{\beta}^{\gamma}  [u]_{\beta}.
    \end{align*}
    Hence, we have
    \[  [T(u)]_{\gamma} = [T]_{\beta}^{\gamma}  [u]_{\beta}. \]
\end{proof}

\begin{eg}
    Let \( T: {P}_{3}(\R) \to {P}_{2}(\R) \) be the linear transformation defined by 
    \[  T(f(x)) = f'(x), \]
    and let \( \beta \) and \( \gamma \) be the standard ordered bases for \( {P}_{3}(\R) \) and \( {P}_{2}(\R) \), respectively. Let \( A = [T]_{\beta}^{\gamma}  \), then we have
    \[ A  = \begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 3
    \end{pmatrix} \]
    from {\hyperref[Example 4 of Section 2.2]{Example 4 of Section 2.2}}. Utilizing {\hyperref[Theorem 2.3.6]{Theorem 2.3.6}}, we can verify that  
    \[  [T(p(x))]_{\gamma} = [T]_{\beta}^{\gamma}  [p(x)]_{\beta} \]
    where \( p(x) \in {P}_{3}(\R) \) is the polynomial defined by
     \[  p(x) = 2 - 4x + x^{2} + 3x^{3}. \]
     Hence, we have
     \[ [T(p(x))]_{\gamma} = [q(x)]_{\gamma} = \begin{pmatrix}
         -4 \\
         2 \\
         9
     \end{pmatrix},  \]
     and
     \[ [T]_{\beta}^{\gamma}  [p(x)]_{\beta} = A [p(x)]_{\beta} = \begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 3
    \end{pmatrix} \begin{pmatrix}
        2 \\
        -4 \\
        1 \\
        3
    \end{pmatrix} = \begin{pmatrix}
        -4 \\
        2 \\
        9 
    \end{pmatrix}.   \]
\end{eg}

\begin{definition}[Left Multiplication Transformation]\label{LMT}
   Let \( A  \) be an \( m \times n  \) matrix with entries from a field \( F  \). We denote by \( {L}_{A} \) the mapping \( {L}_{A} : F^{n} \to F^{m } \) defined by \( {L}_{A} (x) = Ax \) (the matrix product of \( A  \) and \( x \)) for each column vector \( x \in F^{n} \). We call \( {L}_{A} \) a \textbf{left-multiplication transformation}.
\end{definition} 

\begin{eg}
    Let 
    \[  A = \begin{pmatrix}
        1 & 2 & 1 \\
        0 & 1 & 2 
    \end{pmatrix}. \]
    Then \( A \in {M}_{2 \times 3 }(\R) \) and \( {L}_{A}: \R^{3} \to \R^{2} \). If 
    \[  x = \begin{pmatrix}
        1  \\
        3  \\
        -1 
    \end{pmatrix}, \]
    then
    \[  {L}_{A}(x) = Ax = \begin{pmatrix}
        1 & 2 & 1 \\
        0 & 1 & 2 
    \end{pmatrix} \begin{pmatrix}
        1 \\
        3 \\
        -1 
    \end{pmatrix} = \begin{pmatrix}
        6 \\
        1 
    \end{pmatrix}.  \]
\end{eg}

\begin{remark}
   It is relatively straight forward to show that \( {L}_{A} \) is linear. 
\end{remark}

\begin{theorem}[Properties of Left-Multiplication Transformations]\label{Prop of LMT}
   Let \( A  \) be an \( m \times n  \) matrix with entries from \( F  \). Then the left-multiplication transformation \( {L}_{A}: F^{n} \to F^{m}  \) is linear. Furthermore, if \( B  \) is any other \( m \times n  \) matrix (with entries from \( F  \)) and \( \beta \) and \( \gamma \) are the standard ordered bases for \( F^{n}  \) and \( F^{m}  \), respectively, then we have the following properties.
   \begin{enumerate}
       \item[(a)] \( [{L}_{A}]_{\beta}^{\gamma}  = A  \). 
        \item[(b)] \( {L}_{A} = {L}_{B} \) if and only if \( A = B  \).
        \item[(c)] \( {L}_{A + B } = {L}_{A } + {L}_{B} \) and \( {L}_{aA} = a {L}_{A} \) for all \( a \in F  \).
        \item[(d)] If \( T: F^{n} \to F^{m} \) is linear, then there exists a unique \( m \times n  \) matrix \( C  \) such that \( T = {L}_{C}  \). In fact, \( C = [T]_{\beta}^{\gamma}  \).
        \item[(e)] If \( E  \) is an \( n \times p  \) matrix, then \( {L}_{AE} = {L}_{A} {L}_{E} \).
        \item[(f)] If \( m = n  \), then \( {L}_{{I}_{n}} = {I}_{F^{n}}  \)Transformation.
   \end{enumerate}
\end{theorem}
\begin{proof}
First, we show that \( {L}_{A} \) is linear. Let \( A  \) be an \( m \times n  \) matrix and \( L_{A} : F^{n} \to F^{m} \). Given an arbitrary \( c \in F  \) and \( cx + y \in  F^{n}  \), we can write
   \begin{align*}
       {L}_{A}(cx+y) &= A(cx+y)    \\
                     &= A(cx) + A(y) \tag{{\hyperref[prop of matrices]{Part (a) of Property of Matrices}} } \\
                     &= c(A(x)) + A(y) \tag{{\hyperref[prop of matrices]{Part (b) of Property of Matrices}} } \\
                     &=  c {L}_{A}(x) + {L}_{A}(y). 
   \end{align*} 
   Hence, \( {L}_{A} \) is a linear transformation. Now, let's show properties (a)-(f) 
   \begin{enumerate}
       \item[(a)] Let \( 1 \leq j \leq n  \) and \( A  \) be an \( m \times n  \) matrix. Observe that 
           \[  ([{L}_{A}]_{\beta}^{\gamma} )_{j} = {L}_{A}({e}_{j}) = A {e}_{j} = {u}_{j}  \]
           where \( {u}_{j}  \) is the \( j \)th column of \( A  \). Hence, we have 
           \[ [{L}_{A}]_{\beta}^{\gamma} = A.    \]
        \item[(b)] Suppose \( {L}_{A} = {L}_{B} \). By part (a), we get that
            \[  A = [{L}_{A}]_{\beta}^{\gamma} = [{L}_{B}]_{\beta}^{\gamma}  = B. \]
            Conversely, suppose \( A = B  \). Let \( x \in F^{n} \). Then performing left-multiplication, we have
            \[  {L}_{A}(x) =  Ax = Bx = {L}_{B}(x).  \]
            Hence, we have \( {L}_{A} = {L}_{B} \).
        \item[(c)] Let \( x \in F^{n} \). Using the {\hyperref[prop of matrices]{Part (a) of Property of Matrices}}, we must have
            \[  {L}_{A+B}(x) = (A+B)(x) = A(x) + B(x) = {L}_{A}(x) + {L}_{B}(x). \]
            Hence, \( {L}_{A+B} = {L}_{A} + {L}_{B} \). Now, let \( a \in F  \). Using the same reasoning, we have  
            \[  {L}_{aA}(x) = (aA)(x) = a (A(x)) = a {L}_{A}(x).  \]
            Hence, \( {L}_{aA} = a{L}_{A}. \)
        \item[(d)] Let \( c = [T]_{\beta}^{\gamma}  \). Since \( T: F^{n} \to F^{m} \), we see that 
            \[  [T(x)]_{\gamma} = [T]_{\beta}^{\gamma} [x]_{\beta}      \]
            or 
            \[  T(x) = C(x) = {L}_{C}(x) \]
            where \( x \in F^{n} \). Hence, \( T = {L}_{C} \).
        \item[(e)] Let \( 1 \leq j \leq  p  \). Applying {\hyperref[Theorem 2.13]{Theorem 2.13}} several times so we may note that \( (AE){e}_{j}  \) is the \( j \)th column of \( AE  \) and that the \( j \)th column of \( AE \) is also equal to \( A(E {e}_{j}) \); that is, 
            \[  (AE){e}_{j} = A(E {e}_{j}). \]
            Hence, we have
            \begin{align*}  {L}_{AE}({e}_{j}) = (AE) {e}_{j} &= A(E({e}_{j})) \\ 
            &= {L}_{A}(E({e}_{j})) \\ 
            &= {L}_{A} ({L}_{E}({e}_{j})) \\ 
            &= ({L}_{A}{L}_{E})({e}_{j}).
            \end{align*}
            Hence, we have \( {L}_{AE} = {L}_{A} {L}_{E} \).
        \item[(f)] Let \( 1 \leq j \leq n  \). Then
                \begin{align*}  {L}_{{I}_{n}}({e}_{j}) = {I}_{n}({e}_{j}) 
                &= {e}_{j} \\ 
                &= {I}_{F^{n}}({e}_{j}) \tag{{\hyperref[prop of matrices]{Part (d) of Theorem 2.3.4}}}. 
                \end{align*}
            Hence, \( {L}_{{I}_{n}} = {I}_{F^{n}} \).
   \end{enumerate}
\end{proof}

\begin{theorem}[Associativity of Left-Multiplication Transformations]
Let \( A, B  \) and \( C  \) be matrices such that \( A(BC) \) is defined. Then \( (AB)C  \) is also defined and \( A(BC) = (AB)C  \); that is, matrix multiplication is associative.    
\end{theorem}
\begin{proof}
Let \( A  \) be an \( m \times n  \) matrix, \( B  \) an \(  n \times p  \) matrix, and C \( p \times \ell  \) matrix so \( (AB)C \) be defined with dimension \( m \times \ell  \). So, let \( {L}_{(AB)C}: F^{\ell} \to F^{m}   \) be the left multiplication transformation. Our goal is to show that
\[ {L}_{(AB)C} = {L}_{A(BC)}. \]
Using the properties of compositions and left-multiplication transformations, we write 
\begin{align*} 
    {L}_{(AB)C}                  &= ({L}_{AB}) {L}_{C} \tag{{\hyperref[LMT]{Def of Left-Multiplication}} }   \\
                                 &= ( {L}_{A} {L}_{B} )  {L}_{C} \tag{{\hyperref[Prop of LMT]{Prop of Left-Multiplication}}} \\ 
                                 &= {L}_{A} ({L}_{B} {L}_{C}) \tag{{\hyperref[Properties of Compositions]{Prop of Compositions}} } \\ 
                                 &= {L}_{A} ({L}_{BC}) \tag{{\hyperref[Prop of LMT]{Prop of Left-Multiplication}}} \\ 
                                 &=  {L}_{A(BC)} \tag{{\hyperref[LMT]{Def of Left-Multiplication}}}.       
\end{align*}
Hence, we conclude that \( {L}_{(AB)C} = {L}_{A(BC)} \).
\end{proof}

