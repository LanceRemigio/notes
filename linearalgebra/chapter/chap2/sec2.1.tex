\section{Linear Transformations, Null Spaces, and Ranges}

Suppose we have a function \( T  \) with domain \( V  \) and codomain \( W  \) denoted by \( T: V \to W  \).


\begin{definition}[Linear Transformation]\label{Linear Transformation}
   Let \( V  \) and \( W  \) be vector spaces (over \( F  \)). We call a function \( T: V \to W  \) a \textbf{linear transformation from \( V  \) to \( W  \)}, for all \( x,y \in V  \) and \( c \in F  \), we have
   \begin{enumerate}
       \item[(a)] \( T(x+y) = T(x) + T(y) \) and
        \item[(b)] \( T(cx) = c T(x) \).
   \end{enumerate}
\end{definition}

\begin{remark}
    If \( F = \Q   \), then (a) implies (b) in the definition above. Otherwise, (a) and (b) are logically independent statements.  
\end{remark}

The following are a list of properties for linear functions:

\begin{prop}
    Let \( V  \) and \( W  \) be vector spaces (over \( F \)). If \( T: V \to W   \) is a \textbf{linear transformation from \( V \) to \( W  \)}, then the following properties hold:
    \begin{enumerate}
        \item[(a)] If \( T  \) is linear, then \( T(0) = 0  \). 
        \item[(b)] \( T  \) is linear if and only if \( T(x+y) = T(x) + T(y) \) for all \( x,y \in V  \) and \( c \in F  \).
        \item[(c)] If \( T  \) is linear, then \( T(x-y) = T(x) - T(y)  \) for all \( x,y \in V  \).
        \item[(d)] \( T  \) is linear if and only if, for \( x_{1}, x_{2}, \dots, x_{n} \in V  \) and \( a_{1}, a_{2}, \dots, a_{n} \in F  \), we have
            \[  T \Big( \sum_{ i=1 }^{ n }a_{i}x_{i}  \Big) = \sum_{ i=1 }^{ n } a_{i} T(x_{i}). \]
    \end{enumerate}
\end{prop}

\begin{proof}
    Let \( V  \) and \( W  \) be vector spaces such that \( T: V \to W  \).
\begin{enumerate}
    \item[(a)] If \( T  \) is linear, then \( T(0) = 0  \).
        Suppose \( T  \) is linear, then \( T(0 \cdot 0 ) = 0 T(0) = 0  \).
    \item[(b)] \label{Second Property of Linearity} We have \( T  \) is linear if and only if \( T(cx+y) = cT(x) + T(y)  \) for all \( x,y \in V  \) and \( c \in F  \).
        Suppose \( T  \) is linear. Let \( x,y \in V  \) and \( c \in F  \). Then 
        \[  T(cx+y) = T(cx) + T(y) = cT(x) + T(y). \]
        Conversely, if \( c = 1  \) then 
        \[  T(x + y) = T(x) + T(y). \]
        If  \( y = 0  \), then 
        \[ T(cx) = cT(x).  \]
        Hence, \( T  \) is a linear transformation.
    \item[(c)] If \( T  \) is linear, then \( T(x-y) = T(x) - T(y)  \) for all \( x,y \in V  \).
        Let \( x,y \in V  \). Suppose \( T  \) is linear, then 
        \[ T(x - y) = T(x) + T(-y) = T(x) - T(y).  \]
    \item[(d)] \( T  \) is linear if and only if, for \( x_{1}, x_{2} , \dots, x_{n} \in V  \) and \( a_{1}, a_{2}, \dots, a_{n} \in F  \), 
        we have
        \[  T \Big(  \sum_{ i=1 }^{ n } a_{i} x_{i}  \Big) = \sum_{ i=1 }^{ n } a_{i} T(x_{i}). \]
        Suppose \( T  \) is linear. Let \( x_{1}, x_{2}, \dots, x_{n} \in V  \) and \( a_{1}, a_{2}, \dots, a_{n} \in F  \) such that
        \[  v = \sum_{ i=1 }^{ n } a_{i} x_{i}. \]
        Then observe that 
        \[
            T \Big( \sum_{ i=1 }^{ n } a_{i} x_{i} \Big) = \sum_{ i=1 }^{ n } T( a_{i} x_{i}) 
                                                         = \sum_{ i=1 }^{ n } a_{i} T(x_{i}).
        \]
        Conversely, for \( i = 2  \) and denote \( a_{i} x_{i} = y_{i} \). Then
        \[  T\Big(\sum_{ i=1 }^{ 2 } a_{i} x_{i} \Big) = T(a_{1} x_{1}) + T(a_{2} x_{2}) = T(y_{1}) + T(y_{2})  \]  
        and so property 1 is satisfied. 
        If \( i = 1  \), then 
        \[  T \Big( \sum_{ i=1 }^{ 1 } a_{i}x_{i}  \Big) = a_{1} T(x_{1}) \]
        and so property 2 is satisfied. Hence, \( T  \) is a linear transformation.
\end{enumerate}
\end{proof}
\begin{eg}
   Define  
   \begin{center}
       \( T: \R^{2} \to \R^{2}  \) by \( T(a_{1}, a_{2}) = (2a_{1} + a_{2}, a_{1}) \).
   \end{center}
   Show that \( T  \) is linear. Let \( c \in \R  \) and \( x,y \in \R^{2} \), where \( x = (a_{1}, a_{2}) \) and \( y = (b_{1}, b_{2}) \). Then
   \begin{align*}
       cx + y &= c(a_{1}, a_{2}) + (b_{1}, b_{2})  \\
              &=  (ca_{1}, ca_{2} ) + (b_{1}, b_{2}) \\
              &=  (ca_{1} + b_{1} , ca_{2} + b_{2}).
   \end{align*}
   So, we have
   \begin{align*}
                                       T(cx + y)  &= T(ca_{1} + b_{1} , ca_{2} + b_{2})  \\
                                         &= (2(ca_{1} + b_{1})  + ca_{2} + b_{2}, ca_{1} + b_{1} ) \\
                                          &= ((2ca_{1} + ca_{2}) +  (2b_{1} + b_{2}), ca_{1} + b_{1}  ) \\
                                          &= (2ca_{1} + ca_{2}, ca_{1}) + (2b_{1} + b_{2}, b_{1} ) \\
                                          &= c(2a_{1} + a_{2}, a_{1}) + (2b_{1} + b_{2}, b_{1}) \\
                                          &= cT(a_{1}, a_{2}) + T(b_{1}, b_{2}) \\
                                          &= cT(x) + T(y).
   \end{align*}
   Hence, we have that \( T(cx + y) = cT(x) + T(y) \) so \( T: \R^{2} \to \R^{2}   \) is a linear.
\end{eg}

\subsection{Examples of Linear Transformations}

The most common linear transformations come from geometry:
\begin{itemize}
    \item Rotations: \( T_{\theta}(a_{1}, a_{2}) = (a_{1}, a_{2}) \) by performing a counter-clockwise rotation by an angle \( \theta \) if \( (a_{1}, a_{2}) \neq (0,0) \). This is called \textbf{rotation by \( \theta \)} and \( T_{\theta}(0,0) = (0,0) \) otherwise.
    \item Reflections about the \( x- \)axis: \( T(a_{1}, a_{2}) = (a_{1}, - a_{2}) \).
    \item Projections on the \( x- \)axis: \( T(a_{1}, a_{2}) = (a_{1}, 0 ) \).
\end{itemize}

\begin{eg}[Rotations]
    Define the rotation transformation above by \( T_{\theta}: \R^{2} \to \R^{2}  \) by \( T_{\theta}(a_{1}, a_{2}) \) where it is described as above. Our goal in this example is to define an explicit formula of this transformation. Let us fix a nonzero vector \( (a_{1}, a_{2}) \in \R^{2} \). Let \( \alpha  \) be the angle such that \( (a_{1}, a_{2})\) makes with the positive \( x-\)axis, and let \( r = \sqrt{ a_{1}^{2} + a_{2}^{2}  }  \). Then using some trigonometry, we get that \( a_{1} = r \cos(\alpha)  \) and \( a_{2} = r \sin(\alpha) \). Note that \( T_{\theta}(a_{1}, a_{2}) \) has length \( r  \) and makes an angle \( \alpha + \theta  \) with the positive \( x- \)axis. Using some trigonometric identities, we get that
    \begin{align*}
        T_{\theta}(a_{1}, a_{2}) &= (r\cos(\alpha + \theta), r \sin(\alpha + \theta)) \\
                                 &= (r \cos(\alpha) \cos(\theta) - r \sin(\alpha ) \sin(\theta) , r \cos(\alpha) \sin(\theta) + r \sin(\alpha) \cos(\theta) ) \\
                                 &= (a_{1} \cos(\theta) - a_{2} \sin(\theta) , a_{1} \sin(\theta) + a_{2} \cos(\theta)).  \\
    \end{align*}
    Observe that this formula holds for \( (a_{1}, a_{2}) = (0,0) \). One can show that \( T_{\theta}  \) is linear.
\end{eg}

\begin{eg}[Transpose]
    Define \( T: M_{m \times n } (F) \to M_{n \times m}(F) \) by \( T(A) = A^{t}  \), where \( A^{t} \) is the {\hyperref[Transpose]{transpose}}  of \( A  \), defined in Section 1.3. 
\end{eg}

\begin{eg}[Derivatives of Polynomials]
    Define \( T: P_{n}(\R) \to P_{n-1}(\R) \) by \( T(f(x)) = f'(x)  \), where \( f'(x)  \) denotes the derivative of \( f(x)  \). We can show that \( T  \) is linear. Let \( g(x) , h(x) \in P_{n}(\R) \) and \( a \in \R  \).Assuming that the differentiation rules holds for any \( T  \), we have
    \begin{align*}
        T(ag(x) + h(x)) &= (ag(x) + h(x))' \\
                        &= ag'(x) + h'(x) \\
                        &= a T(g(x)) + T(h(x)).
    \end{align*}
    By the {\hyperref[Second Property of Linearity]{second property of linearity}} , we have that \( T  \) is linear.
\end{eg}

\begin{eg}[Integrals]\label{Integrals are Linear}
    Let \( V = C(\R) \) be the vector space of continuous real-valued functions on \( \R \). Let \( a,b \in \R  \), \( a < b  \). Define \( T: V \to \R  \) by
    \[  T(f) = \int_{ a }^{ b }  f(t) \ dt  \] for all \( f \in V  \). We know that \( T  \) is a linear transformation because the definite integral of a linear combination of functions is the same as the linear combination of the definite integrals of the functions; that is, 
    \[  \int_{ a }^{ b } \sum_{ i=1 }^{ n } \gamma_{i} f(t_{i}) \ dt = \sum_{ i=1 }^{ n } \gamma_{i} \int_{ a }^{ b } f(t_{i})   \ dt = \sum_{ i=1 }^{ n } \gamma_{i} T(f_i)  \]
    for scalars \( \gamma_{i}  \) where \( 1 \leq i \leq n\) and  
    \[  T(f_i) =  \int_{ a }^{ b } f(t_{i}) \ dt.  \]
\end{eg}


\subsection{Identity and Zero Transformations}

\begin{remark}
   Two very important examples of linear transformations are the \textbf{identity and zero transformations}. 
\end{remark}

\begin{definition}[Identity Transformation]\label{Identity Transformation}
    The \textbf{identity transformation} is denoted by \( I_{V}: V \to V  \) by \( I_{V}(x) = x  \) for all \( x \in V  \). From now on, we will denote this transformation as \( I  \) instead of \( I_{V} \).
\end{definition}

\begin{definition}[Zero Transformation]\label{Zero Transformation}
    The \textbf{zero transformation} is denoted by \( T_{0}: V \to W  \) by \( T_{0}(x) = 0  \) for all \( x \in V  \).
\end{definition}

\subsection{Range and Null Spaces}

The \textit{Range} and \textit{Null} spaces give us important insights on the intrinsic properties of a linear transformation.



\begin{definition}[Null Space]\label{Null Spaces}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. 
   The \textbf{null space} (or \textbf{kernel}), denoted by \( N(T) \) of \( T  \), is the set of all vectors \( x \in V  \) such that \( T(x) = 0  \); that is, \( N(T) = \{ x \in V : T(x) = 0  \}  \). 
\end{definition}

\begin{definition}[Range]\label{Range}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. 
   The \textbf{range} (or \textbf{image}) \( R(T)  \) of \( T  \) is the subset of \( W  \) consisting of all images (under \( T \)) of vectors in \( V  \); that is, \( R(T) = \{ T(x) : x \in V  \}  \).
\end{definition}

\begin{eg}
    Let \( V  \) and \( W  \) be vector spaces, and let \( I: V \to V  \) and \( T_{0}: V \to W  \) be the identity and zero transformations, respectively. Then \( N(I) = \{ 0  \}  \), \( R(I) = V  \), \( N(T_{0}) = V  \), and \( R(T_{0}) = \{ 0 \}  \).
\end{eg}


\begin{eg}
    Let \( T: \R^{3} \to \R^{2}  \) be the linear transformation defined by  
    \[  T(a_{1}, a_{2}, a_{3}) = (a_{1} - a_{2}, 2a_{3} ). \]
    One can verify that 
    \begin{center}
        \( N(T) = \{ (a,a,0) : a \in \R  \}  \) and \( R(T) = \R^{2} \).
    \end{center}
\end{eg}


\begin{theorem}[The Null and Range are Both Subspaces]
    Let \( V  \) and \( W  \) be vector spaces and \( T: V \to W  \) be linear. Then \( N(T) \) and \( R(T) \) are subspaces of \( V  \) and \( W  \), respectively.
\end{theorem}
\begin{proof}
    Let \( T: V \to W   \) be linear. First, we show that \( N(T) \) is a subspace of \( V  \). 
    \begin{enumerate}
        \item[(a)] Note that \( 0_{V} \in V  \) implies that \( T(0_{V}) = 0_{W}  \) since \( T  \) is linear (property (1) of linearity). Hence, \( 0_{V} \in N(T)  \).
        \item[(b)] Let \( x, y \in N(T) \). By definition, of the null space we have \( T(x) = 0_{W}  \) and \( T(y) = 0_{W} \). Since \( T  \) is linear, we get that
            \[  T(x+y) = T(x) + T(y) = 0_{W} + 0_{W} = 0_{W} \implies x + y \in N(T). \]
        Hence, \( N(T) \) is closed under addition.
        \item[(c)] Let \( x \in N(T)  \) and let \( c \in F  \). Then 
            \[ T(cx) = cT(x) = c 0_{V} = 0_{W} \implies cx \in N(T).  \]
    \end{enumerate}
    Hence, {\hyperref[Subspaces]{Theorem 1.3}} tells us that \( N(T) \) is a subspace of \( V  \).
    Now, we will show that \( R(T) \) is a subspace of \( W  \).
    \begin{enumerate}
        \item[(a)] Note that \( 0_{V} \in V  \) implies that \( T(0_{V}) = 0_{W} \in R(T) \) by property (1) of linearity. So, \( 0_{W} \in R(T) \).
        \item[(b)] Now let \( x,y \in R(T)  \). Then there exists elements \( z, w \in W   \) such that \( T(x) = z  \) and \( T(y) = w  \). Then by linearity, we have \( T(x+y) = T(x) + T(y) = z + y \in W   \). Hence, we must have \( x + y \in R(T) \) and thus \( R(T)  \) is closed under addition.
        \item[(c)] Let \( x \in R(T)  \) and \( c \in F \). Then we have \( T(cx) = c T(x) \in W  \) since \( T(x) \in W  \). Hence, \( cx \in R(T) \).
    \end{enumerate}
    Thus, {\hyperref[Subspaces]{Theorem 1.3}} also tells us that \( R(T)  \) is a subspace of \( W  \).
\end{proof}

The next theorem allows us to find a spanning set for the range of a linear transformation.

\begin{theorem}[Spanning Set for a Linear Transformation]\label{Spanning set for R(T)}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T:V \to W  \) be linear. If \( \beta = \{ v_{1}, v_{2}, \dots v_{n} \}  \) is a basis for \( V  \), then   
    \[  R(T) = \text{span}(T(\beta)) = \text{span}(\{ T(v_{1}), T(v_{2}) , \dots, T(v_{n}) \} ). \]
\end{theorem}

\begin{proof}
Let \( T: V \to W  \) be linear where \( V  \) and \( W  \) are vector spaces. To show that \( T(\beta) \) generates \( R(T) \); we need to show that  
\begin{center}
    \( \text{span}(T(\beta)) \subseteq R(T) \) and \( R(T) \subseteq \text{span}(T(\beta)) \).
\end{center}
    Observe that \( T(v_{i}) \in R(T) \) for all \( i  \). Hence, \( T(\beta) \subseteq R(T) \). By theorem 1.5, we know that \( R(T) \) also contains the span of \( T(\beta) \). Hence, \( \text{span}(T(\beta)) \subseteq R(T) \). 

    Let \( w \in R(T) \). Then for some \( v \in W  \), we have \( T(v) = w \). Since \( \beta \) is a basis for \( V  \), we choose scalars \( \alpha_{1}, \alpha_{2}, \dots, \alpha_{n} \) such that 
    \[  v  = \sum_{ i=1 }^{ n } \alpha_{i} v_{i}. \]
    Since \( T  \) is linear, we have that
    \[  w = T(v) = T \Big( \sum_{ i=1 }^{ n } a_{i} v_{i} \Big) = \sum_{ i=1 }^{ n } a_{i} T(v_{i}). \]
    This tells us that \( v \in \text{span}(T(\beta)) \). Hence, \( T(\beta) \) generates \( R(T) \).
\end{proof}

\begin{eg}
    Define the linear transformation \( T: P_{2}(\R) \to M_{2 \times 2}(\R) \) by 
    \[  T(f(x)) = \begin{pmatrix}
        f(1) - f(2) & 0 \\
        0 & f(0)
    \end{pmatrix}. \]
    Note that 
    \begin{center}
        \( \beta = \{ 1,x, x^{2} \}  \) is a basis for \( P_{2}(\R)  \) and \( T(\beta) = \{ T(1), T(x), T(x^{2}) \}. \)
    \end{center}
    Then observe that
    \begin{align*}
        R(T) &= \text{span}(T(\beta)) \\
             &= \text{span}\Big( \Big\{ \begin{pmatrix}
                         0 & 0 \\
                         0 & 1 
             \end{pmatrix}, \begin{pmatrix}
                         -1 & 0 \\
                         0 & 0 
             \end{pmatrix}, \begin{pmatrix}
                         -3 & 0 \\ 
                         0 & 0 
             \end{pmatrix} \Big\}  \Big).
    \end{align*}
    This tells us that \( R(T) \) contains the basis found in the second equality above. Hence, we must have \( \text{dim}(R(T)) = 2  \). 
\end{eg}

\begin{eg}\label{Example 9}
    Let \( T: \R^{3} \to \R^{2}  \) be the linear transformation defined by 
    \[  T(a_{1}, a_{2}, a_{3}) = (a_{1} - a_{2}, 2a_{3}). \]
    One can show that 
    \begin{center}
        \( N(T) = \{ (a, a, 0) \}: a \in \R   \) and \( R(T) = \R^{2} \).
    \end{center}
\end{eg}



Just like how the 'size' of a given subspace is denoted by its dimension, we can also determine the size of a null spaces and ranges. However, we will attach some special names associated with these sets.
\begin{definition}[Nullity]\label{Nullity}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. If \( N(T)  \) is finite-dimensional, then we define \textbf{nullity} of \( T  \) by \( \text{nullity}(T)  \) to be the dimension of \( N(T) \).
\end{definition}

\begin{definition}[Rank]\label{Rank}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \). If \( R(T)  \) is finite-dimensional, then we denote the \textbf{rank} of \( T  \) by \( \text{rank}(T) \) to be the dimension of \( R(T) \). 
\end{definition}

From these definitions, we can intuit the following relationships between Nullity and Rank of a linear transformation:
\begin{itemize}
    \item The larger the nullity, the smaller the rank of a linear transformation.
    \item the larger the rank, the smaller the nullity.
\end{itemize}
This relationship between the two spaces is encompassed in the next theorem.

\subsection{Dimension Theorem}

\begin{theorem}[Dimension Theorem]\label{Dimension Theorem}
   Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. If \( V  \) is finite-dimensional, then  
   \[ \text{nullity}(T) + \text{rank}(T) = \text{dim}(V).  \]
\end{theorem}
\begin{proof}
Suppose \( \text{dim}(V) = n  \) and \( \text{dim}(N(T)) = k  \), and \( \{ u_{1}, u_{2}, \dots, u_{k } \}  \) is a basis for \( N(T) \). By {\hyperref[Corollary to Theorem 1.11]{corollary to Theorem 1.11}} \( \beta \) can be extended to be a basis for \( V  \). Denote this basis as 
\[ \{ u_{1}, u_{2}, \dots, u_{n} \}.  \]
We claim that that \( S = \{ T(u_{k+1}), T(u_{k+2}), \dots, T(u_{n})  \}  \) is a basis for \( R(T) \).
First, we show that \( S  \) generates \( R(T) \). Using The {\hyperref[Spanning set for R(T)]{Theorem 2.1}} and the fact that \( T(u_{i}) = 0  \) for \( 1 \leq i \leq k  \), we get that 
\begin{align*}
    R(T) &= \text{span}(\{ T(u_{1}), T(u_{2}), \dots, T(u_{n}) \} ) \\
         &= \text{span}(\{T(u_{k+1}), T(u_{k+2}), \dots, T(u_{n})  \} )\\
         &= \text{span}(T(S)) .
\end{align*}
Hence, \( S  \) generates \( R(T) \).

Now, we want to show that \( S \) is linearly independent. Then choose scalars \( \alpha_{k+1}, \alpha_{k+2}, \dots, \alpha_{n} \) such that 
\[  \sum_{ i=k+1 }^{ n } \alpha_{i} T(u_{i}) = 0.  \tag{1} \]
Since \( T  \) is linear, we can re-write (1) into the following form
\[  T \Big( \sum_{ i= k+1  }^{ n } \alpha_{i} u_{i} \Big) = 0.  \]
Hence, we find that
\[  \sum_{ i=k+1 }^{ n }\alpha_{i} u_{i} \in N(T).\tag{2} \]
Since \( N(T) \) contains \( \{ u_{1}, u_{2}, \dots, u_{k} \}  \) as a basis, we can express (2) as a linear combination of vectors in this set. Hence, we have
\[  \sum_{ i=k+1 }^{ n } \alpha_{i} u_{i} = \sum_{ i=1 }^{ k  } \beta_{i} u_{i} \tag{3} \]
for some scalars \( \beta_{1}, \beta_{2}, \dots, \beta_{k} \).
Now, we have
\[  \sum_{ i=k+1 }^{ n } \alpha_{i} u_{i} - \sum_{ i=1 }^{ k  } \beta_{i} u_{i} = 0. \]
Since \( \{ u_{1}, u_{2}, \dots, u_{n}  \}   \) is linearly independent, we find that both \( \alpha_{i} \)'s and \( \beta_{i} \)'s are all zero. Hence, \( S  \) is also linearly independent. Thus, \( S  \) is a basis for \( R(T) \) and that \( \text{rank}(T) = n - k  \) and so we get our desired result
\[  \text{dim}(V) = \text{rank}(T) + \text{nullity}(T). \]
\end{proof}

Applying the dimension theorem to {\hyperref[Example 9]{example 9}} allows us to conclude that
\( \text{nullity}(T) + 2 = 3   \) implies \( \text{nullity}(T) = 1  \).


Before we move on, let us recall two key definitions needed for the next topic.

\begin{definition}[Injective Functions]\label{One-to-one}
    Let \( f: A \to B  \) and \( x,y \in A  \). We call \( f \) \textbf{one-to-one} if \( f(x) = f(y)  \) implies \( x = y  \) or, equivalently, if \( x \neq y  \) implies \( f(x) \neq f(y) \) (this latter part is the contrapositive of the definition).
\end{definition}

\begin{definition}[Surjective Functions]
    Let \( f: A \to B  \). We call \( f \) \textbf{onto} if \( f(A) = B  \); that is, for any \( y \in B  \), there exists an \( x \in A  \) such that \( f(x) = y  \).
\end{definition}

As we will see, these two definitions will give insights into the nullity and rank of linear transformations.

\begin{theorem}[Injectivity \( \iff \) Null Space is \( \{ 0\} \)]\label{Theorem 2.5}
   Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. Then \( T   \) is {\hyperref[One-to-one]{\textbf{one-to-one}}} if and only if \( N(T) = \{ 0 \}  \).  
\end{theorem}
\begin{proof}
    (\( \Rightarrow \)) Suppose \( T  \) is injective. Let \( x \in V  \) be arbitrary. Then \( T(x) = T(0_{V})  \) implies \( x = 0_{V} \). This tells us that \( N(T) = \{ 0 \}   \).
    (\( \Leftarrow \)) Conversely, let \( N(T) = \{ 0 \}  \). Let \( x,y \in V  \) be arbitrary and assume \( T(x) = T(y)  \). By using linearity, we have
    \begin{align*}
        T(x) = T(y) &\iff T(x) - T(y) = 0_{W} \\
                    &\iff T(x-y) = 0_{W} \\
    \end{align*}
    Since \( x - y \in N(T)  \) and \( N(T) = \{ 0 \}  \), we have \( x - y = 0  \) if and only if \( x = y  \). Hence, \( T  \) is an injective transformation.
\end{proof}

Referring back to {\hyperref[Example 9]{example 9}}, we find that \( \text{nullity}(T) = 1  \)implies that example 9 is not injective.

\begin{theorem}[Equal Finite Dimensions Between Vector Spaces]
    Let \( V  \) and \( W  \) be vector spaces of equal (finite) dimension, and let \( T : V \to W  \) be linear. Then the following are equivalent.
    \begin{enumerate}
        \item[(a)] \( T  \) is injective.
        \item[(b)] \( T \) is surjective.
        \item[(c)] \( \text{rank}(T) = \text{dim}(V) \).
    \end{enumerate}
\end{theorem}

\begin{proof}
    Using the dimension theorem, we have
    \[ \text{nullity}(T) + \text{rank}(T) = \text{dim}(V).  \]
    By {\hyperref[Theorem 2.5]{theorem 2.5}}, We know that \( T  \) is injective if and only if \( N(T) = \{ 0  \}  \). By definition, we have  \( \text{nullity}(T) = 0  \). By the {\hyperref[Dimension Theorem]{dimension theorem}}, this is true if and only if 
    \[ \text{rank}(T) = \text{dim}(V) \iff \text{rank}(T) = \text{dim}(W) \]
    where \( V  \) and \( W  \) have equal dimensions.
    This is true if and only if \( \text{dim}(R(T)) = \text{dim}(W ) \). This is true if and only if \( R(T) = W  \) by {\hyperref[Theorem 1.11]{theorem 1.11}}. By definition, we know that \( T \) is surjective. 
\end{proof}

\begin{remark}
    If \( V  \) is not finite-dimensional then we find that neither (a) nor (b) follows from each other.
\end{remark}

\begin{eg}
    Let \( T: P_{2}(\R) \to P_{3}(\R)  \) be the linear transformation defined by 
    \[  T(f(x)) = 2 f'(x) + \int_{ 0 }^{ x }  3f(t)  \ dt. \]
    Observe that
    \begin{center}
        \( R(T) = \text{span}(T(1), T(x), T(x^{2})) = \text{span}(\{ 3x, 2 + \frac{ 3 }{ 2 } x^{2} , 4x + x^{3} \} ). \)
    \end{center}
    One can show that the set \( \{ 3x, 2 + \frac{ 3 }{ 2 } x^{2} , 4x + x^{3} \}  \) is linearly independent and that \( \text{rank}(T) = 3  \). Since \( \text{rank}(T) \neq \text{dim}(P_{3}(\R)) \), we have that \( T  \) is not surjective. Since \( \text{dim}(P_{2}(\R)) = 3  \), we know that
    \[   \text{rank}(T) = \text{dim}(P_{2}(\R)) \]
    by the Dimension Theorem. Thus, \( \text{nullity}(T) = \{ 0 \}  \) and hence \( T  \) is injective.
\end{eg}

\begin{eg}
    Let \( T: F^{2} \to F^{2}  \) be the linear transformation defined by 
    \[ T(a_{1}, a_{2}) = (a_{1} + a_{2}, a_{1}).  \]
    One can show that \( T  \) is injective so that \( N(T) = \{ 0 \}  \). The dimension theorem tells us that \( T  \) must also be onto.
\end{eg}

\begin{eg}
    Let \( T: P_{2}(\R) \to \R^{3} \) be the linear transformation defined by 
    \[  T(a_{0} + a_{1}x + a_{2} x^{2}) = (a_{0}, a_{1}, a_{2}). \]
    One can easily show that \( T  \) is linear and one-to-one. We find that \( S = \{ 2 - x + 3x^{2} , x + x^{2} , 1 - 2x^{2} \}  \) is linearly independent in \( P_{2}(\R) \) since 
    \[  T(S) = \{ (2,-1,3), (0,1,1), (1,0,-2) \}  \] is linearly independent in \( \R^{3} \).
\end{eg}

\begin{theorem}[Unique Linear Transformations]\label{Theorem 2.6}
    Let \( V  \) and \( W  \) be vector spaces over \( F  \), and suppose that \( \{ v_{1}, v_{2}, \dots, v_{n} \}   \) is a basis for \( V  \). For \( w_{1}, w_{2},  \dots, w_{n} \in W   \), there exists exactly one linear transformation \( T: V \to W  \) such that \( T(v_{i}) = w_{i}  \) for all \( 1 \leq i \leq n  \). 
\end{theorem}

\begin{proof}
Let \( x \in V  \). Since \( \{ v_{1}, v_{2}, \dots, v_{n} \}  \) is a basis for \( V  \), we have
\[  x = \sum_{ i=1 }^{ n } \gamma_{i} v_{i}  \] for some scalars \( \gamma_{1}, \gamma_{2}, \dots, \gamma_{n}  \). Define the map \( T: V \to W  \) by
\[  T(x) = \sum_{ i=1 }^{ n } \gamma_{i} w_{i}  \]
for \( w_{i} \in W  \) and \( x \in V  \). 

First, we show that \( T  \) is linear. Let \( x,y \in V  \). Then 
\[ x =  \sum_{ i=1 }^{ n } \gamma_{i} v_{i}  \] and 
\[ y =  \sum_{ i=1 }^{ n } \delta_{i} v_{i}  \] for some scalars \( \gamma_{i}  \) and \( \delta_{i}  \) for all \( 1 \leq i \leq n \). Then by definition of \( T  \), we get that 
\[  T(x) = \sum_{ i=1 }^{ n } \gamma_{i} w_{i}  \] and 
\[ T(y) = \sum_{ i=1 }^{ n } \delta_{i} w_{i}. \]
By definition of \( T  \), we get that  
\[  T(x+y) = \sum_{ i=1 }^{ n } (\gamma_{i} + \delta_{i}) w_{i} = \sum_{ i=1 }^{ n } \gamma_{i} w_{i} + \sum_{ i=1 }^{ n } \delta_{i} w_{i} = T(x) + T(y).  \]
Hence, the first property of linearity holds. For the second property, let \( c \in F  \) and observe that
\[  T(cx) = \sum_{ i=1 }^{ n } \gamma_{i} (cw_{i}) = c \sum_{ i=1 }^{ n } \gamma_{i} w_{i} = c T(x). \]
Hence, the second property of linearity holds. We conclude that \( T  \) is linear and that \( T(u_{i}) = w_{i}  \) for all \( 1 \leq i \leq n \).

Now suppose there exists another linear map \( U: V \to W  \) such that \( U(v_{i}) = w_{i}  \) for all \( 1 \leq i \leq n \) defined by.
\[ U(x) = \sum_{i =1  }^{ n } \gamma_{i} U(v_{i}).  \]
Then observe that
\[ U(x) = \sum_{ i=1 }^{ n } \gamma_{i} U(v_{i}) = \sum_{ i=1 }^{ n } \gamma_{i} w_{i} = T(x). \]
Thus, \( U = T  \) for all \( x \in V  \) and so, we conclude that \( T  \) is a unique linear map.
\end{proof}

\begin{corollary}
  Let \( V  \) and \( W  \) be vector spaces, and suppose that \( V  \) has a finite basis \( \{ v_{1}, v_{2}, \dots, v_{n} \}  \). If \( U,T : V \to W  \) are linear and \( U(v_{i}) = T(v_{i}) \) for all \( 1 \leq i \leq n \), then \( U = T  \).  
\end{corollary}

\begin{eg}
    Let \( T : \R^{2} \to \R^{2}  \) be the linear transformation defined by 
    \[  T(a_{1}, a_{2}) = (2a_{2} - a_{1}, 3a_{1}) \]
    and suppose that \( U: \R^{2} \to \R^{2}  \) is linear. Let \( \{ (1,2), (1,1) \}  \) be a basis for \( \R^{2} \). If \( U(1,2) = (3,3)  \) and \( U(1,1) = (1,3)  \), then we find that \( U(x_{i}) = T(x_{i}) \) \( i = 1,2 \). Thus \( U = T  \) from the corollary to {\hyperref[Theorem 2.6]{theorem 2.6}}. 
\end{eg}

