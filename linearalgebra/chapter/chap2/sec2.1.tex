\section{Linear Transformations, Null Spaces, and Ranges}

Suppose we have a function \( T  \) with domain \( V  \) and codomain \( W  \) denoted by \( T: V \to W  \).


\begin{definition}[Linear Transformation]\label{Linear Transformation}
   Let \( V  \) and \( W  \) be vector spaces (over \( F  \)). We call a function \( T: V \to W  \) a \textbf{linear transformation from \( V  \) to \( W  \)}, for all \( x,y \in V  \) and \( c \in F  \), we have
   \begin{enumerate}
       \item[(a)] \( T(x+y) = T(x) + T(y) \) and
        \item[(b)] \( T(cx) = c T(x) \).
   \end{enumerate}
\end{definition}

\begin{remark}
    If \( F = \Q   \), then (a) implies (b) in the definition above. Otherwise, (a) and (b) are logically independent statements.  
\end{remark}
The following are a list of properties for linear functions:
\begin{enumerate}
    \item If \( T  \) is linear, then \( T(0) = 0  \).
        \begin{proof}
        Suppose \( T  \) is linear, then \( T(0 \cdot 0 ) = 0 T(0) = 0  \).
        \end{proof}
    \item \label{Second Property of Linearity} We have \( T  \) is linear if and only if \( T(cx+y) = cT(x) + T(y)  \) for all \( x,y \in V  \) and \( c \in F  \).
        \begin{proof}
        Suppose \( T  \) is linear. Let \( x,y \in V  \) and \( c \in F  \). Then 
        \[  T(cx+y) = T(cx) + T(y) = cT(x) + T(y). \]
        Conversely, if \( c = 1  \) then 
        \[  T(x + y) = T(x) + T(y). \]
        If  \( y = 0  \), then 
        \[ T(cx) = cT(x).  \]
        Hence, \( T  \) is a linear transformation.
        \end{proof}
    \item If \( T  \) is linear, then \( T(x-y) = T(x) - T(y)  \) for all \( x,y \in V  \).
        \begin{proof}
        Let \( x,y \in V  \). Suppose \( T  \) is linear, then 
        \[ T(x - y) = T(x) + T(-y) = T(x) - T(y).  \]
        \end{proof}
    \item \( T  \) is linear if and only if, for \( x_{1}, x_{2} , \dots, x_{n} \in V  \) and \( a_{1}, a_{2}, \dots, a_{n} \in F  \), 
        we have
        \[  T \Big(  \sum_{ i=1 }^{ n } a_{i} x_{i}  \Big) = \sum_{ i=1 }^{ n } a_{i} T(x_{i}). \]
        \begin{proof}
        Suppose \( T  \) is linear. Let \( x_{1}, x_{2}, \dots, x_{n} \in V  \) and \( a_{1}, a_{2}, \dots, a_{n} \in F  \) such that
        \[  v = \sum_{ i=1 }^{ n } a_{i} x_{i}. \]
        Then observe that 
        \[
            T \Big( \sum_{ i=1 }^{ n } a_{i} x_{i} \Big) = \sum_{ i=1 }^{ n } T( a_{i} x_{i}) 
                                                         = \sum_{ i=1 }^{ n } a_{i} T(x_{i}).
        \]
        Conversely, for \( i = 2  \) and denote \( a_{i} x_{i} = y_{i} \). Then
        \[  T\Big(\sum_{ i=1 }^{ 2 } a_{i} x_{i} \Big) = T(a_{1} x_{1}) + T(a_{2} x_{2}) = T(y_{1}) + T(y_{2})  \]  
        and so property 1 is satisfied. 
        If \( i = 1  \), then 
        \[  T \Big( \sum_{ i=1 }^{ 1 } a_{i}x_{i}  \Big) = a_{1} T(x_{1}) \]
        and so property 2 is satisfied. Hence, \( T  \) is a linear transformation.
        \end{proof}
\end{enumerate}

\begin{eg}
   Define  
   \begin{center}
       \( T: \R^{2} \to \R^{2}  \) by \( T(a_{1}, a_{2}) = (2a_{1} + a_{2}, a_{1}) \).
   \end{center}
   Show that \( T  \) is linear. Let \( c \in \R  \) and \( x,y \in \R^{2} \), where \( x = (a_{1}, a_{2}) \) and \( y = (b_{1}, b_{2}) \). Then
   \begin{align*}
       cx + y &= c(a_{1}, a_{2}) + (b_{1}, b_{2})  \\
              &=  (ca_{1}, ca_{2} ) + (b_{1}, b_{2}) \\
              &=  (ca_{1} + b_{1} , ca_{2} + b_{2}).
   \end{align*}
   So, we have
   \begin{align*}
                                       T(cx + y)  &= T(ca_{1} + b_{1} , ca_{2} + b_{2})  \\
                                         &= (2(ca_{1} + b_{1})  + ca_{2} + b_{2}, ca_{1} + b_{1} ) \\
                                          &= ((2ca_{1} + ca_{2}) +  (2b_{1} + b_{2}), ca_{1} + b_{1}  ) \\
                                          &= (2ca_{1} + ca_{2}, ca_{1}) + (2b_{1} + b_{2}, b_{1} ) \\
                                          &= c(2a_{1} + a_{2}, a_{1}) + (2b_{1} + b_{2}, b_{1}) \\
                                          &= cT(a_{1}, a_{2}) + T(b_{1}, b_{2}) \\
                                          &= cT(x) + T(y).
   \end{align*}
   Hence, we have that \( T(cx + y) = cT(x) + T(y) \) so \( T: \R^{2} \to \R^{2}   \) is a linear.
\end{eg}

\subsection{Examples of Linear Transformations}

The most common linear transformations come from geometry:
\begin{itemize}
    \item Rotations: \( T_{\theta}(a_{1}, a_{2}) = (a_{1}, a_{2}) \) by performing a counter-clockwise rotation by an angle \( \theta \) if \( (a_{1}, a_{2}) \neq (0,0) \). This is called \textbf{rotation by \( \theta \)} and \( T_{\theta}(0,0) = (0,0) \) otherwise.
    \item Reflections about the \( x- \)axis: \( T(a_{1}, a_{2}) = (a_{1}, - a_{2}) \).
    \item Projections on the \( x- \)axis: \( T(a_{1}, a_{2}) = (a_{1}, 0 ) \).
\end{itemize}

\begin{eg}[Rotations]
    Define the rotation transformation above by \( T_{\theta}: \R^{2} \to \R^{2}  \) by \( T_{\theta}(a_{1}, a_{2}) \) where it is described as above. Our goal in this example is to define an explicit formula of this transformation. Let us fix a nonzero vector \( (a_{1}, a_{2}) \in \R^{2} \). Let \( \alpha  \) be the angle such that \( (a_{1}, a_{2})\) makes with the positive \( x-\)axis, and let \( r = \sqrt{ a_{1}^{2} + a_{2}^{2}  }  \). Then using some trigonometry, we get that \( a_{1} = r \cos(\alpha)  \) and \( a_{2} = r \sin(\alpha) \). Note that \( T_{\theta}(a_{1}, a_{2}) \) has length \( r  \) and makes an angle \( \alpha + \theta  \) with the positive \( x- \)axis. Using some trigonometric identities, we get that
    \begin{align*}
        T_{\theta}(a_{1}, a_{2}) &= (r\cos(\alpha + \theta), r \sin(\alpha + \theta)) \\
                                 &= (r \cos(\alpha) \cos(\theta) - r \sin(\alpha ) \sin(\theta) , r \cos(\alpha) \sin(\theta) + r \sin(\alpha) \cos(\theta) ) \\
                                 &= (a_{1} \cos(\theta) - a_{2} \sin(\theta) , a_{1} \sin(\theta) + a_{2} \cos(\theta)).  \\
    \end{align*}
    Observe that this formula holds for \( (a_{1}, a_{2}) = (0,0) \). One can show that \( T_{\theta}  \) is linear.
\end{eg}

\begin{eg}[Transpose]
    Define \( T: M_{m \times n } (F) \to M_{n \times m}(F) \) by \( T(A) = A^{t}  \), where \( A^{t} \) is the {\hyperref[Transpose]{transpose}}  of \( A  \), defined in Section 1.3. 
\end{eg}

\begin{eg}[Derivatives of Polynomials]
    Define \( T: P_{n}(\R) \to P_{n-1}(\R) \) by \( T(f(x)) = f'(x)  \), where \( f'(x)  \) denotes the derivative of \( f(x)  \). We can show that \( T  \) is linear. Let \( g(x) , h(x) \in P_{n}(\R) \) and \( a \in \R  \).Assuming that the differentiation rules holds for any \( T  \), we have
    \begin{align*}
        T(ag(x) + h(x)) &= (ag(x) + h(x))' \\
                        &= ag'(x) + h'(x) \\
                        &= a T(g(x)) + T(h(x)).
    \end{align*}
    By the {\hyperref[Second Property of Linearity]{second property of linearity}} , we have that \( T  \) is linear.
\end{eg}

\begin{eg}[Integrals]\label{Integrals are Linear}
    Let \( V = C(\R) \) be the vector space of continuous real-valued functions on \( \R \). Let \( a,b \in \R  \), \( a < b  \). Define \( T: V \to \R  \) by
    \[  T(f) = \int_{ a }^{ b }  f(t) \ dt  \] for all \( f \in V  \). We know that \( T  \) is a linear transformation because the definite integral of a linear combination of functions is the same as the linear combination of the definite integrals of the functions; that is, 
    \[  \int_{ a }^{ b } \sum_{ i=1 }^{ n } \gamma_{i} f(t_{i}) \ dt = \sum_{ i=1 }^{ n } \gamma_{i} \int_{ a }^{ b } f(t_{i})   \ dt = \sum_{ i=1 }^{ n } \gamma_{i} T(f_i)  \]
    for scalars \( \gamma_{i}  \) where \( 1 \leq i \leq n\) and  
    \[  T(f_i) =  \int_{ a }^{ b } f(t_{i}) \ dt.  \]
\end{eg}


\subsection{Identity and Zero Transformations}

\begin{remark}
   Two very important examples of linear transformations are the \textbf{identity and zero transformations}. 
\end{remark}

\begin{definition}[Identity Transformation]\label{Identity Transformation}
    The \textbf{identity transformation} is denoted by \( I_{V}: V \to V  \) by \( I_{V}(x) = x  \) for all \( x \in V  \). From now on, we will denote this transformation as \( I  \) instead of \( I_{V} \).
\end{definition}

\begin{definition}[Zero Transformation]\label{Zero Transformation}
    The \textbf{zero transformation} is denoted by \( T_{0}: V \to W  \) by \( T_{0}(x) = 0  \) for all \( x \in V  \).
\end{definition}

\subsection{Range and Null Spaces}

The \textit{Range} and \textit{Null} spaces give us important insights on the intrinsic properties of a linear transformation.



\begin{definition}[Null Space]\label{Null Spaces}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. 
   The \textbf{null space} (or \textbf{kernel}), denoted by \( N(T) \) of \( T  \), is the set of all vectors \( x \in V  \) such that \( T(x) = 0  \); that is, \( N(T) = \{ x \in V : T(x) = 0  \}  \). 
\end{definition}

\begin{definition}[Range]\label{Range}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. 
   The \textbf{range} (or \textbf{image}) \( R(T)  \) of \( T  \) is the subset of \( W  \) consisting of all images (under \( T \)) of vectors in \( V  \); that is, \( R(T) = \{ T(x) : x \in V  \}  \).
\end{definition}

\begin{eg}
    Let \( V  \) and \( W  \) be vector spaces, and let \( I: V \to V  \) and \( T_{0}: V \to W  \) be the identity and zero transformations, respectively. Then \( N(I) = \{ 0  \}  \), \( R(I) = V  \), \( N(T_{0}) = V  \), and \( R(T_{0}) = \{ 0 \}  \).
\end{eg}


\begin{eg}
    Let \( T: \R^{3} \to \R^{2}  \) be the linear transformation defined by  
    \[  T(a_{1}, a_{2}, a_{3}) = (a_{1} - a_{2}, 2a_{3} ). \]
    One can verify that 
    \begin{center}
        \( N(T) = \{ (a,a,0) : a \in \R  \}  \) and \( R(T) = \R^{2} \).
    \end{center}
\end{eg}


\begin{theorem}[ ]
    Let \( V  \) and \( W  \) be vector spaces and \( T: V \to W  \) be linear. Then \( N(T) \) and \( R(T) \) are subspaces of \( V  \) and \( W  \), respectively.
\end{theorem}
\begin{proof}
    Let \( T: V \to W   \) be linear. First, we show that \( N(T) \) is a subspace of \( V  \). 
    \begin{enumerate}
        \item[(a)] Note that \( 0_{V} \in V  \) implies that \( T(0_{V}) = 0_{W}  \) since \( T  \) is linear (property (1) of linearity). Hence, \( 0_{V} \in N(T)  \).
        \item[(b)] Let \( x, y \in N(T) \). By definition, of the null space we have \( T(x) = 0_{W}  \) and \( T(y) = 0_{W} \). Since \( T  \) is linear, we get that
            \[  T(x+y) = T(x) + T(y) = 0_{W} + 0_{W} = 0_{W} \in N(T). \]
        Hence, \( N(T) \) is closed under addition.
        \item[(c)] Let \( x \in N(T)  \) and let \( c \in F  \). Then 
            \[ T(cx) = cT(x) = c 0_{V} = 0_{W} \in N(T).  \]
    \end{enumerate}
    Hence, {\hyperref[Subspaces]{Theorem 1.3}} tells us that \( N(T) \) is a subspace of \( V  \).
    Now, we will show that \( R(T) \) is a subspace of \( W  \).
    \begin{enumerate}
        \item[(a)] Note that \( 0_{V} \in V  \) implies that \( T(0_{V}) = 0_{W} \in R(T) \) by property (1) of linearity. So, \( 0_{W} \in R(T) \).
        \item[(b)] Now let \( x,y \in R(T)  \). Then there exists elements \( z, w \in W   \) such that \( T(x) = z  \) and \( T(y) = w  \). Then by linearity, we have \( T(x+y) = T(x) + T(y) = z + y \in W   \). Hence, we must have \( x + y \in R(T) \) and thus \( R(T)  \) is closed under addition.
        \item[(c)] Let \( x \in R(T)  \) and \( c \in F \). Then we have \( T(cx) = c T(x) \in W  \) since \( T(x) \in W  \). Hence, \( cx \in R(T) \).
    \end{enumerate}
    Thus, {\hyperref[Subspaces]{Theorem 1.3}} also tells us that \( R(T)  \) is a subspace of \( W  \).
\end{proof}

The next theorem allows us to find a spanning set for the range of a linear transformation.

\begin{theorem}[Spanning Set for a Linear Transformation]\label{Spanning set for R(T)}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T:V \to W  \) be linear. If \( \beta = \{ v_{1}, v_{2}, \dots v_{n} \}  \) is a basis for \( V  \), then   
    \[  R(T) = \text{span}(T(\beta)) = \text{span}(\{ T(v_{1}), T(v_{2}) , \dots, T(v_{n}) \} ). \]
\end{theorem}

\begin{proof}
Let \( T: V \to W  \) be linear where \( V  \) and \( W  \) are vector spaces. To show that \( T(\beta) \) generates \( R(T) \); we need to show that  
\begin{center}
    \( \text{span}(T(\beta)) \subseteq R(T) \) and \( R(T) \subseteq \text{span}(T(\beta)) \).
\end{center}
    Observe that \( T(v_{i}) \in R(T) \) for all \( i  \). Hence, \( T(\beta) \subseteq R(T) \). By theorem 1.5, we know that \( R(T) \) also contains the span of \( T(\beta) \). Hence, \( \text{span}(T(\beta)) \subseteq R(T) \). 

    Let \( w \in R(T) \). Then for some \( v \in W  \), we have \( T(v) = w \). Since \( \beta \) is a basis for \( V  \), we choose scalars \( \alpha_{1}, \alpha_{2}, \dots, \alpha_{n} \) such that 
    \[  v  = \sum_{ i=1 }^{ n } \alpha_{i} v_{i}. \]
    Since \( T  \) is linear, we have that
    \[  w = T(v) = T \Big( \sum_{ i=1 }^{ n } a_{i} v_{i} \Big) = \sum_{ i=1 }^{ n } a_{i} T(v_{i}). \]
    This tells us that \( v \in \text{span}(T(\beta)) \). Hence, \( T(\beta) \) generates \( R(T) \).
\end{proof}

\begin{eg}
    Define the linear transformation \( T: P_{2}(\R) \to M_{2 \times 2}(\R) \) by 
    \[  T(f(x)) = \begin{pmatrix}
        f(1) - f(2) & 0 \\
        0 & f(0)
    \end{pmatrix}. \]
    Note that 
    \begin{center}
        \( \beta = \{ 1,x, x^{2} \}  \) is a basis for \( P_{2}(\R)  \) and \( T(\beta) = \{ T(1), T(x), T(x^{2}) \}. \)
    \end{center}
    Then observe that
    \begin{align*}
        R(T) &= \text{span}(T(\beta)) \\
             &= \text{span}\Big( \Big\{ \begin{pmatrix}
                         0 & 0 \\
                         0 & 1 
             \end{pmatrix}, \begin{pmatrix}
                         -1 & 0 \\
                         0 & 0 
             \end{pmatrix}, \begin{pmatrix}
                         -3 & 0 \\ 
                         0 & 0 
             \end{pmatrix} \Big\}  \Big).
    \end{align*}
    This tells us that \( R(T) \) contains the basis found in the second equality above. Hence, we must have \( \text{dim}(R(T)) = 2  \). 
\end{eg}

Just like how the 'size' of a given subspace is denoted by its dimension, we can also determine the size of a null spaces and ranges. However, we will attach some special names associated with these sets.
\begin{definition}[Nullity]\label{Nullity}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. If \( N(T)  \) is finite-dimensional, then we define \textbf{nullity} of \( T  \) by \( \text{nullity}(T)  \) to be the dimension of \( N(T) \).
\end{definition}

\begin{definition}[Rank]\label{Rank}
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \). If \( R(T)  \) is finite-dimensional, then we denote the \textbf{rank} of \( T  \) by \( \text{rank}(T) \) to be the dimension of \( R(T) \). 
\end{definition}

From these definitions, we can intuit the following relationships between Nullity and Rank of a linear transformation:
\begin{itemize}
    \item The larger the nullity, the smaller the rank of a linear transformation.
    \item the larger the rank, the smaller the nullity.
\end{itemize}
This relationship between the two spaces is encompassed in the next theorem.

\begin{theorem}[Dimension Theorem]\label{Dimension Theorem}
   Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. If \( V  \) is finite-dimensional, then  
   \[ \text{nullity}(T) + \text{rank}(T) = \text{dim}(V).  \]
\end{theorem}
\begin{proof}

\end{proof}
