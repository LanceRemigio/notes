\section{Invertibility and Isomorphisms}

\subsection{Inverses of Linear Maps}
\begin{itemize}
    \item We will see in this section that inverses of linear maps are linear.
    \item Inverses of linear maps allow us to gain insight on properties of inverted matrices.
    \item Many of the results found in this section are applied to concept of \textit{isomorphisms}. 
\end{itemize}

\begin{definition}[Invertibility of Linear Maps]
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. A function \( U: W \to V  \) is said to be an \textbf{inverse} of \( T  \) if \( TU = {I}_{W} \) and \( UT = {I}_{V} \). If \( T  \) has an inverse, then \( T  \) is said to be \textbf{invertible}. As noted in Appendix \( B  \), if \( T  \) is invertible, then the inverse of \( T  \) is unique and is denoted by \( T^{-1} \). 
\end{definition}

The following facts hold for invertible functions \( T  \) and \( U  \).
\begin{enumerate}
    \item \( (TU)^{-1} = U^{-1} T^{-1} \).
    \item \( (T^{-1})^{-1} = T  \); in particular, \( T^{-1}  \) is invertible.
\end{enumerate}

\begin{itemize}
    \item We often refer to functions being invertible if and only if they are both injective and surjective. This allows us to restate Dimension Theorem when applied to two equal finite-dimensional vector spaces.
\end{itemize}

\begin{theorem}[Dimension Theorem In Terms of Invertibility]
  Let \( T: V \to W  \) be a linear transformation, where \( V  \) and \( W  \) are finite-dimensional spaces of equal dimension. Then \( T  \) is invertible if and only if \( \text{rank}(T) = \text{dim}(V ) \).  
\end{theorem}

\begin{eg}
    Let \( T: {P}_{1}(\R) \to \R^{2}  \) be the linear transformation defined by
    \[  T(a+bx) = (a, a+b). \]
    One can verify directly that \( T^{-1}: \R^{2} \to {P}_{1}(\R) \) is defined by 
    \[  T^{-1}(c,d) = c + (d-c)x \]
    and that \( T^{-1} \) is also linear.
\end{eg}

\begin{theorem}[Inverses Of Linear Maps Are Linear]
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear and invertible. Then \( T^{-1}: W \to V  \) is linear.
\end{theorem}
\begin{proof}
Let \( {y}_{1}, {y}_{2} \in W  \) and \( c \in F  \). Since \( T  \) is surjective and injective, there exists unique vectors \( {x}_{1}  \) and \( {x}_{2}  \) such that \( T({x}_{1}) = {y}_{1}  \) and \( T({x}_{2}) = {y}_{2} \). Thus, we have \( {x}_{1} = T^{-1}({y}_{1})  \) and \( {x}_{2} = T^{-1}({y}_{2}) \). Since \( T  \) is linear, we must have that
\begin{align*}
    T^{-1}( c {y}_{1} + {y}_{2}) &= T^{-1}( c T({x}_{1}) + T({x}_{2})) \\
                                 &= T^{-1}( T(c {x}_{1}) + T({x}_{2})) \\
                                 &= T^{-1}(T({cx}_{1} + {x}_{2})) \\
                                 &= T^{-1}T({cx}_{1} + {x}_{2}) \\
                                 &= {I}_{V}({cx}_{1} + {x}_{2}) \\
                                 &= {cx}_{1} + {x}_{2} \\
                                 &= c T^{-1}({y}_{1}) + T^{-1}({y}_{2}).
\end{align*}
Hence, \( T^{-1} \) is a linear map.
\end{proof}

\begin{itemize}
    \item This tells us that the notions of invertibility, injectivity, and surjectivity in {\hyperref[Theorem 2.5]{Theorem 2.5}} are all equivalent. 
\end{itemize}

\subsection{Inverse of a Matrix}

\begin{definition}[Invertibility of Matrices]
   Let \( A  \) be an \( n \times n  \) matrix. Then \( A  \) is \textbf{invertible} if there exists an \( n \times n  \) matrix \( B  \) such that \( AB = BA = I  \).
\end{definition}

\begin{itemize}
    \item When \( A  \) is invertible, the matrix \( B  \) and its product \( AB  \) can be written in the following way:
        \[  AB = BA = I. \]
    \item If \( C  \) was another such matrix such that the above is true (replace \( B  \) with \( C  \)), then we can write 
        \[  C = CI = C(AB) = (CA)B = IB = B.   \]
    \item The matrix \( B  \), in this case, is called the \textbf{inverse} of \( A  \) and is denoted by \( A^{-1} \). 
\end{itemize}

\begin{lemma}
   Let \( T \) be an invertible linear transformation from \( V  \) to \( W  \). Then \( V  \) is finite-dimensional if and only if \( W  \) is finite-dimensional. In this case, \( \text{dim}(V ) = \text{dim}(W) \). 
\end{lemma}
\begin{proof}
    Suppose that \( V  \) is finite-dimensional. Let \( \beta = \{  {x}_{1}, {x}_{2}, \dots, {x}_{n} \}  \) be a basis for \( V  \). By {\hyperref[Spanning set for R(T)]{Theorem 2.2}}, we get that \( \text{span}(T(\beta)) = R(T) \). Since \( T  \) is invertible, we know that \( T  \) is also surjective. Hence, \( \text{span}(T(\beta)) = R(T) = W  \) and so \( W  \) is finite-dimensional by {\hyperref[Theorem 1.9]{Theorem 1.9}}. Conversely, suppose that \( W  \) is finite-dimensional. So, let \( \gamma = \{ {y}_{1}, {y}_{2}, \dots, {y}_{n} \}   \) be a basis for \( W  \). Since \( T  \) is surjective, we know that for each \( {y}_{i} \) in \( \gamma \) that \( {y}_{i} = T({x}_{i}) \) for each \( i  \). Hence, we have that \( T^{-1}({y}_{i}) = {x}_{i} \) for all \( i \). By Theorem 2.2 again, we know that \( T^{-1}(\gamma) \) spans \( R(T^{-1}) \) where \( R(T^{-1}) = V  \) since \( T^{-1}  \) is surjective. Hence, \( V  \) is finite-dimensional.
    
    Now, suppose that both \( V  \) and \( W  \) are finite-dimensional. Since \( T  \) is injective and surjective, we must have that
    \begin{center}
        \( \text{nullity}(T) = 0  \) and \( \text{rank}(T) = \text{dim}(R(T)) = \text{dim}(W) \).
    \end{center}
    By the {\hyperref[Dimension Theorem]{Dimension Theorem}}, we have that \( \text{rank}(T) = \text{dim}(V) \) which implies that 
    \[  \text{dim}(V) = \text{dim}(W). \]
\end{proof}

\begin{theorem}[Invertibility of Linear Maps \( \iff \) Invertibility of Matrices]
   Let \( V  \) and \( W  \) be finite-dimensional vector spaces with ordered bases \( \beta \) and \( \gamma  \), respectively. Let \( T: V \to W  \) be linear. Then \( T  \) is invertible if and only if \( [T]_{\beta}^{\gamma}   \) is invertible. Furthermore, \( [T^{-1}]_{\gamma}^{\beta}  = ([T]_{\beta}^{\gamma} )^{-1} \).
\end{theorem}
\begin{proof}
Let \( V  \) and \( W  \) be vector spaces with ordered bases \( \beta  \) and \( \gamma \) respectively. Let \( T  \) be linear. Suppose \( T  \) is invertible. Then there exists a unique linear transformation \( T^{-1}: W \to V  \) by definition. Hence, observe that 
\[  [T^{-1}]_{\gamma}^{\beta}  [T]_{\beta}^{\gamma}  = [T^{-1}T ]_{\beta} = [{I}_{V}]_{\beta} = {I}_{n}  \]
and similarly
\[ [T]_{\beta}^{\gamma}  [T^{-1}]_{\gamma}^{\beta}  = [T T^{-1}]_{\beta} = [{I}_{V}]_{\beta} = {I}_{n}.        \]
Hence, \( [T]_{\beta}^{\gamma}  \) is invertible and that 
\[ [T^{-1}]_{\gamma}^{\beta}  = ([T]_{\beta}^{\gamma} )^{-1}.  \]

Conversely, suppose that \( [T]_{\beta}^{\gamma}  \) is invertible. Since \( V \) and \( W  \) are finite-dimensional vector spaces where  \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) and \( \gamma = \{ {w}_{1}, {w}_{2}, \dots, {w}_{n} \}  \) are ordered bases for \( V  \) and \( W  \), respectively. Hence, we know by Theorem 2.6 there exists a unique linear transformation \( U: W \to V  \) such that 
\[  U({w}_{j}) = \sum_{ i = 1   }^{ n } {B}_{ij} {v}_{i}    \]
where \( {v}_{i} \in \beta \). Since \( [T]_{\beta}^{\gamma} = A   \) is invertible, we know that 
\[  AB = BA = I_n.  \] We want to show that \( UT = {I}_{V}  \) and \( TU = {I}_{W} \). Define the matrix representation \( [U]_{\gamma}^{\beta}  \). We need to show that \(  U = T^{-1} \). Hence, we have
\[  [UT]_{\beta} = [U]_{\gamma}^{\beta} [T]_{\beta}^{\gamma} = BA = {I}_{n} = [{I}_{V}]_{\beta}      \]
and 
\[ [TU]_{\gamma} = [T]_{\beta}^{\gamma} [U]_{\gamma}^{\beta} = AB = {I}_{n} = [{I}_{W}]_{\gamma}      \]
by Theorem 2.11. Hence, we have \( UT = {I}_{V}  \) and \( TU = {I}_{W} \). Thus, \( T  \) is invertible.
\end{proof}
