\section{Invertibility and Isomorphisms}

\subsection{Inverses of Linear Maps}
\begin{itemize}
    \item We will see in this section that inverses of linear maps are linear.
    \item Inverses of linear maps allow us to gain insight on properties of inverted matrices.
    \item Many of the results found in this section are applied to concept of \textit{isomorphisms}. 
\end{itemize}

\begin{definition}[Invertibility of Linear Maps]
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear. A function \( U: W \to V  \) is said to be an \textbf{inverse} of \( T  \) if \( TU = {I}_{W} \) and \( UT = {I}_{V} \). If \( T  \) has an inverse, then \( T  \) is said to be \textbf{invertible}. As noted in Appendix \( B  \), if \( T  \) is invertible, then the inverse of \( T  \) is unique and is denoted by \( T^{-1} \). 
\end{definition}

The following facts hold for invertible functions \( T  \) and \( U  \).
\begin{enumerate}
    \item \( (TU)^{-1} = U^{-1} T^{-1} \).
    \item \( (T^{-1})^{-1} = T  \); in particular, \( T^{-1}  \) is invertible.
\end{enumerate}

\begin{itemize}
    \item We often refer to functions being invertible if and only if they are both injective and surjective. This allows us to restate Dimension Theorem when applied to two equal finite-dimensional vector spaces.
\end{itemize}

\begin{theorem}[Dimension Theorem In Terms of Invertibility]
  Let \( T: V \to W  \) be a linear transformation, where \( V  \) and \( W  \) are finite-dimensional spaces of equal dimension. Then \( T  \) is invertible if and only if \( \text{rank}(T) = \text{dim}(V ) \).  
\end{theorem}

\begin{eg}\label{Eg 2.4.1}
    Let \( T: {P}_{1}(\R) \to \R^{2}  \) be the linear transformation defined by
    \[  T(a+bx) = (a, a+b). \]
    One can verify directly that \( T^{-1}: \R^{2} \to {P}_{1}(\R) \) is defined by 
    \[  T^{-1}(c,d) = c + (d-c)x \]
    and that \( T^{-1} \) is also linear.
\end{eg}

\begin{theorem}[Inverses Of Linear Maps Are Linear]
    Let \( V  \) and \( W  \) be vector spaces, and let \( T: V \to W  \) be linear and invertible. Then \( T^{-1}: W \to V  \) is linear.
\end{theorem}
\begin{proof}
Let \( {y}_{1}, {y}_{2} \in W  \) and \( c \in F  \). Since \( T  \) is surjective and injective, there exists unique vectors \( {x}_{1}  \) and \( {x}_{2}  \) such that \( T({x}_{1}) = {y}_{1}  \) and \( T({x}_{2}) = {y}_{2} \). Thus, we have \( {x}_{1} = T^{-1}({y}_{1})  \) and \( {x}_{2} = T^{-1}({y}_{2}) \). Since \( T  \) is linear, we must have that
\begin{align*}
    T^{-1}( c {y}_{1} + {y}_{2}) &= T^{-1}( c T({x}_{1}) + T({x}_{2})) \\
                                 &= T^{-1}( T(c {x}_{1}) + T({x}_{2})) \\
                                 &= T^{-1}(T({cx}_{1} + {x}_{2})) \\
                                 &= T^{-1}T({cx}_{1} + {x}_{2}) \\
                                 &= {I}_{V}({cx}_{1} + {x}_{2}) \\
                                 &= {cx}_{1} + {x}_{2} \\
                                 &= c T^{-1}({y}_{1}) + T^{-1}({y}_{2}).
\end{align*}
Hence, \( T^{-1} \) is a linear map.
\end{proof}

\begin{itemize}
    \item This tells us that the notions of invertibility, injectivity, and surjectivity in {\hyperref[Theorem 2.5]{Theorem 2.5}} are all equivalent. 
\end{itemize}

\subsection{Inverse of a Matrix}

\begin{definition}[Invertibility of Matrices]
   Let \( A  \) be an \( n \times n  \) matrix. Then \( A  \) is \textbf{invertible} if there exists an \( n \times n  \) matrix \( B  \) such that \( AB = BA = I  \).
\end{definition}

\begin{itemize}
    \item When \( A  \) is invertible, the matrix \( B  \) and its product \( AB  \) can be written in the following way:
        \[  AB = BA = I. \]
    \item If \( C  \) was another such matrix such that the above is true (replace \( B  \) with \( C  \)), then we can write 
        \[  C = CI = C(AB) = (CA)B = IB = B.   \]
    \item The matrix \( B  \), in this case, is called the \textbf{inverse} of \( A  \) and is denoted by \( A^{-1} \). 
\end{itemize}

\begin{lemma}\label{Invertibility Implies dim V = dim W}
   Let \( T \) be an invertible linear transformation from \( V  \) to \( W  \). Then \( V  \) is finite-dimensional if and only if \( W  \) is finite-dimensional. In this case, \( \text{dim}(V ) = \text{dim}(W) \). 
\end{lemma}
\begin{proof}
    Suppose that \( V  \) is finite-dimensional. Let \( \beta = \{  {x}_{1}, {x}_{2}, \dots, {x}_{n} \}  \) be a basis for \( V  \). By {\hyperref[Spanning set for R(T)]{Theorem 2.2}}, we get that \( \text{span}(T(\beta)) = R(T) \). Since \( T  \) is invertible, we know that \( T  \) is also surjective. Hence, \( \text{span}(T(\beta)) = R(T) = W  \) and so \( W  \) is finite-dimensional by {\hyperref[Theorem 1.9]{Theorem 1.9}}. Conversely, suppose that \( W  \) is finite-dimensional. So, let \( \gamma = \{ {y}_{1}, {y}_{2}, \dots, {y}_{n} \}   \) be a basis for \( W  \). Since \( T  \) is surjective, we know that for each \( {y}_{i} \) in \( \gamma \) that \( {y}_{i} = T({x}_{i}) \) for each \( i  \). Hence, we have that \( T^{-1}({y}_{i}) = {x}_{i} \) for all \( i \). By Theorem 2.2 again, we know that \( T^{-1}(\gamma) \) spans \( R(T^{-1}) \) where \( R(T^{-1}) = V  \) since \( T^{-1}  \) is surjective. Hence, \( V  \) is finite-dimensional.
    
    Now, suppose that both \( V  \) and \( W  \) are finite-dimensional. Since \( T  \) is injective and surjective, we must have that
    \begin{center}
        \( \text{nullity}(T) = 0  \) and \( \text{rank}(T) = \text{dim}(R(T)) = \text{dim}(W) \).
    \end{center}
    By the {\hyperref[Dimension Theorem]{Dimension Theorem}}, we have that \( \text{rank}(T) = \text{dim}(V) \) which implies that 
    \[  \text{dim}(V) = \text{dim}(W). \]
\end{proof}

\begin{theorem}[Invertibility of Linear Maps \( \iff \) Invertibility of Matrices]
   Let \( V  \) and \( W  \) be finite-dimensional vector spaces with ordered bases \( \beta \) and \( \gamma  \), respectively. Let \( T: V \to W  \) be linear. Then \( T  \) is invertible if and only if \( [T]_{\beta}^{\gamma}   \) is invertible. Furthermore, \( [T^{-1}]_{\gamma}^{\beta}  = ([T]_{\beta}^{\gamma} )^{-1} \).
\end{theorem}
\begin{proof}
Let \( V  \) and \( W  \) be vector spaces with ordered bases \( \beta  \) and \( \gamma \) respectively. Let \( T  \) be linear. Suppose \( T  \) is invertible. Then there exists a unique linear transformation \( T^{-1}: W \to V  \) by definition. Hence, observe that 
\[  [T^{-1}]_{\gamma}^{\beta}  [T]_{\beta}^{\gamma}  = [T^{-1}T ]_{\beta} = [{I}_{V}]_{\beta} = {I}_{n}  \]
and similarly
\[ [T]_{\beta}^{\gamma}  [T^{-1}]_{\gamma}^{\beta}  = [T T^{-1}]_{\beta} = [{I}_{V}]_{\beta} = {I}_{n}.        \]
Hence, \( [T]_{\beta}^{\gamma}  \) is invertible and that 
\[ [T^{-1}]_{\gamma}^{\beta}  = ([T]_{\beta}^{\gamma} )^{-1}.  \]

Conversely, suppose that \( [T]_{\beta}^{\gamma}  \) is invertible. Since \( V \) and \( W  \) are finite-dimensional vector spaces where  \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) and \( \gamma = \{ {w}_{1}, {w}_{2}, \dots, {w}_{n} \}  \) are ordered bases for \( V  \) and \( W  \), respectively. Hence, we know by Theorem 2.6 there exists a unique linear transformation \( U: W \to V  \) such that 
\[  U({w}_{j}) = \sum_{ i = 1   }^{ n } {B}_{ij} {v}_{i}    \]
where \( {v}_{i} \in \beta \). Since \( [T]_{\beta}^{\gamma} = A   \) is invertible, we know that 
\[  AB = BA = I_n.  \] We want to show that \( UT = {I}_{V}  \) and \( TU = {I}_{W} \). Define the matrix representation \( [U]_{\gamma}^{\beta}  \). We need to show that \(  U = T^{-1} \). Hence, we have
\[  [UT]_{\beta} = [U]_{\gamma}^{\beta} [T]_{\beta}^{\gamma} = BA = {I}_{n} = [{I}_{V}]_{\beta}      \]
and 
\[ [TU]_{\gamma} = [T]_{\beta}^{\gamma} [U]_{\gamma}^{\beta} = AB = {I}_{n} = [{I}_{W}]_{\gamma}      \]
by Theorem 2.11. Hence, we have \( UT = {I}_{V}  \) and \( TU = {I}_{W} \). Thus, \( T  \) is invertible.
\end{proof}

\begin{eg}
    Let \( \beta  \) and \( \gamma  \) be the standard ordered bases of \( {P}_{1}(\R)  \) and \( \R^{2} \), respectively. For \( T  \) as in {\hyperref[Eg 2.4.1]{Example 1}}, we have  
    \[  [T]_{\beta}^{\gamma} = \begin{pmatrix}
        1 & 0 \\
        1 & 1 
    \end{pmatrix}  \ \text{ and } \ [T^{-1}]_{\gamma}^{\beta} = \begin{pmatrix}
        1 & 0 \\
        -1 & 1 
    \end{pmatrix}.\]
    We can verify via matrix multiplication that both matrices are inverses of each other.
\end{eg}

\begin{corollary}
    Let \( V  \) be a finite-dimensional vector space with an ordered basis \( \beta \), and let \( T: V \to V  \) be linear. Then \( T  \) is invertible if and only if \( [T]_{\beta}   \) is invertible. Furthermore, \( [T^{-1}]_{\beta} = ([T]_{\beta})^{-1} \).
\end{corollary}
\begin{proof}
    Let \( T: V \to V  \) be linear and let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}   \) be an ordered basis for \( V  \). Suppose \( T  \) is invertible. Then there exists a unique linear transformation denoted by \( T^{-1}: V \to V  \) such that 
    \[
         TT^{-1} = T^{-1}T = {I}_{V}. 
    \]
    By using the Corollary to Theorem 2.11 and part (d) of Theorem 2.12, we must have that
    \[ [T]_{\beta} [T^{-1}]_{\beta} = [T T^{-1}]_{\beta} = [{I}_{V}]_{\beta} = {I}_{n}  \]
    and similarly,
    \[  [T^{-1}]_{\beta} [T]_{\beta} = [T^{-1} T]_{ \beta} = [{I}_{V}]_{\beta} = {I}_{n}. \]
    This tells us that \( [T]_{\beta}  \) is invertible and that 
    \[  [T^{-1}]_{\beta} = ([T]_{\beta})^{-1}. \]

   For the backwards direction, the ordered basis \( \beta \) defined earlier implies that there exists a unique linear transformation \( U: V \to V  \) defined by  
   \[  U({v}_{j}) = \sum_{ i=1  }^{ n } {A}_{ij} {v}_{i} \ \text{for} \ 1 \leq j \leq n. \]
   We need to show that \( U = T^{-1} \). Using the fact that \( [T]_{\beta} \) is invertible, we can write
   \[  [T U]_{\beta} = [T]_{\beta} [U]_{\beta} = {I}_{n} = [{I}_{V}]_{\beta}  \]
   and similarly
   \[  [UT]_{\beta} = [U]_{\beta} [T]_{\beta} = {I}_{n} = [{I}_{V}]_{\beta}.  \]
   But this tells us that \( UT = TU  = {I}_{V} \). So, \( U = T^{-1}  \) and that \( T \) is invertible.
\end{proof}

\begin{corollary}
    Let \( A  \) be an \( n \times n  \) matrix. Then \( A  \) is invertible if and only if \( {L}_{A} \) is invertible. Furthermore, \( ({L}_{A})^{-1} = {L}_{A^{-1}} \).
\end{corollary}

\begin{proof}
Let \( {L}_{A}: F^{n} \to F^{n} \). Let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) be a basis for \( F^{n} \). By part (a) of {\hyperref[Prop of LMT]{Theorem 2.15}}, we have that \( [{L}_{A}]_{\beta} = A   \). Since \( A \) is invertible, we know that \( {L}_{A} \) must also be invertible by Corollary to Theorem 2.18. Furthermore, we have that \( {L}_{A} {L}_{A^{-1}} = I   \) implies that 
\[  {L}_{A^{-1}} = ({L}_{A})^{-1}.  \]

Conversely, \( {L}_{A} \) invertible implies that \( [{L}_{A}]_{\beta}  \) is invertible by Corollary to Theorem 2.18. By Theorem 2.15, we must have that \( [L_{A}]_{\beta}  = A  \). But this means that \( A  \) is invertible.
\end{proof}

\subsection{Isomorphisms}

\begin{definition}[Isomorphisms]
   Let \( V  \) and \( W  \) be vector spaces. We say that \( V  \) is \textbf{isomorphic} to \( W  \) if there exists a linear transformation \( T: V \to W  \) that is invertible. Such a linear transformation is called an \textbf{isomorphism} from \( V  \) to \( W  \). 
\end{definition}

\begin{theorem}[Isomorphisms \( \iff \) Equal Finite-Dimensions]
   Let \( V  \) and \( W  \) be finite-dimensional vector spaces (over the same field). Then \( V  \) is isomorphic to \( W  \) if and only if \( \text{dim}(V) = \text{dim}(W) \).  
\end{theorem}
\begin{proof}
    Suppose that \( V  \) is isomorphic to \( W  \) and \( T: V \to W  \) is an isomorphism from \( V  \) to \( W  \). By the {\hyperref[Invertibility Implies dim V = dim W]{lemma}} preceding Theorem 2.18, we have that \( \text{dim}(V) = \text{dim}(W) \).

    Now, suppose that \( \text{dim}(V) = \text{dim}(W) \), and let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \} \) and \( \gamma = \{ {w}_{1}, {w}_{2}, \dots, {w}_{n} \}  \). By Theorem 2.6, there exists a unique linear transformation \( T: V \to W  \) such that \( T({v}_{i}) = {w}_{i} \) for all \( i \). Then by Theorem 2.2, we must have that 
    \[  R(T) = \text{span}(T(\beta)) = \text{span}(\gamma) = W. \]
    Hence, \( T  \) is onto. By Theorem 2.5, we must also have that \( T  \) is one-to-one. Hence, \( T  \) is an isomorphism.
\end{proof}

\begin{remark}
    Using the lemma to Theorem 2.18, we find that \( V  \) and \( W  \) are finite-dimensional or infinite-dimensional when they are isomorphic to each other.  
\end{remark}

\begin{corollary}
    Let \( V  \) be a vector space over \( F  \). Then \( V  \) is isomorphic to \( F^{n}  \) if and only if \( \text{dim}(V) = n  \).
\end{corollary}

We are now in the position to show that the collection of all linear transformations between two vector spaces, say \( V  \) and \( W  \) with different dimensions \( m  \) and \( n \), can be identified with the appropriate vector space of \( m \times n  \) matrices.

\begin{theorem}[Isomorphism Between Linear Transformations and Matrices]
    Let \( V  \) and \( W  \) be finite-dimensional vector spaces \( F  \) of dimensions \( n  \) and \( m  \), respectively, and let \( \beta  \) and \( \gamma  \) be ordered bases for \( V  \) and \( W  \) respectively. Then the function \( \Phi: \mathcal{L}(V,W) \to {M}_{m \times n}(F)   \), defined by  
    \begin{center}
        \( \Phi(T) = [T]_{\beta}^{\gamma}  \) for \( T \in \mathcal{L}(V,W) \)
    \end{center}
    is an isomorphism.
\end{theorem}
\begin{proof}
First, we show that \( \Phi  \) is linear. Using Theorem 2.8, we find that 
\begin{align*}
    \Phi (aT+U) &= [aT + U]_{\beta}^{\gamma}  \\
                &= a [T]_{\beta}^{\gamma}  + [U]_{\beta}^{\gamma} \\
                &= a \Phi(T) + \Phi(U).
\end{align*}
Hence, \( \Phi  \) is linear.

In order to show that \( \Phi  \) is isomorphic, it suffices to show that \( \Phi  \) is both injective and surjective. Clearly, \( \Phi  \) is injective since for every \( T,U \in \mathcal{L}(V,W) \), we have \( \Phi(T) = \Phi(U)  \) implies \( [T]_{\beta}^{\gamma}  = [U]_{\beta}^{\gamma} \implies T = U   \). For surjectivity, let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) and \( \gamma = \{ {w}_{1}, {w}_{2}, \dots, {w}_{m} \}  \) be ordered bases for \( V  \) and \( W \) respectively, and let \( A  \) be a given \( m \times n  \) matrix. By Theorem 2.6, there exists a unique linear transformation \( T: V \to W  \) such that 
\[  T({v}_{j}) = \sum_{ i=1 }^{ m  }A_{ij} {w}_{i}, \ \text{for} \ 1 \leq j \leq n. \]
But then we have  \( [T]_{\beta}^{\gamma} = A  \) or \( \Phi(T) = A  \). Hence, \( \Phi  \) is an isomorphism. 
\end{proof}

\begin{corollary}
    Let \( V  \) and \( W  \) be finite-dimensional vector spaces of dimensions \( n  \) and \( m  \), respectively. Then \( \mathcal{L}(V,W) \) is finite-dimensional of dimension \( mn  \).
\end{corollary} 
\begin{proof}
    By Theorem 2.20, \( \Phi: \mathcal{L}(V,W) \to {M}_{m \times n}(F) \) is an isomorphism. Then we have \( \mathcal{L}(V,W) \) finite-dimensional with  \( \text{dim}(\mathcal{L}(V,W)) = \text{dim}({M}_{m \times n }(F)) = mn  \) by Theorem 2.19.
\end{proof}

\subsection{Standard Representation of Vector Spaces}

\begin{definition}[Standard Representation of \( V  \)]
    Let \( \beta \) be an ordered basis for an \( n- \)dimensional vector space \( V  \) over the field \( F \). The \textbf{standard representation of \( V \) with respect to} \( \beta  \) is the function \( {\phi}_{\beta}: V \to F^{n} \) defined by \( {\phi}_{\beta}(x) = [x]_{\beta} \) for each \( x \in V  \) .    
\end{definition}

\begin{eg}
    Let \( \beta = \{ (1,0), (0,1) \}  \) and \( \gamma = \{ (1,2), (3,4) \}  \). It is easily observed that \( \beta  \) and \( \gamma  \) are ordered bases for \( \R^{2} \). For \( x = (1,-2) \), we have
    \[  {\phi}_{\beta}(x) = [x]_{\beta} = \begin{pmatrix}
        1 \\
        -2
        \end{pmatrix} \ \text{and} \ {\phi}_{\gamma}(x) = [x]_{\gamma} = \begin{pmatrix}
        -5 \\
        2 
    \end{pmatrix}.  \]
\end{eg}

In the last two sections, we showed that \( {\phi}_{\beta} \) is a linear transformation. 

\begin{theorem}
    For any finite-dimensional vector space \( V  \) with ordered basis \( \beta  \), \( {\phi}_{\beta}  \) is an isomorphism.
\end{theorem}
\begin{proof}
    Let \( \text{dim}(V) = n  \). Let \( \beta = \{ {v}_{1}, { v }_{2}, \dots, {v}_{n} \}   \) be an ordered basis for \( V  \). We can show that \( {\phi}_{\beta}(y)  \) is an isomorphism by showing that \( {\phi}_{\beta} \) is a surjective and injective linear map. Suppose \( {\phi}_{\beta}(x) = {\phi}_{\beta} \). Then by definition of \( {\phi}_{\beta} \), we must have that \( [x]_{\beta} = [y]_{\beta} \) which further implies that \( x = y  \). Now, let \( y \in V  \). Since \( \beta \) is an ordered basis for \( V  \), we can find scalars \( {\delta}_{1}, {\delta}_{2}, \dots, {\delta}_{n} \) such that 
    \[  y = \sum_{ i=1  }^{ n } {\delta}_{i} {v}_{i}. \]
    This implies that we have constructed a coordinate vector such that \( [y]_{\beta} = {\phi}_{\beta}(y)\). Hence, \( {\phi}_{\beta} \) is surjective. Thus, we find that \( {\phi}_{\beta} \) is an isomorphism.
\end{proof}


\begin{itemize}
    \item Given finite-dimensional vector spaces \( V  \) and \( W  \) with corresponding bases \( \beta \) and \( \gamma \) respectively, we can use \( {\phi}_{\beta} \) and \( {\phi}_{\gamma} \) to map \( V  \) to \( F^{n}  \) and \( W \) to \( F^{m} \) respectively by the Theorem above.
    \item Let \( T: V \to W  \) be a linear transformation. We can map \( V  \) into \( W  \) using \( T  \) and use \( {\phi}_{\gamma} \) to map \( W  \) onto \( F^{m} \).
    \item Likewise, let \( A = [T]_{\beta}^{\gamma}  \). Then mapping \( V  \) onto \( F^{n} \) using \( {\phi}_{\beta} \) allows us to map the result onto \( F^{m} \) using \( {L}_{A}: F^{n} \to F^{m} \) which produces the same vector in \( F^{m} \) as the transformation above.
    \item We can conclude, using {\hyperref[Theorem 2.14]{Theorem 2.14}}, that 
        \[  {L}_{A} {\phi}_{\beta} = {\phi}_{\gamma}T. \]
    \item Since \( V \) and \( W  \) are associated with \( F^{n} \) and \( F^{m} \) respectively, we can now 'identify' \( T \) with \( {L}_{A} \).

\end{itemize}

\begin{eg}
    Suppose we have the linear transformation \( T: {P}_{3}(\R) \to {P}_{2}(\R) \) defined by
    \[  T(f(x)) = f'(x). \]
    Let \( \beta \) and \( \gamma \) be the standard ordered bases for \( {P}_{3}(\R) \) and \( {P}_{2}(\R) \), respectively. Let \( {\phi}_{\beta} : {P}_{3}(\R) \to \R^{4} \) and \( {\phi}_{\gamma}: {P}_{2}(\R) \to R^{3} \) be the corresponding standard representations of \( {P}_{3}(\R) \) and \( {P}_{2}(\R) \). If \( A = [T]_{\beta}^{\gamma}  \), then
    \[  A = \begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 3
    \end{pmatrix}. \]
    Consider the polynomial \( p(x) = 2 + x - 3x^{2} + 5x^{3}. \) We will show that 
    \[  {L}_{A}{\phi}_{\beta}(p(x)) = {\phi}_{\gamma}T(p(x)). \]
    Using standard matrix operations, we get
    \[  {L}_{A}{\phi}_{\beta}(p(x)) = \begin{pmatrix}
        0 & 1 & 0 & 0 \\
        0 & 0 & 2 & 0 \\
        0 & 0 & 0 & 3
    \end{pmatrix} \begin{pmatrix}
        2 \\
        1 \\
        -3 \\
        5
    \end{pmatrix} = \begin{pmatrix}
        1 \\
        -6 \\
        15
    \end{pmatrix}. \]
    But notice that by the same operations, we have
    \[  {\phi}_{\gamma}T(p(x)) = \begin{pmatrix}
        1 \\
        -6 \\
        15
    \end{pmatrix}. \]
    So \( {L}_{A}{\phi}_{\beta}(p(x)) = {\phi}_{\gamma}T(p(x)). \)
\end{eg}
