\section{Unitary and Orthogonal Operators and Their Matrices}

\subsection{Unitary and Orthogonal Operators}

\begin{itemize}
    \item In the past chapters, we were concerned with how operations and structures are preserved with functions preserving the operations concerned with vector spaces and isomorphisms preserving the structure of the space itself. 
    \item In this section, we will develop the notion of what it means to preserve "length"; that is, the norm of some vector space.
\end{itemize}

\begin{definition}[Unitary Operators and Orthogonal Operators]
    Let \( T  \) be a linear operator on a finite-dimensional inner product space \( V  \) (over \( F  \)). If \( \|T(x)\| = \|x\| \) for all \( x \in V  \), we call \( T  \) a \textbf{unitary operator} if \( F = \C  \) and an \textbf{othogonal operator} if \( F = \R  \).
\end{definition}

\begin{remark}
    Under infinite-dimensional vector spaces, an operator that preserves the norm is injective, but not surjective. If it is also surjective, then we call that operator either a \textbf{unitary} or \textbf{orthogonal operator}. 
\end{remark}

\begin{lemma}
    Let \( U  \) be a self-adjoint operator on an inner product space \( V  \), and suppose that \( \langle x  , U(x) \rangle = 0  \) for all \( x \in V  \). Then \( U = {T}_{0} \).
\end{lemma}
\begin{proof}
For any \( x \in V  \), we have
\begin{align*}
    0 &= \langle x + U(x) , U(x + U(x)) \rangle \\
      &= \langle x + U(x)  , U(x) + U^{2}(x) \rangle \\
      &= \langle  x  , U(x) \rangle + \langle x  , U^{2}(x) \rangle + \langle U(x) , U(x) \rangle + \langle U(x) , U^{2}(x) \rangle \\
      &= 0 + \langle x  , U^{2}(x) \rangle + \langle U(x) , U(x) \rangle + 0 \\
      &= \langle x  , U^{*}U(x) \rangle + \|U(x)\|^{2} \\
      &= \langle U(x) , u(x) \rangle + \|U(x)\|^{2} \\
      &= 2\|U(x)\|^{2}. 
\end{align*}
So, for any \( x \in V  \), we have \( \|U(x)\| = 0  \) which implies that \( U = {T}_{0} \).
\end{proof}

\begin{theorem}
    Let \( T  \) be a linear operator on a finite-dimensional inner product space \( V  \). Then the following statements are equivalent.
    \begin{enumerate}
        \item[(a)] \( T^{*} T = I  \).
        \item[(b)] \( T T^{*} = I  \).
        \item[(c)] \( \langle T(x) , T(y) \rangle = \langle x , y \rangle  \) for all \( x , y \in V  \).
        \item[(d)] If \( \beta  \) is an orthonormal basis for \( V  \), then \( T(\beta) \) is an orthonormal basis for \( V  \).
        \item[(e)] There exists an orthonormal basis \( \beta  \) for \( V  \) such that \( T(\beta) \) is an orthonormal basis for \( V  \).
        \item[(f)] \( \|T(x)\| = \|x\|  \) for all \( x \in V  \).
    \end{enumerate}
\end{theorem} 
\begin{proof}
    \begin{itemize}
        \item We will show that part (a) implies part (b). Suppose \( T^{*}T = I  \). By Theorem 6.10, we have  
            \[ {I}_{n} = [T^{*}T]_{\beta} = [T^{*}]_{\beta} [T]_{\beta} = [T]_{\beta}^{*} [T]_{\beta}.  \]
            Let \( A = [T]_{\beta}^{*}  \) and \( B = [T]_{\beta} \). Since \( A  \) and \( B  \) are invertible and that \( A = B^{-1} \) and \( B = A^{-1} \), we must have
            \[ {I}_{n} = BA = [T]_{\beta}[T]_{\beta}^{*} = [T]_{\beta} [T^{*}]_{\beta} = [T T^{*}]_{\beta}.   \]
            Thus, we have \( TT^{*} = I \).
        \item Next, we wills show that part (b) implies part (c). Let \(x,y \in V  \). Using part (b), we find that
            \[  \langle x , y \rangle = \langle T^{*}T(x) , y  \rangle = \langle T(x) , T(y) \rangle \]
            which is our desired result.
        \item Next, let us show that part (c) implies part (d). Let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) be an orthonormal basis for \( V  \). First, we will show that \( T(\beta) \) is an orthonormal subset of \( V  \). Using part (c), we find that 
            \[  \langle T({v}_{i}) , T({v}_{j}) \rangle = \langle {v}_{i} ,  {v}_{j} \rangle = {\delta}_{ij} \]
            for all \( i  \) and \( j  \). Thus, \( T(\beta)  \) is orthonormal.By the 2nd Corollary to Theorem 6.3, \( T(\beta) \) must also be linearly independent. Clearly, \( T(\beta) \subseteq V  \). Let \( v \in V  \) that is not in \( T(\beta) \). Then by Theorem 1.7, adjoining \( v \in V \) to \( \beta \) generates a linearly dependent set which implies \( v \in \text{span}(T(\beta)) \). So, \( T(\beta) \) spans \( V  \).
        \item Clearly, (d) implies (e).
        \item Next, we prove that (e) implies (f). Let \( x \in V  \), and let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \). Thus, 
            \[  x = \sum_{ i=1  }^{ n } {a}_{i}{v}_{i} \]
            where \( {a}_{i} = \langle x , {v}_{i} \rangle \). Then observe that 
            \begin{align*}
                \|x\|^{2} = \Big\| \sum_{ i=1  }^{ n } {a}_{i} {v}_{i} \Big\|^{2}
                          &= \Big\langle \sum_{ i=1  }^{ n } {a}_{i} {v}_{i} ,  \sum_{ j=1  }^{ n } {a}_{j} {v}_{j} \Big\rangle \\ 
                          &= \sum_{ i=1  }^{ n } {a}_{i} \sum_{ j=1 }^{ n } \overline{{a}_{j}} \langle {v}_{i}  , {v}_{j} \rangle \\
                          &= \sum_{ i=1  }^{ n } {a}_{i} \sum_{ j=1  }^{ n } \overline{{a}_{j}} \langle T({v}_{i}) , T({v}_{j}) \rangle \\
                          &= \Big\langle \sum_{ i=1  }^{ n } {a}_{i} T({v}_{i}),  \sum_{ j=1 }^{  n } {a}_{j} T({v}_{j}) \Big\rangle \\
                          &= \Big\langle T \Big(  \sum_{ i=1  }^{ n } {a}_{i} {v}_{i} \Big), T \Big(  \sum_{ j=1  }^{ n } {a}_{j} {v}_{j} \Big)  \Big\rangle \\
                          &= \langle T(x) , T(x) \rangle \\
                          &= \|T(x)\|^{2}.
            \end{align*}
            Therefore, we have \( \|x\| = \|T(x)\| \).
        \item Finally, let us show that part (f) implies part (a). Let \( x \in V  \). Then we have  
            \[  \langle x  ,  x  \rangle = \|x\|^{2} = \|T(x)\|^{2} = \langle T(x) , T(x) \rangle = \langle x  , T^{*}T(x) \rangle. \]
            So, we have
            \[  \langle x  , (T^{*}T - I) (x) \rangle = 0  \]
            and thus \( T^{*}T - I = {T}_{0} \) by lemma which subsequently implies that \( T^{*}T = I  \).
    \end{itemize}
\end{proof}
Note that the conditions above are all equivalent to the definition of unitary or orthogonal operators.

\begin{corollary}
    Let \( T  \) be a linear operator on a finite-dimensional real inner product space \( V  \). Then \( V  \) has an orthonormal basis of eigenvectors of \( T  \) with corresponding eigenvalues of absolute value \( 1  \) if and only if \( T  \) is both self-adjoint and orthogonal.
\end{corollary}
\begin{proof}
Suppose \( V  \) has an orthonormal basis \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) consisting of eigenvectors corresponding to eigenvalues \( | {\lambda}_{i} | = 1    \) for each \( i  \). We need to show that \( T  \) is self-adjoint and orthogonal. By {\hyperref[Theorem 6.17]{Theorem 6.17}}, \( T  \) is self-adjoint. Thus, \( T  \) is a diagonalizable linear operator; that is, \( T({v}_{i}) = {\lambda}_{i} {v}_{i} \) for all \( i  \). Since \( | {\lambda}_{i} |  = 1  \) for all \( i  \), we must have
\[  T^{*}T({v}_{i}) = T(T({v}_{i})) = T({\lambda}_{i} {v}_{i} ) = {\lambda}_{i} T({v}_{i}) = {\lambda}_{i}^{2} {v}_{i} = {v}_{i}.   \]
So, \( T^{*}T = I  \) and thus \( T  \) is orthogonal. Conversely, suppose \( T  \) is self-adjoint and normal. Then \( V  \) contains an orthonormal basis \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}    \) with eigenvectors corresponding to the eigenvalues \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{n} \), by Theorem 6.17. We need to show that \( | {\lambda}_{i} |  =  1  \) for all \( i \). Since \( T  \) is normal, we must have
\begin{align*}
    | {\lambda}_{i} |^{2} \langle {v}_{i}  , {v}_{i} \rangle = | {\lambda}_{i} |^{2} \|{v}_{i}\|^{2} &= \|{\lambda}_{i} {v}_{i}\|^{2}  \\
                                                                                                 &= \|T({v}_{i})\|^{2} \\
                                                                                                 &= \langle {v}_{i}  ,  T^{*}T({v}_{i}) \rangle \\
                                                                                                 &= \langle {v}_{i}  , {v}_{i} \rangle.
\end{align*}
Since each \( {v}_{i} \neq 0  \), \( \langle {v}_{i} ,  {v}_{i} \rangle > 0  \) and so
\[  | {\lambda}_{i} |^{2} \|{v}_{i}\|^{2} = \|{v}_{i}\|^{2} \implies | {\lambda}_{i} | = 1  \  \text{for all} \  i \]
which is our desired result.
\end{proof}

\begin{corollary}
    Let \( T  \) be a linear operator on a finite-dimensional complex inner product space \( V  \). Then \( V  \) has an orthonormal basis of eigenvectors of \( T  \) with corresponding eigenvalues of absolute value \( 1  \) if and only if \( T  \) is both self-adjoint and unitary.
\end{corollary}

\begin{proof}
    The proof is similar to the proof of corollary 1.
\end{proof}

\begin{definition}[Reflection About \( \R^2 \)]
    Let \( L  \) be a one-dimensional subspace of \( \R^{2} \). We may view \( L  \) as a line in the plane through the origin. A linear operator \( T  \) on \( \R^{2} \) is called a \textbf{reflection of \( \R^{2} \) about \( L  \)} if \( T(x) = x  \) for all \( x \in L  \) and \( T(x) = -x  \) for all \( x \in L^{\perp} \).
\end{definition}

\begin{definition}[Orthogonal and Unitary Matrices]
    A square matrix \( A  \) is called an \textbf{orthogonal matrix} if \( A^{t} A = A A^{t} = I  \) and \textbf{unitary} if \( A^{*} A = A A^{*} = I  \).
\end{definition}

\begin{itemize}
    \item If given a real matrix \( A  \), we have \( A^{*} = A^{t} \) where a real unitary matrix is also orthogonal in which case we just call \( A  \) orthogonal.
    \item Note that \( A A^{*} = I  \) is equivalent to the statement that the rows of \( A  \) form an orthonormal basis for \( F^{n} \) since
        \[  {\delta}_{ij} = {I}_{ij} = (AA^{*})_{ij} = \sum_{ k=1  }^{ n } {A}_{ik } {(A^{*})}_{kj} = \sum_{ k=1  }^{ n } {A}_{ik} \overline{{A}_{jk}}. \]
    \item The last term above represents the inner product of the \( i \)th and \( j \)th rows of \( A  \).
\end{itemize}

\begin{prop}
    Suppose \( V  \) is a finite-dimensional inner product space and \( T  \) is a linear operator on \( V  \). Then \( T  \) is unitary [orthogonal] if and only if \( [T]_{\beta} \) is unitary [orthogonal]. 
\end{prop}
\begin{proof}
    Follows from the definition of unitary/orthogonal matrices and {\hyperref[Theorem 6.10]{Theorem 6.10}}.
\end{proof}

\begin{itemize}
    \item For complex normal [real symmetric] matrix \( A  \), there exists an orthonormal basis \( \beta  \) for \( F^{n} \) consisting of eigenvectors of \( A  \). Thus, \( A  \) must be similar to a diagonal matrix \( D  \).
    \item The invertible matrix \( Q  \) is made up of basis vectors from \( \beta  \) such that \( D = Q^{-1} A Q  \). But this tells us that \( Q  \) is unitary since the columns of \( Q  \) are an orthonormal basis for \( F^{n} \).
\end{itemize}

\begin{definition}[Unitary/Orthogonal Equivalence]
    Let \( A  \) be an \( n \times n  \) matrix with [real/complex] entries. We call \( A  \) \textbf{unitarily equivalent} [\textbf{orthogonally equivalent}] to \( B  \) if there exists a unitary [orthogonal] matrix \( P  \) such that \( A = P^{*}BP \).
\end{definition}

\begin{theorem}\label{Theorem 6.19}
   Let \( A  \) be a complex \( n \times n  \) matrix. Then \( A  \) is normal if and only if \( A  \) is unitarily equivalent to a diagonal matrix. 
\end{theorem}
\begin{proof} Note that the preceding paragraph already proved the forwards direction, so we will only have to show the backwards direction. Suppose \( A  \) is unitarily equivalent to a diagonal matrix, and call this matrix \( D  \). Then there exists a unitary matrix \( P  \) such that \( A = P^{*} D P  \). We need to show that \( A  \) is normal; that is, \( A^{*}A = A A^{*} \). Note that diagonal matrices are also normal, so \( D^{*}D = DD^{*} \). Thus, we have 
    \begin{align*}
        A^{*}A &= (P^{*}(DP))^{*} (P^{*} D P) \\
               &= ((DP)^{*} P) (P^{*} D P) \\
               &= (P^{*}D^{*}P)(P^{*} D P) \\
               &=  P^{*}D^{*}DP \tag{\( D^{*}D = D D^{*} \)} \\
               &= P^{*}D D^{*} P \\
               &= P^{*} D D^{*}P \\
               &= P^{*} D P P^{*} D^{*} P \\
               &= (P^{*} D P) ((DP)^{*} P) \\
               &= (P^{*} D P) (P^{*} D P) \\
               &= A A^{*}. 
    \end{align*}
    Thus, \( A  \) is normal.

\end{proof}

\begin{theorem}
   Let \( A  \) be a real \(  n \times n  \) matrix. Then \( A  \) is symmetric if and only if \( A  \) is orthogonally equivalent to a real diagonal matrix. 
\end{theorem}
\begin{proof}
The proof is similar to the proof to the theorem above. Just replace the adjoints with transposes.
\end{proof}

Using Schur's Theorem, the next result immediately follows. In fact, the theorem below is just the matrix form of {\hyperref[Schur]{Schur's Theorem}}.

\begin{theorem}[Schur]
  Let \( A \in {M}_{ n \times n }(F) \) be a matrix whose characteristic polynomial splits over \( F  \).    
  \begin{enumerate}
      \item[(a)] If \( F = C  \), then \( A  \) is unitarily equivalent to a complex upper triangular matrix.
        \item[(b)] If \( F = \R  \), then \( A  \) is orthogonally equivalent to a real upper triangular matrix.
  \end{enumerate}
\end{theorem}

\subsection{Rigid Motions}

One application of unitary and orthogonal operators is the notion of \textit{rigid motions} and how, in a finite-dimensional inner product space, transforming a figure does alter its shape. A key requirement is the transformation to preserve distances.

\begin{definition}[Rigid Motion]
    Let \( V  \) be a real inner product space. A function \( f: V \to V  \) is called a \textbf{rigid motion} if 
    \[ \|f(x) - f(y)\| = \| x-  y\|  \]
    for all \( x,y \in V  \).
\end{definition}

\begin{itemize}
    \item Any orthogonal operator is a rigid motion.
    \item Translations are another class of rigid motions.
\end{itemize}

\begin{definition}[Translations]
    A function \( g: V \to V  \), where \( V  \) is a real inner product space, is called a \textbf{translation} if there exists a vector \( {v}_{0} \in V  \) such that \( g(x) = x + {v}_{0} \) for all \( x \in V  \). We call that \( g  \) is a \textit{translation by \( {v}_{0} \)}. 
\end{definition}

    On a real finite-dimensional inner product space, an orthogonal operator followed by a translation is also a rigid motion.

\begin{theorem}
    Let \( f: V \to V  \) be a rigid motion on a finite-dimensional real inner product space \( V  \). Then there exists a unique orthogonal operator \( T  \) on \( V  \) and a unique translation \( g  \) on \( V  \) such that \( f  = g \circ T  \).
\end{theorem} 

\begin{itemize}
    \item An orthogonal operator is a translation by the zero vector \( O \).
    \item Any translation is characterized as the identity operator.
\end{itemize}

\begin{proof}
Let \( T: V \to V  \) be defined by 
\[  T(x) = f(x) - f(0) \]
for all \( x \in V  \).
\end{proof}
