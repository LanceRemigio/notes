\section{Diagonalization}

\subsection*{Exercise 5.2.5} State and prove the matrix version of Theorem 5.6.
\begin{proof}
Let \( A \in {M}_{n \times n}(F) \) and let \( f(t) = \text{det}(A - tI) \) is the characteristic polynomial of \( A \). Suppose \( {L}_{A} \) is diagonalizable. The proof is the same except \( T  \) is replaced with \( {L}_{A} \) instead. Thus, the characteristic polynomial \( f(t)  \) of \( A  \) splits over \( F  \).
\end{proof}

\subsection*{Exercise 5.2.8} Suppose that \( A \in {M}_{n \times n }(F) \) has two distinct eigenvalues \( {\lambda}_{1} \) and \( {\lambda}_{2} \) and that \( \text{dim}({E}_{{\lambda}_{1}}) = n - 1  \). Prove that \( A  \) is diagonalizable.
\begin{proof}
    Let \( A \in {M}_{n \times n}(F) \) and \( {\lambda}_{1} \) and \( {\lambda}_{2} \) be two distinct eigenvalues where \( {E}_{{\lambda}_{1}} \) and \( {E}_{{\lambda}_{2}} \) are the eigenspaces associated with these eigenvalues (related to the linear operator \( {L}_{A} \)). Let \( \beta_{1} \) be a basis for \( {E}_{\lambda_{1}} \). Since \( \text{dim}({E}_{\lambda_{1}}) = n - 1  \), we have
    \[  {\beta}_{1} = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n-1} \}.   \]
    Note that the dimensions of \( {E}_{{\lambda}_{1}} \) and \( {E}_{{\lambda}_{2}}  \) must sum to \( n \). Thus, we must have \( \text{dim}({E}_{{\lambda}_{2}}) = 1  \) and let \( {\beta}_{2} = \{ v'  \}  \) be the basis for \( {E}_{{\lambda}_{2}} \) where \( v' \neq 0  \). So, extend \( {\beta}_{1} \) to a basis \( \beta  \) for \( V  \) taking the union of \(  {\beta}_{2}   \) with this basis. Since \( \beta = {\beta}_{1} \cup {\beta}_{2} \) is a set that contains exactly \( n  \) linearly independent vectors that are eigenvectors of \( {L}_{A} \) corresponding to eigenvalues \( {\lambda}_{1} \) and \( {\lambda}_{2} \), we have that \( {L}_{A} \) must be diagonalizable by part (b) of Theorem 5.9. Thus, \( A  \) must be diagonalizable.
\end{proof}

\subsection*{Exercise 5.2.9} Let \( T \) be a linear operator on a finite-dimensional vector space \( V  \), and suppose there exists an ordered basis for \( V  \) such that \( [T]_{\beta} \) is an upper triangular matrix.  
\begin{enumerate}
    \item[(a)] Prove that the characteristic polynomial for \( T  \) splits.
        \begin{proof}
        Let 
        \[  f(t) = \text{det}([T]_{\beta} - tI) \]
        be the characteristic polynomial of \( [T]_{\beta}  \). By Exercise 23 of Section 4.2, we can see that the determinant of this upper triangular matrix is just the product of its diagonal entries. Thus, we have 
        \[  f(t) = (-1)^{n} ({[T]_{\beta}}_{11} - t)({[T]_{\beta}}_{22} - t) \cdots ({[T]_{\beta}}_{nn}  - t). \]
        Hence, \( f(t)  \) splits.
        \end{proof}
    \item[(b)] State and prove an analogous result for matrices.
        \begin{proof}
        Substitute \( {L}_{A} \) in place of \( T  \) and apply part (a).
        \end{proof}
\end{enumerate}

\subsection*{Exercise 5.2.10} Let \( T  \) be a linear operator on a finite-dimensional vector space \( V  \) with the distinct eigenvalues \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) and corresponding multiplicities \( {m}_{1}, {m}_{2}, \dots, {m}_{k}  \). Suppose that \( \beta \) is a basis for \( V  \) such that \( [T]_{\beta} \) is an upper triangular matrix. Prove that the diagonal entries of \( [T]_{\beta} \) are \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) and that each \( {\lambda}_{i} \) occurs \( {m}_{i} \) times (\( 1 \leq i \leq k  \)).
\begin{proof}
Let \( [T]_{\beta} = A  \) and \(  \text{dim}(V) = n \). Since \( A  \) is an upper triangular matrix, we know that the characteristic polynomial of \( A  \) splits by Exercise 5.2.10. Thus, we have
\begin{align*}
    f(t) &= \text{det}(A - tI) \\
         &= (-1)^{n} ({A}_{11} - t)({A}_{22} - t) \cdots ({A}_{nn} -t).
\end{align*}
By Theorem 5.3, \( f(t) \) contains at most \( n   \) distinct eigenvalues. So, we must have \( f(t) = 0  \) if and only if \( \lambda = {A}_{11}, {A}_{22}, \dots, {A}_{nn} \). But this means that each \( {A}_{ii} \) for \( 1 \leq i \leq n  \) is an eigenvalue of \( A  \) by Theorem 5.2. Thus, the eigenvalues of \(T\) must occur on the diagonals of \( A  \). But we only have distinct eigenvalues \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k }  \) of \( T  \) with corresponding multiplicities \( {m}_{1}, {m}_{2}, \dots, {m}_{k } \) where \( k \leq n  \). So, each \( {m}_{i} \) corresponding to each \( {\lambda}_{i} \) for \( 1 \leq i \leq k  \) must sum up to \( n  \) since \( f(t) \) is an \( n \)th degree polynomial and there must be \( n  \) solutions to \( f(t) = 0  \). Thus, each \( {\lambda}_{i} \) must occur \( {m}_{i} \) times along the diagonal of \( A  \). 
\end{proof}

\subsection*{Exercise 5.2.11} Let \( A  \) be an \( n \times n  \) matrix that is similar to an upper triangular matrix and has the distinct eigenvalues \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) with corresponding multiplicities \( {m}_{1}, {m}_{2}, \dots, {m}_{k } \). Prove the following statements. 
\begin{enumerate}
    \item[(a)] \( \text{tr}(A) = \sum_{ i=1  }^{ k  } {m}_{i} {\lambda}_{i} \).
        \begin{proof}
            Let \( A \in {M}_{n \times n}(F) \). Suppose \( A  \) is similar to some upper triangular matrix \( B  \). Let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k } \) be the eigenvalues with corresponding multiplicities \( {m}_{1}, {m}_{2}, \dots, {m}_{k } \) of \( B  \). Since these eigenvalues must occur on the diagonal of \( A  \) (by Exercise 5.2.10) where each \( {\lambda}_{i} \) occurs \( {m}_{i}  \) times along with \( \text{tr}(A) = \text{tr}(B) \) (since \( A  \) is similar to \( B \)), we can take the sum of our diagonal entries to get
            \[  \text{tr}(A) = \text{tr}(B) =  \sum_{ i=1 }^{ k   } {m}_{i} {\lambda}_{i}. \]
        \end{proof}
    \item[(b)] \( \text{det}(A) = ({\lambda}_{1})^{{m}_{1}} ({\lambda}_{2})^{{m}_{2}} \cdots ({\lambda}_{k})^{{m}_{k}} \).
        \begin{proof}
        Since \( A  \) is similar to \( B  \), we know that \( \text{det}(A) = \text{det}(B) \) by {\hyperref[Exercise 2.5.10]{Exercise 2.5.10}}. Since \( B  \) is upper triangular, we know that by {\hyperref[Exercise 4.2.23]{Exercise 4.2.23}} that the determinant of \( B  \) is just the product of its diagonal entries. Since the diagonal entries of \( B  \) consists of eigenvalues \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) corresponding to multiplicities \( {m}_{1}, {m}_{2}, \dots, {m}_{k} \), we have  
        \[  \text{det}(A) = \text{det}(B) = \prod_{i=1}^{k} ({\lambda}_{i})^{{m}_{i}}  \]
        which is our desired result.
        \end{proof}
\end{enumerate}

\subsection*{Exercise 5.2.12} Let \( T  \) be an invertible linear operator on a finite-dimensional vector space \( V  \). 
\begin{enumerate}
    \item[(a)] Recall that for any eigenvalue of \( T  \), \( \lambda^{-1} \) is an eigenvalue of \( T^{-1} \) (Exercise 8 of Section 5.1). Prove that the eigenspace of \( T  \) corresponding to \( \lambda  \) is the same as the eigenspace of \( T^{-1} \) corresponding to \( \lambda^{-1} \). % Show that \( dim(E_{\lambda}) = dim(E_{\lambda^{-1}})  \)
        \begin{proof}
            Let \( \lambda  \) be an eigenvalue of \( T  \). Denote the eigenspaces \( {E}_{\lambda} \) and \( {E}_{\lambda^{-1}} \) of \( T  \) and \( T^{-1} \) respectively. We need to show that \( {E}_{\lambda} = {E}_{\lambda^{-1}} \). Note that \( {E}_{\lambda} = N(T - \lambda I ) \) by definition.  Let \( x \in {E}_{\lambda}  \). Then this is true if and only if \( x \neq 0    \) is an eigenvector of \( T  \) corresponding to \( \lambda \) by Theorem 5.4. By {\hyperref[Exercise 5.1.8]{Exercise 5.1.8}}, this is true if and only if \( x  \) is an eigenvector of \( T^{-1} \) corresponding to \( \lambda^{-1} \). By Theorem 5.4 again, this is true if and only if \( x \in N(T^{-1} - \lambda^{-1}I) \). So, \( x \in {E}_{\lambda^{-1}} \) and thus \( {E}_{\lambda} \subseteq {E}_{\lambda^{-1}} \). We can reverse this argument to get the other containment. Thus, \( {E}_{\lambda^{-1}} \subseteq {E}_{\lambda} \) and so we conclude that \( {E}_{\lambda} = {E}_{\lambda^{-1}} \).
        \end{proof}
    \item[(b)] Prove that if \( T  \) is diagonalizable, then \( T^{-1} \) is diagonalizable.
        \begin{proof}
        Let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \) be the eigenvalues of \( T  \) corresponding to multiplicities \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \). Suppose that \( T  \) is diagonalizable. By Theorem 5.9, the multiplicity of each \( {\lambda}_{i} \) is equal to \( \text{dim}({E}_{{\lambda}_{i}}) \) for all \(1 \leq  i  \leq k \). Using part (a), we obtain
        \[  {m}_{i} = \text{dim}({E}_{\lambda_i}) = \text{dim}({E}_{\lambda^{-1}_{i}}) \ \text{for all} \ 1 \leq i \leq k.   \]
        Since each \( {\lambda}_{i}^{-1} \) is an eigenvalue of \( T^{-1} \) and \( \text{dim}({E}_{{\lambda}_{i}^{-1}}) = {m}_{i} \), \( T^{-1} \) must be diagonalizable by part (a) of Theorem 5.9.
        \end{proof}
\end{enumerate}

\subsection*{Exercise 5.2.13} Let \( A \in {M}_{n \times n}(F)  \). Recall from Exercise 14 of Section 5.1 that \( A  \) and \( A^{t} \) have the same characteristic polynomial and hence share the same eigenvalues with the same multiplicities. For any eigenvalue \( \lambda  \) of \( A  \) and \( A^{t} \), let \( {E}_{\lambda} \) and \( {E}_{\lambda}' \) denote the corresponding eigenspaces for \( A  \) and \( A^{t} \), respectively.
\begin{enumerate}
    \item[(a)] Show by way of example that for a given common eigenvalue, these two eigenspaces need not be the same.
        \begin{solution}
        
        \end{solution}
    \item[(b)] Prove that for any eigenvalue \( \lambda  \), \( \text{dim}({E}_{\lambda}) = \text{dim}({E}_{\lambda}') \).
        \begin{proof}
        Let \( A \in {M}_{n \times n}(F) \) and denote \( {E}_{\lambda} = N({L}_{A} - \lambda I) \). Let \( x \in {E}_{\lambda}  \). By Theorem 5.9, \( x \neq 0  \) is an eigenvector of \( {L}_{A} \) corresponding to the eigenvalue \( \lambda  \). So, \( \lambda  \) must be an eigenvalue of \( A  \) by definition. By Theorem 5.2, we must have \( \text{det}(A - \lambda I ) = 0  \). By {\hyperref[Exercise 5.1.14]{Exercise 5.1.14}}, we have   
        \[ \text{det}(A - \lambda I ) = \text{det}(A^{t} - \lambda I ) = 0.  \]
        Thus, \( \lambda  \) is an eigenvalue of \( A^{t} \) by Theorem 5.2 and so \( \lambda  \) is corresponding to the eigenvector \( x  \) of \( L_{A^{t}} \). By Theorem 5.4 again, we can see that \( x \in {E}'_{\lambda} \). Thus, \( {E}_{\lambda} \subseteq {E}'_{\lambda} \). We can reverse this argument to show the other containment. Thus, \( {E}'_{\lambda} \subseteq {E}_{\lambda} \) and so we conclude that \( {E}_{\lambda} = E'_{\lambda} \).
        \end{proof}
    \item[(c)] Prove that if \( A  \) is diagonalizable, then \( A^{t}  \) is diagonalizable.
        \begin{proof}
        Apply the same argument in part (b) of Exercise 5.2.12 and part (a) of this exercise to show that \( A^{t} \) is diagonalizable.
        \end{proof}
\end{enumerate}

\begin{definition}[Simultaneously Diagonalizable]
    Two linear operators \( T  \) and \( U  \) on a finite-dimensional vector space \( V  \) are called \textbf{simultaneously diagonalizable} if there exists an ordered basis \( \beta \) for \( V  \) such that both \( [T]_{\beta} \) and \( [U]_{\beta} \) are diagonal matrices. Similarly, \( A,B \in {M}_{n \times n}(F) \) are called \textbf{simultaneously diagonalizable} if there exists an invertible matrix \( Q \in {M}_{n \times n}(F) \) such that both \( Q^{-1} A Q  \) and \( Q^{-1} B Q   \) are diagonal matrices.
\end{definition} 

\subsection*{Exercise 5.2.17} 
\begin{enumerate}
    \item[(a)] Prove that if \( T  \) and \( U  \) are simultaneously diagonalizable linear operators on a finite-dimensional vector space \( V  \), then the matrices \( [T]_{\beta} \) and \( [U]_{\beta} \) are simultaneously diagonalizable for any ordered basis \( \beta \).
        \begin{proof}
        Let \( T, U  \) be simultaneously diagonalizable linear operators. Since \( V  \) is finite-dimensional let \( \beta'  \) be another ordered basis for \( V  \) (that is distinct from \( \beta \)). Using Theorem 2.23, we have
        \[  [T]_{\beta'} = Q^{-1} [T]_{\beta} Q \]
        where \( Q^{-1}, Q  \) are invertible and diagonal where 
        \begin{center}
            \( Q = [{I}_{V}]_{\beta'}^{\beta}  \) and \( Q^{-1} = [{I}_{V}]_{\beta}^{\beta'}  \).
        \end{center}
        Since \( [T]_{\beta} \) is diagonal, \( [T]_{\beta'}  \) is diagonal as well. We can apply this same argument to \( U  \) to show that \( Q^{-1} [U]_{\beta} Q   \) for some invertible matrix \( Q \).
        \end{proof}
    \item[(b)] Prove that if \( A  \) and \( B  \) are simultaneously diagonalizable matrices, then \( {L}_{A} \) and \( {L}_{B} \) are simultaneously diagonalizable linear operators.
        \begin{proof}
        Let \( T = {L}_{A} \) and \( U = {L}_{B} \). Apply part (a) to show that \( {L}_{A} \) and \( {L}_{B} \) are also simultaneously diagonalizable linear operators. 
        \end{proof}
\end{enumerate}

\subsection*{Exercise 5.2.18} 
\begin{enumerate}
    \item[(a)] Prove that if \( T  \) and \( U  \) are simultaneously diagonalizable operators, then \( T  \) and \( U  \) commute (i.e., \( TU = UT \)). 
        \begin{proof}
        Let \( \beta = \{ {v}_{1}, {v}_{2}, \dots ,{v}_{n} \}  \) be a basis for a finite dimensional vector space \( V  \). First, we need to ensure that given \( T \) and \( U  \) are simultaneously diagonalizable that both \( TU \) and \( UT  \) are simultaneously diagonalizable as well; that is, \( [TU]_{\beta} \) and \( [UT]_{\beta} \) are diagonal matrices. Since \( T \) and \( U  \) are simultaneous diagonalizable, there exists a invertible matrix \( Q  \) such that  
        \begin{center}
            \( Q^{-1}[T]_{\beta} Q \) and \( Q^{-1} [U]_{\beta} Q  \) 
        \end{center}
        are diagonal matrices. Since \( V  \) is finite-dimensional, let \( \beta'  \) be a basis such that \( [T]_{\beta'} = Q^{-1} [T]_{\beta}  Q  \) and \( [U]_{\beta'} = Q^{-1} [U]_{\beta} Q  \). Using Theorem 2.11, we have
        \begin{align*}
            [TU]_{\beta} &= [T]_{\beta} [U]_{\beta} \\
                         &= (Q [T]_{\beta'} Q^{-1})(Q [U]_{\beta'} Q^{-1}) \\
                         &= Q [T]_{\beta'} [U]_{\beta'} Q^{-1} \\
                         &= Q [TU]_{\beta'} Q^{-1}.
        \end{align*}
        But this implies that \( TU \) is simultaneously diagonalizable. A similar argument shows \( UT  \) being simultaneously diagonalizable. Hence, \( [TU]_{\beta} \) and \( [U]_{\beta} \) are diagonal matrices. Let \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{n} \) and \( {\lambda}_{1}', {\lambda}_{2}', \dots, {\lambda}_{n}'  \) be the eigenvalues of   \( T  \) and \( U  \) respectively. Thus, for \( 1 \leq j \leq n  \) we have
        \begin{align*}
            (TU)({v}_{j}) &=  \sum_{ i=1 }^{ n } {D}_{ij} {v}_{j} = {D}_{jj} 
                          = ({\lambda}_{j} {\lambda}_{j}') {v}_{j} 
                          = ({\lambda}_{j}'{\lambda}_{j}) {v}_{j} =  {C}_{jj} \\
                          &= \sum_{ i=1 }^{ n } {C}_{ij} {v}_{j} = (UT)({v}_{j}).
        \end{align*}
        Thus, \( TU \) and \( UT  \) commute. 
        \end{proof}
    \item[(b)] Show that if \( A  \) and \( B  \) are simultaneously diagonalizable matrices, then \( A  \) and \( B  \) commute.
        \begin{proof}
        Let \( T = {L}_{A} \) and \( U = {L}_{B} \), then apply part (a).
        \end{proof}
\end{enumerate}

\subsection*{Exercise 5.2.19} Let \( T  \) be a diagonalizable linear operator on a finite-dimensional vector space, and let \( m  \) be any positive integer. Prove that \( T  \) and \( T^{m} \) are simultaneously diagonalizable.
\begin{proof}
Let \( V \) be a finite-dimensional vector space. Let \( \beta \) and \( \beta'  \) be two ordered bases for \( V  \). We need to show that \( [T]_{\beta} \) and \(  [T^{m}]_{\beta}  \)  are diagonal matrices. The former is shown via part (a) of Exercise 5.2.17. We will induct on \(  m  \). The base case is taken care of. Let's assume that the result holds for \( m - 1  \) case. That is, there exists an invertible matrix \( Q  \) such that \( Q^{-1} [T^{m-1}]_{\beta} Q = [T]_{\beta'} \). Similarly, we have \( Q^{-1} [T]_{\beta} Q = [T]_{\beta'} \). We will show that the result holds for the \(m  \)th case. By the induction hypothesis and Theorem 2.11, we can write        
\begin{align*}
    [T^{m}]_{\beta} &= [T^{(m-1) + 1}]_{\beta} = [T^{m-1} T]_{\beta} = [T^{m-1}]_{\beta} [T]_{\beta} \\
                    &= (Q [T^{m-1}]_{\beta'} Q^{-1})(Q [T]_{\beta'}Q^{-1}) = Q [T^{m}]_{\beta'} Q^{-1}. 
\end{align*}
Thus, we have \( Q^{-1} [T^{m}]_{\beta} Q = [T^{m}]_{\beta'}  \). Notice how \( T^{m-1}  \) and \( T \) commute via part (a) of Exercise 5.2.18. Thus, we get that both \( T \) and \( T^{m} \) are simultaneously diagonalizable linear operators. 
\end{proof}

\subsection*{Exercise 5.2.20} Let \( {W}_{1}, {W}_{2}, \dots, {W}_{k} \) be subspaces of a finite-dimensional vector space \( V  \) such that 
\[  \sum_{ i=1 }^{ k  } {W}_{i} = V. \]
Prove that \( V  \) is the direct sum of \( {W}_{1}, {W}_{2}, \dots, {W}_{k} \) if and only if 
\[  \text{dim}(V) = \sum_{ i=1 }^{ k } \text{dim}({W}_{i}). \]
\begin{proof}
For the forwards direction, suppose \( V \) is the direct sum of \( {W}_{1}, {W}_{2}, \dots, {W}_{k} \). By definition, we have 
\[  V = \sum_{ i=1  }^{ n  } {W}_{i} \]
and 
\[ {W}_{j} \cap \sum_{ i \neq j  }^{  } {W}_{i} = \{ 0 \}   \] 
for some \( 1 \leq j \leq k   \).
Using repeated applications of part (a) of Exercise 1.6.29, we get that 
\begin{align*}
    \text{dim}(V) = \text{dim} \Big(  \sum_{ i=1  }^{ n } {W}_{i} \Big)  
                  = \sum_{ i=1  }^{ n } \text{dim}({W}_{i}) + \text{dim}\Big(  {W}_{j} \cap \sum_{ i \neq j }^{  } {W}_{i} \Big) 
                  = \sum_{ i=1  }^{ n } \text{dim}({W}_{i}).
\end{align*}
Thus, we get that
\[  \text{dim}(V) = \sum_{ i=1  }^{ n } \text{dim}({W}_{i}). \]
Conversely, each \( {W}_{i} \) is finite-dimensional, there exists a basis \( {\beta}_{i}  \) for each \( {W}_{i} \) for \( 1 \leq i \leq k  \). Thus, we have \( \beta = {\beta}_{1} \cup {\beta}_{2} \cup \cdots \cup {\beta}_{k} \) is a basis for \( V  \) which implies that
\[  \bigoplus_{i=1}^{k} {W}_{i} = V  \]
by part (e) of Theorem 5.9.
\end{proof} 

\begin{definition}[Partitions]
    We call the sets \( {\beta}_{1}, {\beta}_{2}, \dots, {\beta}_{k} \) a partition of \( \beta \) if \(   {\beta}_{1}, {\beta}_{2}, \dots, {\beta}_{k} \) are subsets of \( \beta \) such that
    \begin{center}
        \( \beta = {\beta}_{1} \cup {\beta}_{2} \cup \cdots \cup {\beta}_{k} \) and \( {\beta}_{i} \cap {\beta}_{j}  \) if \( i \neq j  \).
    \end{center}
\end{definition}

\subsection*{Exercise 5.2.21} Let \( V  \) be a finite-dimensional vector space with a basis \( \beta  \), and let \( {\beta}_{1}, {\beta}_{2}, \dots, {\beta}_{k} \) be a \textbf{partition} of \( \beta  \). Prove that  
\[  V = \text{span}({\beta}_{1}) \oplus \text{span}({\beta}_{2}) \oplus \cdots \oplus \text{span}({\beta}_{k}).  \]
\begin{proof}
Let \( \beta \) be a basis for \( V  \) and let \( {\beta}_{1}, {\beta}_{2}, \dots, {\beta}_{k} \) be a partition of \( \beta \). By definition, \( {\beta}_{1}, {\beta}_{2}, \dots, {\beta}_{k } \) are subsets of \( \beta \) such that
\begin{center}
    \( \beta = {\beta}_{1} \cup {\beta}_{2} \cup \cdots \cup {\beta}_{k} \) and \( {\beta}_{i} \cap {\beta}_{j} \) if \( i \neq j \).
\end{center}
Since \( \beta \) is a basis, we must have \( \text{span}(\beta) = V  \). By repeated applications of {\hyperref[Exercise 1.4.14]{Exercise 1.4.14}}, we must have
\begin{align*}
    V = \text{span}(\beta) = \text{span}\Big(\bigcup_{ i=1 }^{ k  }  {\beta}_{i}\Big) = \sum_{ i=1  }^{ k  } \text{span}({\beta}_{i}).
\end{align*}
Thus, we establish that
\[  V = \sum_{ i=1  }^{ k  } \text{span}({\beta}_{i}). \]
Now suppose for sake of contradiction that there exists a \( 1 \leq j \leq k  \) such that
\[  v \in \text{span}({B}_{j}) \cap \sum_{  i \neq j  }^{  } \text{span}({\beta}_{i}). \]
But this implies that \( {\beta}_{j} \cap {\beta}_{i} \neq \emptyset \) which is a contradiction. Thus, we must have that
\[  \text{span}({\beta}_{j}) \cap \sum_{ i=1  }^{ k  } \text{span}({\beta}_{i})  = \{  0  \}.\]
Hence, we must have 
\[  V = \bigoplus_{i=1}^{k} \text{span}({\beta}_{i}). \]
\end{proof}

\subsection*{Exercise 5.2.22} Let \( T  \) be a linear operator on a finite-dimensional vector space \( V  \), and suppose that the distinct eigenvalues of \( T  \) are \( {\lambda}_{1}, {\lambda}_{2}, \dots, {\lambda}_{k} \). Prove that
\[  \text{span}(\{ x \in V : x \ \text{is an eigenvector of } T  \} ) = {E}_{{\lambda}_{1}} \oplus {E}_{{\lambda}_{2}} \oplus \cdots \oplus {E}_{\lambda_{k }}. \]
\begin{proof}

\end{proof}
