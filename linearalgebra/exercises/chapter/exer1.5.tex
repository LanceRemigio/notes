\section{Linear Dependence and Linear Independence}

\subsubsection{Exercise 1.5.1} Label the following statements as true or false.

\begin{enumerate}
    \item[(a)] If \( S  \) is a linearly dependent set, then each vector in \( S  \) is a linear combination of other vector in \( S  \).
        \begin{proof}
        \textbf{True}
        \end{proof}
    \item[(b)] Any set containing the zero vector is linearly dependent.
        \begin{proof}
        \textbf{True}
        \end{proof}
    \item[(c)] The empty set is linearly dependent.
        \begin{proof}
        \textbf{False}. It is linearly independent.
        \end{proof}
    \item[(d)] Subsets of linearly dependent sets are linearly dependent. 
        \textbf{True} by Theorem 6.
    \item[(e)] Subsets of linearly independent sets are linearly independent.
        \begin{proof}
        \textbf{True} by corollary to Theorem 6.
        \end{proof}
    \item[(f)] If \( a_{1} x_{1} + a_{2} x_{2} + \cdots + a_{n} x_{n} = 0  \) and \( x_{1}, x_{2}, \dots, x_{n}  \) are linearly independent, then all the scalars \( a_{i}  \) are zero.
        \begin{proof}
       \textbf{True} this is by definition. 
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 1.5.4} In \( F^{n}  \), let \( e_{j}  \) denote the vector whose \( j \)th coordinate is \( 1  \) and whose other coordinates are \( 0  \). Prove that \( \{ e_{1}, e_{2}, \dots, e_{n}  \}  \) is linearly independent.
\begin{proof}
    Choose a finite amount of scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) to create the following linear combination:
    \[  a_{1} e_{1} + a_{2} e_{2} + \cdots + a_{n} e_{n} = (0,0, \dots, 0). \tag{1} \]
    To show that the set \( \{ e_{1} ,e_{2}, \dots, e_{n}  \}  \) is linearly independent, we need to show that the scalars \( a_{1}, a_{2}, \dots, a_{n} \in F  \) have the trivial representation; that is, \( a_{1} = a_{2} = \cdots = a_{n} = 0  \).
    Since the \( j \)th coordinate of \( e_{j} \) is \( 1  \) but \( 0 \) in all the other entries, we have that 
    \begin{align*}
        &a_{1} (1,0,\dots, 0) + a_{2} (0,1,\dots,0) + \cdots + a_{n} (0,0, \dots, 1 )    \\
        &= (a_{1}, 0, \dots, 0  ) + (0 , a_{2}, \dots, 0 ) + \cdots + (0,0, \dots, a_{n}) \\
        &= (a_{1}, a_{2}, \dots , a_{n}  ).
    \end{align*}
    Hence, we have 
    \[  (a_{1}, a_{2} , \dots, a_{n} ) = (0,0, \dots, 0). \]
    Equating each entry of the left side of the equation above to \( 0 \), we find that \( a_{i} = 0  \) for all \( 1 \leq j \leq n  \). Hence, the set \( \{ e_{1}, e_{2}, \dots, e_{n}  \}  \) is linearly independent.
\end{proof}


\subsubsection{Exercise 1.5.4} Show that the set \( \{ 1,x, x^{2}, \dots, x^{n}  \}  \) is linearly independent in \( P_{n}(F) \).
\begin{proof}
    Just like the prior exercise, we need to show that we can find scalars \( a_{0}, a_{1}, \dots, a_{n} \in F  \)  such that 
    \[  a_{0} + a_{1} x + a_{2} x^{2} + \cdots + a_{n} x^{n} = 0   \]
    where \( a_{i} = 0  \) for all \( 0 \leq i \leq n \). Note that the \( 0  \) polynomial is just 
    \[  0 + 0 x + 0 x^{2} + \cdots + 0 x^{n} = 0.  \]
    Hence, equating coefficients we immediately get that \( a_{i} = 0  \) for all \( 0 \leq i \leq n \). Thus, the set \( \{ 1 , x , x^{2}, \dots, x^{n} \}  \)  is linearly independent. 
\end{proof}

\subsubsection{Exercise 1.5.6} In \( M_{m \times n}(F) \), let \( E^{ij}  \) denote the matrix whose only nonzero entry is \( 1  \) in the \( i \)th row and \( j \)th column. Prove that 
\( \{ E^{ij} : 1 \leq i \leq m , 1 \leq j \leq n  \}  \) is linearly independent.
\begin{proof}
    First, we create a  linear combination of a finite amount vectors in \( E  = \{ E^{ij} : 1 \leq i  \leq m , 1 \leq j \leq n   \}  \) with scalars \( \delta_{k}  \) for \( 1 \leq k \leq N  \) with \( N = mn \) as the number of total entries in each matrix in \( \{ E^{ij} : 1 \leq i \leq m , 1 \leq j \leq n  \}  \). Note that after doing our scalar multiplication and summing up each term, we find that each  \( \delta_{k} E_{ij} = \delta_{k}   \) in our linear combination can equated with a corresponding \( i  \) and  \( j  \) entry in the zero matrix such that \( \delta_{k} =  0  \) for all \( 1 \leq k \leq N  \). Hence, \( E  \) is a linearly independent set.
\end{proof}

\subsubsection{Exercise 1.5.7} Recall from Example 3 in Section 1.3 that the set of diagonal matrices in \( M_{2 \times 2 }(F) \) is a subspace. Find a linearly independent set that generates this subspace.
\begin{proof}
Define \( W  \) as the linearly independent spanning set of the set of diagonal matrices in  \( M_{2 \times 2 } \) where 
\begin{align*}
    W &= \Bigg\{ \begin{pmatrix}
            1 & 0 \\
            0 & 0 
    \end{pmatrix} , \begin{pmatrix}
            0 & 0 \\
            0 & 1 
    \end{pmatrix} \Bigg\}. \\
\end{align*}
To see why \( W  \) is a linearly independent set, choose scalars \( \delta_{1} , \delta_{2} \in F  \) such that 
\[  \delta_{1} \begin{pmatrix}
    1 & 0 \\
    0 & 0 
\end{pmatrix}  + \delta_{2} \begin{pmatrix}
    0 & 0 \\
    0 & 1 
\end{pmatrix} = \begin{pmatrix}
    0 & 0 \\
    0 & 0 
\end{pmatrix}.\]
Performing scalar multiplication and vector addition gives us the following equation 
\[  \begin{pmatrix}
    \delta_1 & 0 \\ 
    0 & \delta_2  
\end{pmatrix}  = \begin{pmatrix}
    0 & 0 \\
    0 & 0 
\end{pmatrix}. \]
Since the zero matrix is a diagonal matrix, we know that equation entries where  \( i = j    \)yields \( \delta_{1} = \delta_{2} = 0 \). Hence, \( W  \) is a linearly independent set that generates the set of diagonal matrices of \( M_{2 \times 2 }(F ) \).
\end{proof}

\subsubsection{Exercise 1.5.8} Let \( S = \{ (1,1,0) , (1,0,1), (0,1,1) \}  \) be a subset of the vector space \( F^{3} \).
\begin{enumerate}
    \item[(a)] Prove that if \( F = \R  \), then \( S  \) is linearly independent.
        \begin{proof}
        
        \end{proof}
    \item[(b)] Prove that if \( F  \) has characteristic two, then \( S  \) is linearly dependent.
        \begin{proof}
        
        \end{proof}
\end{enumerate} 


\subsubsection{Exercise 1.5.9} Let \( u \) and \( v  \) be distinct vectors in a vector space \( V  \). Show that \( \{ u,v  \}  \) is linearly dependent if and only if \( u  \) or \( v  \) is a multiple of the other.
I have written two proofs for this:
\begin{proof}
Let \( u  \) and \( v  \) be distinct vectors in a vector space \( V  \). 

( \(  \Rightarrow \) ) Since \( \{ u,v  \}   \) is a linearly dependent set, we can find scalars \( a_{1} , a_{2} \in F   \) such that 
\[ a_{1} u + a_{2} v = 0 \tag{1} \] 
Suppose \( v  \) is not a multiple of \( u  \) and choose \( a_{1} \neq 0  \) since \( \{ u,v  \}   \) is linearly dependent. We need to show that \( u  \) is a multiple of \( v  \). Solving for \( u  \), we get that
\[  u =  - \frac{ a_{2} }{ a_{1} }   v. \]
Hence, \( u  \) is a multiple of \( v  \). 

( \( \Leftarrow \) ) Suppose \( u  \) or \( v  \) is a scalar multiple of the other. Assume \( u  \) is the scalar multiple of \( v  \). Then for some \( c \neq 0 \in F  \), we have \( u = cv  \).  Hence, we have \( u - cv = 1u - cv = 0  \). This tells us that \( \{ u,v  \}  \) is linearly dependent. 
\end{proof}


\subsubsection{Exercise 1.5.12} Prove Theorem 1.6 and its corollary.
\begin{proof}
See proof in notes.
\end{proof}

\subsubsection{Exercise 1.5.13} Let \( V  \) be a vector space over a field of characteristic not equal to two.
\begin{enumerate}
    \item[(a)] Let \( u  \) and \( v  \) be distinct vectors in \( V  \). Prove that \( \{ u,v  \}   \) is linearly independent if and only if \( \{ u + v , u - v  \}   \) is linearly independent.
        \begin{proof}
        Let \( u \) and \( v  \) be distinct vectors in \( V  \). 

        For the forwards direction, assume \( \{ u,v  \}   \) is a linearly independent set. We need to show that \( \{ u + v , u - v  \}  \) is linearly independent. Hence, we need to find \( a,b \in F  \) such that 
        \[  a(u+v) + b(u-v) = 0. \tag{1} \]
        Note that (1) leads to 
        \begin{align*}
            a(u+v) + b(u-v) &= au + av + bu - bv \\
                            &= au - bv + av + bu. 
        \end{align*}
        Since \( \{ u,v  \}   \) is a linearly independent set, we have that 
        \[  au - bv = 0  \]
        and 
        \[  av + bu = 0  \] for \( a=b = 0 \). Hence, 
        \[  a(u+v) + b(u-v) = 0  \] for \( a = b = 0  \) and so \( \{ u - v , u + v  \}   \) is a linearly independent set.

        For the backwards direction, suppose \( \{ u + v , u - v  \}  \) is linearly independent. We need to show that \( \{ u , v  \}   \) is linearly independent. Note that \( a,b \in F  \) such that 
        \[  a(u+v) + b(u-v) = 0  \]
        for \( a = b = 0  \) since \( \{ u - v, u + v  \}  \) is linearly independent. Note that
        \begin{align*}
            a(u+v) + b(u-v) &= au + av + bu - bv \\
                            &= au - bv + av + bu \\
                            &= 0 + av + bu \\
                            &= 0. 
        \end{align*}
        Thus, \( av + bu = 0  \) where \( a,b  \) both zero. Thus, the set \( \{ u,v  \}   \) is linearly independent.
        \end{proof}
    \item[(b)] Let \( u, v ,  \) and \( w  \) be distinct vectors in \( V  \). Prove that \( \{ u,v,w  \}  \) is linearly independent if and only if \( \{ u+v , u + w , v + w  \}  \) is linearly independent.
        \begin{proof}
        For the forwards direction, suppose \( \{ u,v ,w  \}  \) is linearly independent. Then choose  scalars \( a_{1}, a_{2}, a_{3} \in  F    \) such that 
        \[  a_{1} u + a_{2} v  + a_{3} w = 0   \]
        with \( a_{1} = a_{2} = a_{3} = 0  \). We need to show that \( \{ u+v , u + w , v + w  \}  \) is linearly independent; that is, we need to show that we can find scalars \( a_{1}, a_{2}, a_{3} \in F  \) such that 
        \[  a_{1}(u+v) + a_{2} (u+w) + a_{3} (v+w) = 0 \tag{1} \]
        for \( a_{1} = a_{2} = a_{3} = 0  \). Observe that (1) can be written in the following way 
        \[  ( a_{1} u + a_{3} v + a_{2} w )  + ( a_{1} v + a_{2} u + a_{3}w )  = 0 \tag{2} \]
        Since \( \{ u,v,w \}  \) is linearly independent, we know that \( a_{1} = a_{2} = a_{3} = 0  \). But this also has to mean that \( \{ u+ v, u + w , v + w  \}  \) is a linearly independent set.
        
        For the backwards direction, suppose \( \{ u + v , u + w , v + w  \}  \) is linearly independent. Then choose scalars \( a_{1}, a_{2}, a_{3}  \in F  \) such that 
        \[  a_{1} (u + v ) + a_{2} (u + w) + a_{3} (v + w) = 0 \tag{1}.  \]
        We need to show that \( \{ u,v,w \}  \) is linearly independent. Observe that (1) can be re-written as 
        \begin{align*}
            &(a_{1} u + a_{3} v + a_{2} w )  + (a_{1} v + a_{2} u + a_{3} w ) = 0 \\
            &\implies 0 + (a_{1} v + a_{2} u + a_{3} w ) = 0 \\
            &\implies a_{1} v + a_{2} u + a_{3} w = 0
        \end{align*}
        where \( a_{1} = a_{2} = a_{3} = 0  \). Hence, \( \{ u,v ,w  \}   \) is linearly independent.
        \end{proof}
\end{enumerate}


\subsubsection{Exercise 1.5.14} Prove that a set \( S  \) is linearly dependent if and only if \( S = \{ 0  \}  \) or there exists distinct vectors \( v , u_{1}, u_{2}, \dots, u_{n} \in S  \) such that \( v  \) is a linear combination of \( u_{1}, u_{2}, \dots, u_{n} \).
\begin{proof}
    For the forwards direction, Let \( S  \) be a linearly dependent. Then we need to show that either \( S = \{ 0  \}  \) or \( S  \) contains distinct vectors \( v , u_{1}, u_{2} , \dots, u_{n} \in S  \) such that \( v \in \text{span}(S ) \). Suppose there does not exists distinct vectors  \( v , u_{1}, u_{2}, \dots, u_{n} \in  S  \) such that \( v \in \text{span}(S ) \). This tells us that \( S  \) only contains the singleton \( S = \{ v  \}  \). Furthermore, we must require \( v = 0  \) since \( S  \) is linearly dependent. Otherwise, \( v \neq 0  \) would imply that \( S  \) is linearly independent. Hence, we have \( S = \{ 0 \}  \).  
    Now suppose \( S \neq \{ 0 \}  \). Since \( S  \) is linearly dependent, there exists scalars \( a_{1}, a_{2}, \dots, a_{n+1} \)  and vectors \( v , u_{1}, u_{2}, \dots, u_{n}     \) such that 
    \[  a_{1} v +  a_{2} u_{1} + a_{3} u_{2} + \cdots + a_{n+1} u_{n} = 0   \] with all \( a_{1} , a_{2}, \dots , a_{n},  a_{n+1} \) not all equal to zero. Solving for \( v  \), we get that
    \[  v = -(a^{-1}_{1} a_{2} ) u_{1} - (a^{-1}_{1} a_{3}) u_{2} - \cdots - (a^{-1}_{1} a_{n+1}) u_{n}. \] 
    Since \( v  \) is a linear combination of \( u_{1}, u_{2}, \dots, u_{n}  \), we have that \( v \in \text{span}(S) \).

    Conversely, suppose that either \( S = \{ 0  \}  \) or there exists distinct vectors \( v , u_{1}, u_{2}, \dots, u_{n} \in S  \) such that \( v  \) is a linear combination of \( u_{1}, u_{2}, \dots, u_{n}   \). Assume \( S = \{ 0 \}  \). Then \( S  \) is linearly dependent because the singleton is the zero vector. Now suppose \( v  \) is a linear combination of vectors \( u_{1}, u_{2}, \dots, u_{n}  \). Then there exists scalars  \( a_{1}, a_{2}, \dots, a_{n} \in F  \) and distinct vectors \( u_{1}, u_{2}, \dots, u_{n} \in S  \) such that
    \[  v = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n}. \]
    Subtracting \( v  \) from both sides of this equation yields the following equation 
    \[  a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n} u_{n} - 1 v = 0.  \]
    Since not all scalars in the equation above are zero and \( v  , u_{1}, u_{2}, \dots, u_{n} \in S  \), we must have that \( S  \) is a linearly dependent set.
\end{proof}


\subsubsection{Exercise 1.5.15} Prove that a set \( S = \{ u_{1} , u_{2}, \dots, u_{n} \}   \) be a finite set of vectors. Prove that \( S  \) is linearly dependent if and only if \( u_{1} = 0  \)  or \( u_{k+1} \in \text{span}(\{ u_{1}, u_{2}, \dots, u_{k } \} ) \) for some \( k  \) where \( 1 \leq k < n  \). 
\begin{proof}
Suppose  \( u_{k+1} \notin \text{span}(\{ u_{1} , u_{2}, \dots, u_{k }  \} ) \) for all \( 1 \leq k < n \). Since \( S  \) is linearly dependent, we know that the zero vector is contained in \( S  \). Choose \( k =1  \) such that \( u_{1} = 0  \) and we are done. On the other hand, suppose \( u_{1} \neq 0  \). We need to show that \( u_{k+1} \in \text{span}(\{ u_{1} , u_{2} , \dots, u_{k } \} ) \). Choose \( k = n -1  \). Then clearly \( n = k + 1  \). Since \( S  \) is linear independent, choose scalars \( a_{1} , a_{2}, \dots, a_{n} \in F   \) such that
\[ a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{k } u_{k } + a_{k+1}u_{k+1} = 0.    \tag{1}\]
where \( a_{1}, a_{2} , \dots, a_{k } ,  a_{k+1} \) not all zero. Solving for \( u_{k+1}  \) by subtracting \( a_{k+1} u_{k+1} \) on both sides of (1) and multiplying \( -a^{-1}_{k+1}  \) on both sides of (1), we end up with the following equation:
\[ u_{k+1} = - (a^{-1}_{k+1} a_{1})  u_{1} - ( a^{-1}_{k+1} a_{2} ) u_{2}  - \cdots -  ( a^{-1}_{k+1} a_{k}    ) u_{k}.    \]
This tells us that \( u_{k+1} \) can be written as a linear combination of vectors  \( u_{1}, u_{2}, \dots, u_{k}   \). Hence, \( u_{k+1} \in \text{span}(\{ u_{1}, u_{2}, \dots u_{k} \} ) \).

Conversely, either \( u_{1} = 0  \) or \( u_{k+1} \in \text{span}(u_{1}, u_{2}, \dots, u_{k} ) \). Suppose \( u_{1} = 0  \). Then \( S  \) contains the zero vector so \( S  \) must be linearly dependent. On the other hand, choose \( k = n-1  \) where \( 1 \leq k < n  \) such that  \( u_{k+1} \in \text{span}(\{ u_{1}, u_{2}, \dots, u_{k } \} ) \) implies that there exists scalars \( a_{1}, a_{2}, \dots, a_{k }  \) such that 
\begin{align*} 
    &u_{k+1} = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{k } u_{k } \\ 
    &\implies u_{n} = a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n-1} u_{n-1}. \tag{1}
\end{align*}
Subtracting \( u_{n}  \) on both sides of (1) implies that
\[ ( a_{1} u_{1} + a_{2} u_{2} + \cdots + a_{n-1} u_{n-1} ) - 1  u_{n} = 0.   \]
Since not all scalars in the linear combination above are zero, we know that \( S  \) must be linearly dependent.

\end{proof}

\subsubsection{Exercise 1.5.16} Prove that a set \( S  \) of vectors is linearly independent if and only if each finite subset of \( S  \) is linearly independent.
\begin{proof}
    (  \( \Rightarrow \)  ) Suppose \( S  \) is a linearly independent set. Let \( S' \) be any finite subset of \( S  \). By corollary to Theorem 6, we can see that \( S' \subseteq S  \) implies that \( S'  \) is also linearly independent. 
    ( \( \Leftarrow \) ) We will proceed by proving the contrapositive. Let \( S' \subseteq S  \) be a finite subset that is linearly dependent set. We will prove that \( S  \) is a linearly dependent set. Since \( S' \subseteq S  \), we have that \( S  \) must be a linearly dependent set by Theorem 6.
\end{proof}

\subsubsection{Exercise 1.5.17} Let \( M  \) be a square upper triangular matrix (as defined in Section 1.3) with nonzero diagonal entries. Prove that the columns of M are linearly independent.
\begin{proof}
Let \( M  \) be a square upper triangular matrix. Note that \( M  \) have the following form:
\[  
    \begin{pmatrix}
        a_{11} & a_{12} & a_{13} &  \cdots &  a_{1n} \\
        0 & a_{22} &  a_{23} &  \cdots &  a_{2n} \\
        \vdots & \vdots & \ddots & \ddots & \vdots \\ 
        \vdots & \vdots & \cdots & \ddots & a_{n-1 n } \\
        0 & 0  & \cdots & \cdots & a_{n n }
    \end{pmatrix}.
 \]
We can define the following columns as vectors \( v_{1}, v_{2}, \dots v_{n}   \) where 
\[  
    v_{1}  =
    \begin{pmatrix}
        a_{11} \\
        0 \\
        \vdots \\
        0
    \end{pmatrix}, v_{2} = \begin{pmatrix}
        a_{12} \\
        a_{22} \\
        \vdots \\
        0 
    \end{pmatrix}, \dots , v_{n} = \begin{pmatrix}
        a_{1n} \\
        a_{2n} \\
        \vdots \\
        a_{n n }
    \end{pmatrix}.
\]
Choose scalars \( \delta_{1} , \delta_{2}, \dots, \delta_{n} \in F  \) such that 
\[  \delta_{1} v_{1} + \delta_{2} v_{2} + \cdots + \delta_{n} v_{n} = 0 \]
where the zero vector \( 0  \) is denoted as 
\[  0 = \begin{pmatrix}
    0 \\ 
    \vdots \\
    0
\end{pmatrix}. \]
Using scalar multiplication and addition, we arrive at the following system of equations:
\begin{align*}
    \delta_{1} a_{11} + \delta_{2} a_{12} + \cdots + \delta_{n-1} a_{1 n - 1} +  \delta_{n} a_{1n} &= 0 \\
      \delta_{2} a_{22} + \delta_{3} a_{23} + \cdots +  \delta_{n}  a_{2n} &= 0 \\
                                                                         &\vdots  \\
                                                                     \delta_{n-1} a_{n-1 n-1 } + \delta_{n} a_{n-1 n }     &= 0 \\
                                                                     \delta_{n} a_{n n } &= 0.
\end{align*}
Since all the diagonal entries of \( M  \) are non-zero, we can see from the equation above that \( \delta_{n } = 0  \) which subsequently tells us that \( \delta_{n -1 } = 0  \). We claim that \( \delta_{n-1} = \delta_{n } = 0  \) for all \( n > 1  \). We can prove this via induction. Let our base case be \( n = 2  \). Then observe that we have an upper triangular \( 2 \times 2  \) matrix such that the linear combination of the columns lead to the following system of linear equations:
\begin{align*}
    \delta_{1} a_{11} + \delta_{2} a_{12} &= 0 \\
    0 + \delta_{2} a_{22} &= 0. 
\end{align*}
Observe that \( \delta_{2} = 0  \) which also implies that \( \delta_{1} =0  \). Now suppose our claim that \( \delta_{n-1} = \delta_{n} = 0   \) holds for all \( n > 1  \). We want to show that our claim still holds for the \( n + 1  \) case. Observe that \( \delta_{n + 1} = 0   \). Using the same process that proved the base case, we have that \( \delta_{n} = \delta_{n+1} = 0  \). By our inductive hypothesis, we know that \( \delta_{n-1} = \delta_{n} = 0  \). This tells us that \( \delta_{n} = \delta_{n+1} \) for all \( n > 1  \).  Hence, the columns of \( M  \) are linearly independent.  


\end{proof}

\subsubsection{Exercise 1.5.18} Let \( S  \) be a set of nonzero polynomials in \( P(F) \) such that no two have the same degree. Prove that \( S  \) is linearly independent.
\begin{proof}
\textbf{TO DO.}
\end{proof}

\subsubsection{Exercise 1.5.19} Prove that if \( \{ A_{1}, A_{3}, \dots, A_{k} \}  \) is a linearly independent subset of \( M_{n \times n }(F) \), then \( \{ A^{t}_{1} , A^{t}_{2} , A^{t}_{3}, \dots, A^{t}_{k } \}  \) is also linearly independent.
\begin{proof}
\textbf{TO DO.}
\end{proof}

\subsubsection{Exercise 1.5.20} Let \( f,g \in \mathcal{F}(\R , \R ) \) be the functions defined by \( f(t) = e^{rt} \) and \( g(t) = e^{s t  } \), where \( r \neq s  \). Prove that \( f  \) and \( g  \) are linearly independent in \( \mathcal{F}(\R , \R ) \).
\begin{proof}
\textbf{TO DO.} 
\end{proof}


