\section{Bases and Dimension}

\subsubsection{Exercise 1.6.11} Let \( u  \) and \( v \) be distinct vectors of a vector space \( V  \). Show that if \( \{ u,v \}  \) is a basis for \( V  \) and \( a \) and \( b \) are nonzero scalars, then both \( \{ u + v , au  \}  \) and \( \{ au, bv \}  \) are also bases for \( V  \).

\begin{proof}
    (\( \Rightarrow \)) We want to show that \( \{ u+v , au \}  \) and \( \{ au, bv  \}  \) is a basis for \( V  \); that is, we want to show that \( \{ u +v , au  \}   \)  and \( \{ au, bv  \}  \) is both linearly independent and generates \( V  \). We will start by showing that \( \{ u+v , au \}  \) is linearly independent. Choose scalars \( \delta_{1}, \delta_{2}  \) such that 
    \[  \delta_{1} (u+v) + \delta_{2} (au) = 0 \tag{1}  \]
    with \( \delta_{1} =  \delta_{2} = 0  \). Let us algebraically manipulate (1) into the following form:
    \begin{align*} 
    \delta_{1} u + \delta_{1} v  + (\delta_{2}a) u &= 0.  
\end{align*}
Since \( \{ u,v  \}   \) is linearly independent and \( a \neq 0  \), we get that 
\[  \delta_{1} v + (\delta_{2}a) u = 0   \]
implies \( \delta_{1} = 0  \) and \( \delta_{2}a = 0  \) such that \( \delta_{2} = 0  \). But this implies that \( \{ u+v, au \}  \) is also linearly independent. To show that \( \{ u+v , au \}  \) spans \( V  \), it suffices to show that \( V \subseteq \{ u+v , au \}  \) since the other containment \( \{ u+ v , au  \} \subseteq V  \) follows immediately. Let \( v \in V  \). By Theorem 1.7, we know that adjoining an arbitrary vector \( w \in V  \) but not in \( \{ u+v , au \}  \) creates a linearly dependent set. So, we must have \( w \in \text{span}(\{ u+v , au \} ) \) and thus \( V \subseteq \text{span}(\{ a+v , au \} ) \). 

Now, we want to show that \( \{ au, bv  \}  \) is a basis. Choose scalars \( \delta_{1}, \delta_{2}  \) such that 
\[  \delta_{1} (au) + \delta_{2} (bv) = 0 \tag{2}   \]  
such that \( \delta_{1} = \delta_{2} = 0  \). We can manipulate (2) by rewriting it in the following form: 
\[  (\delta_{1}a) u + (\delta_{2}b) v = 0. \tag{3}  \]
Since \( \{ u,v  \}   \) is a linearly independent set, we know that   \( \delta_{1}a = \delta_{2}b = 0  \). Since \( a,b \neq 0 \), this implies that \( \delta_{1} = \delta_{2} = 0 \). Hence, the representation in (2) is trivial and thus the set \( \{ au, bv  \}  \) is linearly independent. Since adjoining any \( w \in V  \) not in \( \{ au, bv  \}  \) creates a linearly dependent set, we get that \( w \in \text{span}(\{ au,bv \} ) \) by Theorem 1.7. Hence, \( \{ au,bv \}  \) generates \( V  \). 


\end{proof}

\subsubsection{Exercise 1.6.12}
Let \( u,v,  \) and \( w  \) be distinct vectors of a vector space \( V  \). Show that if \( \{ u,v,w \}  \) is a basis for \( V  \), then \( \{ u + v   + w , v + w , w  \}  \) is also a basis for \( V  \). 
\begin{proof}
    First, we prove that \( \{ u+v + w, v + w , w  \}   \) is linearly independent. Choose \( \delta_{1} , \delta_{2} , \delta_{3} \in F  \) such that 
    \[  \delta_{1} (u+v+w) + \delta_{2} (v +w) + \delta_{3} w = 0. \tag{1}  \]
    We can rewrite (1) in the following way:
    \[ (\delta_{1} u + \delta_{2} v + \delta_{3} w ) + \delta_{1} (v+w) + \delta_{2} w  = 0.  \tag{2}.  \]
    Since \( \{ u,v, w \}  \) is also a basis, we know that \( \{ u,v,w  \}   \) is also linearly independent. Hence, \( \delta_{1} = \delta_{2} = \delta_{3} = 0  \). Thus, (1) contains the trivial representation and so \( \{ u + v + w, v + w , w  \}  \) is linearly independent. 

    Now, to prove that \( S = \{ u+v+w, v + w , w  \}   \) generates \( V  \), it suffices to show that \( V \subseteq \text{span}(S) \). Adjoining a vector \( x \in  V  \) but not in \( S  \) produces a linearly independent set. Hence, Theorem 1.7 implies that \( s \in \text{span}(S) \). Hence, \( S  \) generates \( V   \) and that \( S  \) is a basis for \( V  \).

\end{proof}


\subsubsection{Exercise 1.6.19} Complete the proof of Theorem 1.8.
\begin{proof}
See proof in notes.
\end{proof}


\subsubsection{Exercise 1.6.20} Let \( V  \) be a vector space having dimension \( n  \), and let \( S  \) be a subset of \( V  \) that generates \( V  \).
\begin{enumerate}
    \item[(a)] Prove that there is a subset of \( S  \) that is a basis for \( V  \). (Be careful not to assume that \( S  \) is finite.)
        \begin{proof}
        Let \( \text{dim}(V) = n  \). Suppose \( S  \) is a subset of \( V  \) such that \( S  \) generates \( V  \). Then \( S  \) could be either \( S = \{ 0  \}   \) or \( S = \emptyset \). In either case, we find that \( \text{span}(\emptyset) = \{  0  \}  = V  \) or \( \text{span}(\{ 0 \} ) = \{ 0 \}  = V  \). Now, suppose \( S  \) contains a non-zero vector \( u_{1} \). Thus, the set \( \{ u_{1} \}  \) is linearly independent. Suppose we continue adding vectors inductively \( u_{2}, u_{3} , \dots, u_{k }  \) into this set such that this process stops at exactly \( k  \) vectors. We claim that our constructed set 
        \[  L = \{ u_{1}, u_{2}, \dots, u_{k } \} \] 
        is linearly independent for \( k \geq 1 \). Suppose we assume that \( L  \) holds for the \( k  \)th case. We want to show that it also holds for the \( k + 1  \) case. Observe that
        \[  L = \{ u_{1}, u_{2} , \dots, u_{k+1} \} = \{ u_{1}, u_{2}, \dots, u_{k } \} \cup \{ u_{k+1} \}.  \] If \( u_{k+1} = 0  \), then \( L   \) would be linearly dependent. Otherwise \( u_{k+1} \neq 0  \) and so \( \{ u_{k+1} \}  \) is linearly independent. By inductive hypothesis, we also know that \( \{ u_{1}, u_{2}, \dots, u_{k } \}  \) is linearly independent. Since \( \text{span}(\{ u_{1}, u_{2}, \dots, u_{k } \}) \cap \text{span}(\{ u_{k+1} \} ) = \{ 0 \}  \) and that the two sets are disjoint, we know that \( \{ u_{1}, u_{2}, \dots, u_{k } \} \cup \{ u_{k+1} \}   \) is linearly independent. This ends our induction proof. 

    Note that we cannot have \( S \neq L   \) since \( S  \) could be an infinite set. Since \( L  \) is a subset of \( V  \) where \( \text{dim}(V) = n  \), \( L  \) can be extended into a basis for \( V  \) by Corollary 2 of the Replacement Theorem that contains exactly \( n  \) vectors.
        \end{proof}
    \item[(b)] Prove that \( S  \) contains at least \( n  \) vectors.
        \begin{proof}
        Denote the basis constructed from part (a) as \( \beta \). Since \( \beta \) is a basis for \( V  \), \( \beta \) must contain exactly \( n  \) vectors. Since \( \beta \subseteq S  \) and \( S  \) is a generating set for \( V  \), then \( S  \) must contain at least \( n  \) vectors.
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 1.6.21} Prove that a vectors space is infinite-dimensional if and only if it contains an infinite linearly independent subset. 
\begin{proof}
Let \( V  \) be a vector space. For the forwards direction, suppose \( V  \) is an infinite-dimensional vector space. By definition, \( V  \) contains a basis \( \beta \) that is infinite-dimensional. By definition, \( \beta \) is also linearly independent. Thus, \( V \) contains an infinite linearly independent set. 

For the backwards direction, we proceed using the converse. Suppose \( V  \) is a finite-dimensional vector space. Let \( \text{dim}(V) = n  \). By definition, \( V  \) contains a basis \( \beta \) that contains exactly \( n  \) vectors. Since \( \beta \) is also linearly independent, \( \beta \) is a finite linearly independent subset.
\end{proof}


\subsubsection{Exercise 1.6.22} Let \( W_{1} \) and \( W_{2} \) be subspaces of a finite-dimensional vector space \( V  \). Determine the necessary and sufficient conditions on \( W_{1}  \) and \( W_{2} \) so that \( \text{dim}(W_{1} \cap W_{2}) = \text{dim}(W_{1}). \)    
\begin{proof}
We must have \( W_{1} \subseteq W_{2}  \) in order for \( \text{dim}(W_{1} \cap W_{2}) = \text{dim}(W_{1}) \). Let \( W_{1}  \) and \( W_{2} \) be subspaces of a finite dimensional vector space \( V  \). Since \( W_{1}  \) and \( W_{2}  \) are subspaces, we must also have \( W_{1} \cap W_{2} \) as a subspace. Hence, \( W_{1} \cap W_{2}  \) is finite-dimensional by Theorem 1.11. This implies that \( W_{1} \cap W_{2}  \) contains a basis \( \beta \) containing exactly \( \text{dim}(W_{1} \cap W_{2}) \) vectors. Since \( \beta \) is a linearly independent subset of \( W_{1} \), we know that \( \beta \) must contain at most \( \text{dim}(W_{1}) \) vectors. Hence, we have \( \text{dim}(W_{1} \cap W_{2}) \leq \text{dim}(W_{1}) \). Since \( W_{1} \cup W_{2} \), then \( W_{1} \subseteq W_{1} \cap W_{2} \). Since \( W_{1} \) is finite-dimensional, let \( \alpha \) be a basis containing exactly \( \text{dim}(W_{1})  \) vectors. Since \( \alpha \subseteq W_{1} \cap W_{2} \) and \( \alpha \) is a linearly independent set, \( \alpha  \) must contain at most \( \text{dim}(W_{1} \cap W_{2}) \) amount of vectors. Hence, \( \text{dim}(W_{1}) \leq \text{dim}(W_{1} \cap W_{2}) \). Thus, we have \( \text{dim}(W_{1}) = \text{dim}(W_{1} \cap W_{2}) \).

Conversely, we have \( \text{dim}(W_{1} \cap W_{2}) = \text{dim}(W_{1}) \). By Theorem 1.11, we have \( W_{1} \cap W_{2} = W_{1} \). Since \( W_{1} \cap W_{2} \subseteq W_{2} \), we know that \( W_{1} \subseteq W_{2} \).  
\end{proof}

\subsubsection{Exercise 1.6.23} Let \( v_{1}, v_{2}, \dots, v_{k }, v  \) be vectors in a vector space \( V  \), and define \( W_{1} = \text{span}(\{ v_{1}, v_{2}, \dots, v_{k } \} )  \), and \( W_{2} = \text{span}(\{ v_{1}, v_{2}, \dots, v_{k }, v  \} ) \).
\begin{enumerate}
    \item[(a)] Find necessary and sufficient conditions on \( v  \) such that \( \text{dim}(W_{1}) = \text{dim}(W_{2}) \).
        \begin{proof}
            The condition we need is \( v \in W_{1} \). Since \( W_{1} \) and \( W_{2} \) are subspaces, we also have \( W_{1} \cap W_{2} \) is a subspace. Hence, theorem 1.11 tells us that \( W_{1} \cap W_{2} \) is also finite-dimensional. Suppose \( v \in W_{1} \). Since \( v \in W_{2} \) as well, we have that \( W_{1} \subseteq W_{2} \). Now let \( v \in W_{2} \). Then choose scalars \( a_{1}, a_{2} , \dots , a_{k} \) such that 
            \[ a_{1} v_{1} + a_{2} v_{2} + \cdots + a_{n} v_{k } = v.  \]
            But this tells us that \( v \in W_{1} \). So, \( W_{2} \subseteq W_{1} \) and thus \( W_{1} = W_{2} \). By theorem 1.11, \( \text{dim}(W_{1}) = \text{dim}(W_{2})  \).

            Conversely, \( \text{dim}(W_{1}) = \text{dim}(W_{2}) \). Since \( v \in W_{2}  \), this also means that \( v \in W_{1} \) since \( W_{1} = W_{2} \) by theorem 1.11.  
        \end{proof}
    \item[(b)] State and prove a relationship involving \( \text{dim}(W_{1}) \) and \( \text{dim}(W_{2}) \) in the case that \( \text{dim}(W_{1}) \neq \text{dim}(W_{2})  \).
        \begin{proof}
        If \( \text{dim}(W_{1}) \neq \text{dim}(W_{2}) \), then \( v \notin W_{1} \). This is just the contrapositive of the statement above.
        \end{proof}
\end{enumerate}


\subsubsection{Exercise 1.6.24} Let \( f(x) \) be a polynomial of degree \( n \) in \( P_{n}(\R) \). Prove that for any \( g(x) \in P_{n}(\R)  \) there exists scalars \( c_{0}, c_{1}, \dots, c_{n}  \) such that  
\[  g(x) = c_{0} f(x) + c_{1} f^{(1)}(x) + c_{2} f^{(2)}(x) + \cdots + c_{n} f^{(n)}(x), \]
where \( f^{(n)}(x) \) denotes the \( n \)th derivative of \( f(x)  \).

\begin{proof}
    Since \( f \) is differentiable \( n  \) times, we can construct the set 
    \[  W = \{ f(x), f^{(1)}(x) , f^{(2)}(x), \dots, f^{(n)}(x) \}  \] containing \( n + 1  \) polynomials such that no two polynomials contain the same degree (with each derivative of \( f(x) \), the degree decreases by one). Since \( W  \) is a subset of \( P_{n}(\R) \) with no two polynomials having the same degree, we see that following the process seen in example 4 in section 1.5 shows that \( W  \) is a linearly independent set containing \( n + 1  \) vectors. Hence, \( W  \) is a basis for \( P_{n}(\R) \) such that any \( g(x) \in P_{n}(\R) \) by Theorem 1.11. Consequently, \( g(x) \) can be expressed in terms of the vectors in \( W  \) such that  
    \[  g(x) = c_{0} f(x) + c_{1} f^{(1)}(x) + c_{2} f^{(2)}(x) + \cdots + c^{n} f^{(n)}(x)  \]
    for unique scalars \( c_{0}, c_{1}, \dots, c_{n} \) by Theorem 1.8. 
\end{proof}
\subsubsection{Exercise 1.6.29} 
\begin{enumerate}
    \item[(a)] Prove that if \( W_{1}  \) and \( W_{2}  \) are finite-dimensional subspaces of a vector space \( V  \), then the subspace \( W_{1} + W_{2} \) is finite-dimensional, and \( \text{dim}(W_{1} + W_{2}) = \text{dim}(W_{1}) + \text{dim}(W_{2}) - \text{dim}(W_{1} \cap W_{2}) \).
        \begin{proof}
        Let \( W_{1} \) and \( W_{2} \) be subspaces of \( V \). Since \( W_{1}  \) and \( W_{2} \) are finite-dimensional, we also know that \( W_{1} + W_{2}  \) is finite-dimensional. Now, we will show that  
        \[  \text{dim}(W_{1} + W_{2}) = \text{dim}(W_{1}) +  \text{dim}(W_{2}) - \text{dim}(W_{1} \cap W_{2}). \]
        Consider \( W_{1} \cap W_{2} \) and note that \( W_{1} \cap W_{2} \) being finite-dimensional implies that it contains a basis \( \beta_{0} = \{ u_{1}, u_{2}, \dots, u_{k }  \}  \). We can extend \( \beta_{0} \) into a basis for \( W_{1} \) by adding vectors \( v_{1}, v_{2}, \dots, v_{m}  \) into \( \beta_{0} \). Denote this new set as \( \beta_{1} \). Likewise, we add vectors \( w_{1}, w_{2}, \dots, w_{p}   \) into \( \beta_{0}  \) to make a basis \( \beta_{2} \) for \( W_{2} \). We claim that \( \beta = \beta_{0} \cup \beta_{1} \cup \beta_{2} \) is a basis for \( W_{1} + W_{2} \). First, we will show that \( \beta \) is linearly independent. To do this, we need to show that 
        \[  \sum_{ i=1 }^{ k  } a_{i} u_{i} + \sum_{ j=1 }^{ m } b_{j} v_{j} + \sum_{ \ell = 1  }^{ p  } \gamma_{\ell} w_{\ell} = 0. \tag{1} \]
        Subtracting the third term on both sides of (1) produces the following equation:
        \[ \sum_{ j=1 }^{ k  } a_{i} u_{i} + \sum_{ j=1 }^{ m } b_{j} v_{j} = - \sum_{ \ell =1  }^{ p } \gamma_{\ell} w_{\ell}. \]
        Observe that the left-hand side is an element of \( W_{1} \) while the other side is an element of \( W_{2} \). Hence, we know that the term on the right-hand side of (1) is also an element of \( W_{1} \cap W_{2} \). This implies that 
        \[  - \sum_{ \ell = 1  }^{ p } \gamma_{\ell} w_{\ell} = \sum_{ i=1 }^{ k  } \delta_{i} u_{i} \]
        which can be re-written as 
        \[  \sum_{ i=1 }^{ k  } \delta_{i} u_{i} + \sum_{ \ell = 1  }^{ p } \gamma_{\ell} w_{\ell} = 0. \]
        Since \( \beta_{2} \) is a basis for \( W_{2} \), we know that \( \delta_{i} = 0  \) and \( \gamma_{\ell } = 0  \) implying that \( a_{i} = 0  \) and \( \gamma_{\ell} = 0  \). We can re-write (1) in the following form:
        \[  \sum_{ j=1 }^{ k  } a_{i} u_{i} + \sum_{ j=1 }^{ m }b_{j} v_{j} = 0. \tag{2} \]
        Since \( \beta_{1}  \) is a linearly independent set, we get that \( a_{i} = 0  \) and \( b_{j} = 0  \). Hence, (1) contains the trivial-representation which implies that \( \beta = \beta_{0} \cup \beta_{1} \cup \beta_{2}  \) is a linearly independent set.

        Now, we will show that \( \beta \) spans \( W_{1} + W_{2} \). Observe that \( \text{span}(\beta) \subseteq W_{1} + W_{2} \). Now, we will show \( W_{1} + W_{2} \subseteq \text{span}(\beta) \). Suppose we take a vector \( v \in W_{1} + W_{2} \) that is not in \( \beta \) and adjoin this vector in \( \beta \). Note that \( \beta \cup \{ v \}  \) produces a linearly dependent set that by which Theorem 1.7 implies that \( v \in \text{span}(\beta) \) and we are done. Hence, \( \beta \) spans \( W_{1} + W_{2} \) and thus \( \beta \) is a basis.
    
        Note that \( \beta \) contains exactly \( m + p + k  \) vectors. Hence,  denote \( \text{dim}(W_{1} + W_{2}) = m + p + k  \) which can be re-written as 
        \begin{align*}
            \text{dim}(W_{1} + W_{2}) &= m + p + k  \\
                                      &= (k+m) + (k+p) - k \\
                                      &= \text{dim}(W_{1}) + \text{dim}(W_{2}) - \text{dim}(W_{1} \cap W_{2}).
        \end{align*}
        \end{proof}
    \item[(b)] Let \( W_{1}  \) and \( W_{2} \) be finite-dimensional subspaces of a vector space \( V  \), and let \( V = W_{1} + W_{2} \). Deduce that \( V  \) is the direct sum of \( W_{1} \) and \( W_{2} \) if and only if \( \text{dim}(V) = \text{dim}(W_{1}) + \text{dim}(W_{2}) \).
        \begin{proof}
        Suppose \( V  \) is a direct sum of \( W_{1}  \) and \( W_{2} \). Then \( V = W_{1} + W_{2}  \) and \( W_{1} \cap W_{2} = \{ 0 \}  \). We need to show that \( \text{dim}(V) = \text{dim}(W_{1}) + \text{dim}(W_{2}) \). Since \( W_{1} \cap W_{2} = \{ 0  \}  \), we know that it contains the empty set \( \emptyset \) as the basis for \( W_{1} \cap W_{2} \). Hence, \( \text{dim}(W_{1} \cap W_{2}) = 0  \). Using the formula derived in part (a), we can write
        \begin{align*}
            \text{dim}(V) &= \text{dim}(W_{1} + W_{2}) \\
                          &= \text{dim}(W_{1}) + \text{dim}(W_{2}) - \text{dim}(W_{1} \cap W_{2}) \\
                          &= \text{dim}(W_{1}) + \text{dim}(W_{2})
        \end{align*}
        and we are done.

        Conversely, \( \text{dim}(V) = \text{dim}(W_{1}) + \text{dim}(W_{2}) \) implies that \( V = W_{1} + W_{2} \). Using part (a) again, we see that the sum \( \text{dim}(V) = \text{dim}(W_{1}) + \text{dim}(W_{2}) -  0  \) implies that \( \text{dim}(W_{1} \cap W_{2}) = 0  \)  and hence \( W_{1} \cap W_{2}  \) must be equal to the zero set \( \{ 0 \}  \) (which we know by definition that \( \text{span}(\emptyset) = \{ 0 \}  \). Hence, \( V  \) is a direct sum of \( W_{1} \) and \( W_{2} \).
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 1.6.31} Let \( W_{1}  \) and \( W_{2}  \) be subspaces of a vector space \( V  \) having dimensions \( m  \) and \( n \), respectively, where \( m \geq n \).
\begin{enumerate}
    \item[(a)] Prove that \( \text{dim}(W_{1} \cap W_{2}) \leq n \).
        \begin{proof}
            Observe that \( W_{1} \) and \( W_{2} \) being subspaces of \( V  \) implies that \( W_{1} \cap W_{2}  \) is a subspace of \( V  \). Hence, \( W_{1} \cap W_{2} \) is finite-dimensional. Denote \( \text{dim}(W_{1} \cap W_{2}) = k  \) and let \( \beta \) be a basis for \( W_{1} \cap W_{2}  \). Since \( W_{1} \cap W_{2} \subseteq W_{2} \), we know that \( \beta \) must contain at most \( \text{dim}(W_{2}) =  n  \). Hence, \( \text{dim}(W_{1} \cap W_{2}) \leq n  \).
        \end{proof}
    \item[(b)] Prove that \( \text{dim}(W_{1} + W_{2}) \leq m + n  \).
        \begin{proof}
        Using the formula found in part (a) of Exercise 1.3.29, part (a) of this exercise, and \( \text{dim}(W_{1}) \geq \text{dim}(W_{2}) \), we find that 
        \[ \text{dim}(W_{1} + W_{2}) = \text{dim}(W_{1}) + \text{dim}(W_{1}) - \text{dim}(W_{1} \cap W_{2}) \leq \text{dim}(W_{1}) + \text{dim}(W_{2}). \]
        \end{proof}
\end{enumerate}


\subsubsection{Exercise 1.6.33} 
\begin{enumerate}
    \item[(a)] Let \( W_{1} \) and \( W_{2} \) be subspaces of a vector space \( V  \) such that \( V = W_{1} \oplus W_{2} \). If \( \beta_{1}  \) and \( \beta_{2} \) are bases for \( W_{1} \) and \( W_{2} \), respectively, show that \( \beta_{1} \cup \beta_{2} \) is a basis for \( V  \).
        \begin{proof}
            Let \( W_{1}  \) and \( W_{2} \) be subspaces of \( V  \). Assume \( \beta_{1} \) and \( \beta_{2}  \) are bases for \( W_{1} \) and \( W_{2} \) respectively. We need to show that \( \beta_{1} \cap \beta_{2} = \emptyset \) and \( \beta_{1} \cup \beta_{2} \) is a basis for \( V  \). 

            Since \( \beta_{1}  \) and \( \beta_{2} \) contain distinct linearly independent vectors, we must have \( \beta_{1} \cap \beta_{2} = \emptyset \). Since \( V  \) is a direct sum of the \( W_{1} \) and \( W_{2} \), we know that \( W_{1} \cap W_{2} = \{ 0 \}  \) by definition. Since \( \beta_{1}  \) and \( \beta_{2} \) generate \( W_{1} \) and \( W_{2} \) respectively, we must have \( \text{span}(\beta_{1}) \cap \text{span}(\beta_{2}) = \{ 0 \}  \). Now, we have the set \( \beta_{1} \cup \beta_{2}  \) as a linearly independent set by exercise 1.5.21. Observe that \(\text{span}(\beta_{1} \cup \beta_{2})  \subseteq V  \) follows immediately. Now, take any \( v \in V  \) that is not in \( \beta_{1} \cup \beta_{2} \) such that adjoining this vector \( v \in V  \) produces a linearly dependent set. By Theorem 1.7, we have \( v \in \text{span}(\beta_{1} \cup \beta_{2}) \). Thus, we have \( V \subseteq \text{span}(\beta_{1} \cup \beta_{2}) \). Hence, \( \beta_{1} \cup \beta_{2} \) is a generating set for \( V  \) and we are done.
        \end{proof}
    \item[(b)] Conversely, let \( \beta_{1} \) and \( \beta_{2} \) be disjoint bases for subspaces \( W_{1} \) and \( W_{2} \), respectively, of a vector space \( V  \). Prove that if \( \beta_{1} \cup \beta_{2} \) is a basis for \( V  \), then \( V = W_{1} \oplus W_{2} \).
        \begin{proof}
        Let \( \beta_{1} \) and \( \beta_{2} \) be disjoint bases for subspaces \( W_{1} \) and \( W_{2} \) respectively. Suppose \( \beta_{1} \cup \beta_{2} \) is a basis for \( V  \). This tells us that \( \beta_{1} \cup \beta_{2} \) is linearly independent. Thus, \( \text{span}(\beta_{1}) \cap \text{span}(\beta_{2}) = \{ 0 \}  \) and hence \( W_{1} \cap W_{2} = \{ 0 \}  \) since \( \text{span}(\beta_{1}) = W_{1} \) and \( \text{span}(\beta_{2}) = W_{2} \). This tells us that \( \text{dim}(W_{1} \cap W_{2}) = 0   \).  Using the fact that \( \beta_{1} \cup \beta_{2} \) is a basis for \( V  \) that contains exactly \( \text{dim}(W_{1}) + \text{dim}(W_{2}) \), we get that          
        \[ \text{dim}(V) = \text{dim}(W_{1}) + \text{dim}(W_{2}). \]
        By part (b) of Exercise 1.6.29, we get that \( V = W_{1} \oplus W_{2} \).
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 1.6.34} 
\begin{enumerate}
    \item[(a)] Prove that if \( W_{1} \) is any subspace of a finite-dimensional vector space \( V  \), then there exists a subspace \( W_{2} \) of \( V  \) such that \( V = W_{1} \oplus W_{2}  \).
        \begin{proof}
        Since \( W_{1}  \) is a subspace of a finite-dimensional vector space \( V  \), we know that \( W_{1} \) is also finite-dimensional and \( \text{dim}(W_{1}) \leq \text{dim}(V) \) by Theorem 1.11. Thus, let \( \beta  \) be a basis for \( W_{1} \) and let \( \alpha  \) be a basis for \( V  \). Since \( \alpha \) is a generating set consisting of \( \text{dim}(V) \) vectors and \( \beta   \) is a linearly independent subset of \( V  \), we can find a subset \( \sigma  \) of \( \alpha  \) consisting of \( \text{dim}(V) - \text{dim}(W_{1}) \) vectors such that \( \beta \cup \sigma  \) generates \( V  \) by the Replacement Theorem. Suppose \( \sigma  \) is a basis for a subspace of \( V  \) denoted by \( W_{2} \) for which \( \text{dim}(W_{2}) = \text{dim}(V) - \text{dim}(W_{1}) \). Note that \( \beta \cup \sigma \) contains exactly \( \text{dim}(V) \) vectors so it is also a basis for \( V  \) and that \( \beta \cap \sigma = \emptyset \). Hence, \( \text{dim}(V) = \text{dim}(W_{1}) + \text{dim}(W_{2}) \) for which it implies that \( V = W_{1} \oplus W_{2} \).
        \end{proof}
    \item[(b)] Let \( V = \R^{2} \) and \( W_{1} = \{ (a_{1}, 0 ) : a_{1} \in \R  \}   \). Give examples of two different subspaces \( W_{2} \) and \( W'_{2}  \) such that \( V = W_{1} \oplus W_{2} \) and \( V = W_{1} \oplus W'_{2} \).
        \begin{proof}
        \textbf{TO DO.}
        \end{proof}
\end{enumerate}


\subsubsection{Exercise 1.6.35} Let \( W  \) be a subspace of a finite-dimensional vector space \( V  \), and consider the basis \( \beta_{0} =  \{ u_{1}, u_{2}, \dots, u_{k } \}   \) for \( W  \). Let \( \beta_{1} = \{ u_{1}, u_{2}, \dots, u_{k }, u_{k+1}, \dots, u_{n} \}  \) be an extension of this basis to a basis for \( V  \).
\begin{enumerate}
    \item[(a)] Prove that \( \beta_{2} = \{ u_{k+1} + W , u_{k+2} + W, \dots, u_{n} + W  \}  \) is a basis for \( V / W  \).
        \begin{proof}
            To show that \( \beta_{2}  \) is a basis, we need to show that \( \beta_{2} \) is a linearly independent set and a generating set for \( V  \). Observe that 
            \[  \sum_{ j=k+1 }^{ n } \delta_{j} (u_{j} + W) = W  \tag{1} \] 
             for scalars \( \delta_{j}  \) for \( k+1 \leq j \leq n  \). Note that \( W  \), in this case, is the zero vector of \( V / W  \). Hence, (1) implies that  
             \[ \Big[\sum_{ j=k+1 }^{ n  } \delta_{j} u_{j}  \Big] + W = W. \tag{2}      \]
            This implies that 
            \[  \sum_{ j=k+1 }^{ n } \delta_{j} u_{j} \in W. \tag{3}  \]
            Since \( W  \) contains \( \beta_{0}  \) as a basis, we can write
            \[  \sum_{ j=k+1 }^{ n } \delta_{j} u_{j} = \sum_{ i=1 }^{ k }\gamma_{i} u_{i} \]
            for scalars \( \gamma_{i}  \) for all \( 1 \leq i \leq k  \) which can be re-written to 
            \[ \sum_{ j=k+1 }^{ n } \delta_{j} u_{j} - \sum_{ j=1 }^{ k  }\gamma_{i} u_{i} = 0  \]
            where all \( \delta_{j} = 0  \) and \( \gamma_{i} = 0  \) since \( \beta_{1} \) is a basis for \( V  \). Since all \( \delta_{j} = 0 \), we get that \( \beta_{2}  \) is a linearly independent set. 

            To show that \( \beta_{2}  \) is a generating set for \( V / W  \), we need to show that \( \text{span}(\beta_{2}) = V / W  \). Note that the containment \( \text{span}(\beta_{2}) \subseteq V / W  \). To show that other containment, let \( v \in V  \) not in \( \beta_{2} \). Observe that adjoining \( v  \) to \( \beta_{2} \) creates a linearly dependent set. By Theorem 1.7, we have \( v \in \text{span}(\beta_{2}) \). Hence, \( V \subseteq \text{span}(\beta_{2}) \). 

        \end{proof}
    \item[(b)] Derive a formula relating \( \text{dim}(V), \text{dim}(W),  \) and \( \text{dim}(V / W ) \).
        \begin{proof}
            In part (a), we see that \( \beta_{2} \) contains \( n \) amount of vectors. Unioning this set with \( \beta_{1}   \) creates a basis for \( V  \) that contains \( \text{dim}(W) + \text{dim}(V/W) \) vectors. Hence, we must have 
            \[  \text{dim}(V/W) = \text{dim}(V) - \text{dim}(W). \]
        \end{proof}
\end{enumerate}

