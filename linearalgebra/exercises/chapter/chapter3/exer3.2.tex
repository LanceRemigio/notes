\section{The Rank of a Matrix and Matrix Inverses}

\subsection*{Exercise 3.2.3} Prove that for any \( m \times n  \) matrix \( A  \), \( \text{rank}(A) = 0  \) if and only if \( A  \) is the zero matrix.
\begin{proof}
Let \( A  \) be an \( m \times n  \) matrix. Notice that \( \text{rank}(A) =  \text{dim}(R({L}_{A})) \). By the Dimension Theorem, we can see that \( \text{rank}(A) = 0  \) implies that
\[  \text{dim}(F^{n}) = \text{dim}(R({L}_{A})) + \text{dim}(N({L}_{A})) = \text{dim}(N({L}_{A})).  \]
Let \( \beta = \{ {e}_{1}, {e}_{2}, \dots, {e}_{n} \}  \) is the standard ordered basis of \( F^{n} \). Since \( \text{dim}(F^{n}) = \text{dim}(N({L}_{A})) = n \), we know that 
\[  {L}_{A}({e}_{j}) = {a}_{j} = 0 \ \text{ for all } \ 1 \leq j \leq n  \]
where \( {a}_{j} \) is the \( j \)th column of \( A  \). Thus, \( A  \) must be the zero matrix.

Conversely, suppose that \( A  \) is the zero matrix. Then we can see that 
\[   {a}_{j} = 0 \ \text{ for all } \ 1 \leq j \leq n.  \]
Using Theorem 3.5, we can see that
\[  R({L}_{A}) = \text{span}(\{ {a}_{j} : 1 \leq j \leq n  \} ) = \text{span}(\{ 0 \} ). \] 
Hence, we have
\[  \text{rank}(A) = \text{dim}(R({L}_{A})) = \text{dim}(\text{span}(\{ 0 \} )) = 0. \]
Thus, \( \text{rank}(A) = 0  \).
\end{proof}

\subsection*{Exercise 3.2.7} Express the invertible matrix 
\[ 
    \begin{pmatrix}
        1 & 2 & 1 \\
        1 & 0 & 1 \\
        1 & 1 & 2
    \end{pmatrix}
\]
as a product of elementary matrices.
\begin{proof}

\end{proof}

\subsection*{Exercise 3.2.8} Let \( A  \) be an \( m \times n  \) matrix. Prove that if \( c  \) is any nonzero scalar, then \( \text{rank}(cA) = \text{rank}(A) \).
\begin{proof}
Let \( A  \) be an \( m \times n \) matrix. Our goal is to show that
\[  R({L}_{cA}) = R({L}_{A}). \] Let \( y \in R({L}_{cA}) \). Then for some \( x \in F^{n} \), we have that 
\[ y = {L}_{cA}(x) = c {L}_{A}(x) = A(cx) = {L}_{A}(cx).   \] via part (c) of Theorem 2.15. Hence, \( y \in R({L}_{A}) \). Now, let \( y \in R({L}_{A}) \). Then for some \( x \in F^{n} \), we have \( y = {L}_{A}(x) \). But note that this can be re-written in the following form 
\[  y = {L}_{A}(x) = c {L}_{A} \Big( \frac{ 1 }{ c }  x  \Big) = {L}_{cA} \Big( \frac{ 1 }{ c } x \Big)  \]
via part (c) of Theorem 2.15.
Thus, we can see that \( y \in R({L}_{A}) \). This tells us that \( R({L}_{cA}) = R({L}_{A}) \) and that we have
\[  \text{rank}(cA) = \text{dim}(R({L}_{cA})) = \text{dim}(R({L}_{A})) = \text{rank}(A) \]
by theorem 1.11.
\end{proof}
\subsection*{Exercise 3.2.9} Complete the proof of the corollary to Theorem 3.4 by showing that elementary column operations preserve rank.
\begin{proof}
If \( B  \) is obtained from a matrix \( A  \) via an elementary column operation, then there exists an elementary matrix \( E  \) such that \( B = AE  \). Using Theorem 3.2, we can see that \( E  \) is invertible and that 
\[  \text{rank}(B) = \text{rank}(AE) = \text{rank}(A) \] by part (a) of Theorem 3.4.
Hence, elementary column operations preserve rank.
\end{proof}

\subsection*{Exercise 3.2.10} Prove Theorem 3.6 for the case that \( A  \) is an \( m \times 1  \) matrix.
\begin{proof}
Fix \( n = 1  \). Using at most one type 1 row operation and at most one type 2 row operation, we can transform \( A  \) to have a \( 1  \) in the \( 1,1 \) position. By means of at most \( m - 1  \) type 3 column operations, \( A  \) can be transformed into the following matrix
\[  D = \begin{pmatrix} 
           1 \\
           0 \\
           \vdots \\
           0 
          \end{pmatrix} \] where the only linearly independent row is the first row. Hence, \( \text{rank}(D) = \text{rank}(A) = 1  \).
\end{proof}

\subsection*{Exercise 3.2.11} Let
    \[  B = \begin{pmatrix} 
        1 & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & B' &   \\
        0 &    &
              \end{pmatrix} \]
    where \( B'  \) is an \( m \times n \) submatrix of \( B  \). Prove that if \( \text{rank}(B) = r  \), then \( \text{rank}(B') = r - 1  \).
    \begin{proof}
    Let \( \beta = \{ {e}_{1}, {e}_{2}, \dots, {e}_{n} \}  \) be the standard ordered basis of \( F^{n} \). Observe by Theorem 3.5 that
    \[  R({L}_{B}) = \text{span}({L}_{B}(\beta)) =  \text{span}(\{ {b}_{j} : 1 \leq j \leq n  \} ) \tag{1} \]
    where \( {b}_{j} \) is the \( j \)th column of \( B \). Note that
    \[  {L}_{B}(\beta) = \{ {b}_{j} : 1 \leq j \leq n  \} = \{ {b}_{1} \} \cup \{ {b}_{j} : 1 <  j \leq n  \}.   \]
    So, (1) can be re-written as
    \begin{align*}  
    R({L}_{B}) &= \text{span}(\{ {b}_{1} \} \cup \{ {b}_{j} : 1 < j \leq n \}) \\
               &=  \text{span}(\{ {b}_{1} \} ) + \text{span}(\{ {b}_{j} : 1 < j \leq  n \} )
\end{align*}
by Exercise 14 of Section 1.4. Note that \( \{ {b}_{1} \}  \cap \{ {b}_{j} : 1 < j \leq n \}  \) is disjoint. By the formula found in Exercise 29 in Section 1.6, we find that
\begin{align*}
    \text{rank}(B) &= \text{dim}(R({L}_{B})) \\
                   &= \text{dim}(\text{span}({b}_{1}) + \text{span}(\{ {b}_{j} : 1 < j \leq n  \} )) \\
                   &= \text{dim}(\text{span}(\{ {b}_{1} \})) + \text{dim}(\text{span}(\{ {b}_{j} : 1 < j \leq  n \})) \\ 
                   &= 1 + \text{dim}(R({L}_{B'})) \tag{Theorem 3.5} \\
                   &= 1 + \text{rank}(B').
\end{align*}
Solving for \( \text{rank}(B') \), we get our desired result that
\[  \text{rank}(B') = \text{rank}(B) - 1 = r - 1. \]
    \end{proof}

\subsection*{Exercise 3.2.12} Let \( B'  \) and \( D'  \) be \( m \times n  \) matrices, and let \( B  \) and \( D  \) be \( (m + 1) \times (n+1) \) matrices respectively defined by 
\[  B = \begin{pmatrix} 
        1 & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & B' &   \\
        0 &    &
    \end{pmatrix} \ \text{ and } \ D = \begin{pmatrix} 
        1 & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & D' &   \\
        0 &    &
              \end{pmatrix}.   \]
              Prove that if \( B'  \) can be transformed into \( D'  \) by an elementary row [column] operation, then \( B  \) can be transformed into \( D  \) by an elementary row [column] operation.
\begin{proof}
    If \( B'  \) can be transformed into \( D'  \) by an elementary row operation, then there exists an invertible matrix \( E'  \) such that \( D' = E'B'  \). Observe that
    \[ D = \begin{pmatrix} 
        1 & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & D' &   \\
        0 &    &
    \end{pmatrix} =  \begin{pmatrix}
        1 & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & E'B' &   \\
        0 &    &
    \end{pmatrix}.
\]
Now, let's apply an elementary row operation on \( D  \) so that \( D  \) can be written as 
\[  \begin{pmatrix} 
        1 & 0 & \cdots & 0  \\
        0  &  \\
        \vdots & & B' &   \\
        0 &    &
          \end{pmatrix}.  \]
          But this tells us that there exists some invertible matrix \( E  \) such that \[ B = ED.   \tag{1}\] Since \( E  \) is invertible, we can apply its inverse \( E^{-1} \) on the left side of (1) to get that
          \[  D = E^{-1} B. \]
          Thus, \( D  \) can be obtained from \( B  \) via an elementary row operation.
\end{proof}


\subsection*{Exercise 3.2.13} Prove (b) and (c) of Corollary 2 to Theorem 3.6.
\begin{proof}
    Let \( A \in {M}_{m \times n}(F) \) be arbitrary.
    \begin{enumerate}
        \item[(b)] Let \( \gamma = \{ {e}_{1}, {e}_{2}, \dots, {e}_{m} \}  \) be the standard ordered basis for \( F^{m} \). Since \( {L}_{A^{t}}: F^{m} \to F^{n} \) is linear, we can see by Theorem 2.2 that
            \[ R({L}_{A^{t}}) = \text{span}({L}_{A^{t}}(\gamma)) = \text{span}( \{ {L}_{A^{t}}({e}_{j}): 1 \leq j \leq m  \} ).   \]
            Furthermore, we have that \( {L}_{A^{t}}({e}_{j}) = A^{t} {e}_{j} = a^{t}_{j} \) where \( a^{t}_j \) is the \( j \)th column of \( A^{t} \) (or the \( i \)th row of \( A \)). Since \( \text{rank}(A^{t}) = \text{rank}(A)  \) by part (a), we can write that
            \begin{align*}
                \text{rank}(A) = \text{rank}(A^{t}) &= \text{dim}(R({L}_{A^{t}}(\gamma))) \\
                                                    &= \text{dim}(\text{span}(\{ {L}_{A^{t}}({e}_{j}) : 1 \leq j \leq m  \} )).
            \end{align*}
            Thus, the rank of \( A  \) is generated by its rows.
        \item[(c)] Part (b) and Theorem 3.5 tells us that the rows and columns generate subspaces of the same dimension that are numerically equal to the rank of \( A  \).
    \end{enumerate}
\end{proof}

\subsection*{Exercise 3.2.14} Let \( T, U : V \to W  \) be linear transformations.
\begin{enumerate}
    \item[(a)] Prove that \( R(T+U) \subseteq R(T) + R(U)  \). (See the definition of the sum of subsets of a vector space in Section 1.3).
        \begin{proof}
        Let \( y \in R(T+U)  \). Then for some \( x \in V  \), we know that \( (T+U)(x) = y  \). Since \( U  \) and \( T  \) are both linear, we have that
        \[  y = (T+U)(x) = T(x) + U(x). \]
        But note that \( T(x) \in R(T)  \) and \( U(x) \in R(U)  \). Thus, \( y \in R(T) + R(U) \) and so \( R(T+U) \subseteq R(T) + R(U) \).
        \end{proof}
    \item[(b)] Prove that if \( W  \) is finite-dimensional, then \( \text{rank}(T+U) \leq \text{rank}(T) + \text{rank}(U) \).
        \begin{proof}
        Let \( W  \) be a finite-dimensional vector space. Since \( R(T+U)  \) and \( R(T) + R(U)  \) are subspaces of \( W  \), we know that these subspaces are also finite-dimensional vector spaces by Theorem 1.11. By part (a), we can see that
        \begin{align*}
            \text{rank}(T+U)  &= \text{dim}(R(T+U))  \\
                              &\leq \text{dim}(R(T) + R(U)) \\
                              &= \text{dim}(R(T)) + \text{dim}(R(U)) - \text{dim}(R(T) \cap R(U)) \\
                              &\leq \text{dim}(R(T)) + \text{dim}(R(U)) \\
                              &= \text{rank}(T) + \text{rank}(U).
        \end{align*}
        Thus, we have that
        \[  \text{rank}(T+U) \leq \text{rank}(T) + \text{rank}(U). \]
        \end{proof}
    \item[(c)] Deduce from (b) that \( \text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B) \) for any \( m \times n  \) matrices \( A  \) and \( B  \).
        \begin{proof}
        Observer that
        \begin{align*}
            \text{rank}(A+B)   &= \text{rank}({L}_{A+B}) \\
                               &= \text{rank}({L}_{A} + {L}_{B}) \tag{part (c) of Theorem 2.15} \\
                               &\leq \text{rank}({L}_{A}) + \text{rank}({L}_{B}) \tag{part (b)} \\
                               &= \text{rank}(A) + \text{rank}(B).
        \end{align*}
        Hence, we can see that \( \text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B) \).
        \end{proof}
\end{enumerate}

\subsection*{Exercise 3.2.15} Suppose that \( A   \) and \( B  \) are matrices having \( n  \) rows. Prove that \( M(A|B) = (MA|MB) \) for any \( m \times n  \) matrices \( M  \).
\begin{proof}
Let \( A  \) and \( B  \) be \( n \times p  \) and \( n \times \ell  \) matrices, respectively. Suppose \( M  \) be is an arbitrary \( m \times n  \) matrix. Then define the product \( M (A | B ) \) as 
\[  M (A|B) = \sum_{ k =1  }^{ n  } {M}_{ik } (A | B)_{kj} \tag{1} \]
for \( 1 \leq i \leq m  \) and \( 1 \leq j \leq  p + \ell  \). For \( 1 \leq j \leq p  \), we can see that product in (1) can be re-written as
\[  M (A|B) = \sum_{ k= 1  }^{ n } {M}_{ik } {A}_{kj} = MA. \tag{2} \]
For \( p \leq j \leq \ell  \), (1) can be re-written into
\[  M (A|B) = \sum_{ k=1  }^{ n } {M}_{ik } {B}_{kj} = MB. \tag{3} \]
So, with (2) and (3) we can write that
\[  M (A|B) = (MA | MB). \]
\end{proof}

\subsection*{Exercise 3.2.16} Supply the details to the proof of (b) of Theorem 3.4.
\begin{proof}
Observe that
\begin{align*}
   R({L}_{PA}) &= R({L}_{P} {L}_{A}) \\
               &=  {L}_{P} {L}_{A}(F^{n}) \\
               &= {L}_{P} \Big( {L}_{A} (F^{n}) \Big) \\
               &= {L}_{P} \Big(  R( {L}_{A}) \Big). \tag{1}
\end{align*}
Note that \( R({L}_{A})  \) is a subspace of \( F^{m} \). By exercise 17 of Section 2.4, we can see that the invertibility of \( {L}_{P} \) also implies that \( {L}_{P}\Big(R({L}_{A})\Big) \) is also a subspace of \( F^{m} \). Thus, we have that \( \text{dim}(R({L}_{A})) = \text{dim}({L}_{P}({R}({L}_{A}))) \) implies \( R({L}_{A}) = {L}_{P}(R({L}_{A})) \) by Theorem 1.11. So (1) implies that \( R({L}_{PA}) = R({L}_{A})   \) and thus \[ \text{rank}(PA) = \text{rank}(A). \]
\end{proof}

\subsection*{Exercise 3.2.17} Prove that if \( B  \) is a \( 3 \times 1  \) matrix and \( C  \) is a \( 1 \times 3 \) matrix, then the \( 3 \times 4 \) matrix \( BC  \) has rank at most \( 1  \). Conversely, show that if \( A  \) is any \( 3 \times 3  \) matrix having rank \( 1  \), then there exists a \( 3 \times 1  \) matrix \( B  \) and a \( 1 \times 3  \) matrix \( C  \) such that \( A = BC  \).
\begin{proof}

\end{proof}

\subsection*{Exercise 3.2.18} Let \( A  \) be an \( m \times n  \) matrix and \( B  \) be an \( n \times p  \) matrix. Prove that \( AB  \) can be written as a sum of \( n  \) matrices of rank one.
\begin{proof}

\end{proof}

\subsection*{Exercise 3.2.19} Let \( A  \) be an \( m \times n  \) matrix with rank \(  m  \) and \( B  \) be an \( n \times p  \) matrix with rank \( n  \). Determine the rank of \( AB  \). Justify your answer.
\begin{proof}

\end{proof}
