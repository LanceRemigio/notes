\section{Linear Transformations, Null spaces, and Ranges}

\subsubsection{Exercise 2.1.1} Label the following statements as true or false. In each part, \( V  \) and \( W  \) are finite-dimensional vector spaces (over \( F \)), and \( T  \) is a function from \( V  \) to \( W  \).
\begin{enumerate}
    \item[(a)] If \( T  \) is linear, then \( T  \) preserves sums and scalar products.
        \begin{solution}
           True. This is by definition.
        \end{solution}
    \item[(b)] If \( T(x+y) = T(x) + T(y)   \), then \( T  \) is linear.
        \begin{solution}
            False. We also need to have \( T(cx) = cT(x)  \) where \( x \in V  \) and \( c \in F  \) in order for \( T  \) to be a linear map.
        \end{solution}
    \item[(c)] If \( T  \) is one-to-one if and only if the only vector \( x  \) such that \( T(x) = 0  \) is \( x = 0  \).
        \begin{solution}
            True. This is Theorem 2.2.
        \end{solution}
    \item[(d)] If \( T  \) is linear, then \( T(0_{V}) = 0_{W} \).
        \begin{solution}
            True by properties of linear maps.
        \end{solution}
    \item[(e)] If \( T  \) is linear, then \( \text{nullity}(T) + \text{rank}(T) = \text{dim}(W) \).
        \begin{solution}
         False. This only happens when \( V  \) and \( W  \) have equal equal dimensions.   
        \end{solution}
    \item[(f)] If \( T  \) is linear, then \( T  \) carries linearly independent subsets of \( V  \) onto linearly independent subsets of \( W  \).
        \begin{solution}
            False. We need \( T  \) to be injective in order to make this valid.
        \end{solution} 
    \item[(g)] If \( T,U  : V \to W  \) are both linear and agree on a basis for \( V  \), then \( T = U  \).
        \begin{solution}
            True by Corollary to Theorem 2.6.
        \end{solution}
    \item[(h)] Given \( x_{1}, x_{2} \in V  \) and \( y_{1}, y_{2} \in W  \), there exists a linear transformation \( T: V \to W  \) such that \( T(x_{1}) =y_{1}  \) and \( T(x_{2}) =y_{2} \). 
        \begin{solution}
           False. We need to have a basis for \( V  \). 
        \end{solution}
\end{enumerate}

For Exercises 2 through 6, prove that \( T  \) is a linear transformation, and find bases for both \( N(T) \) and \( R(T) \). Then compute the nullity and rank of \( T  \), and verify the dimension theorem. Finally, use the appropriate theorems in this section to determine whether \( T  \) is injective or surjective.

\subsubsection{Exercise 2.1.2} \( T: \R^{3} \to \R^{2}  \) defined by \( T(a_{1}, a_{2}, a_{3}) = (a_{1} - a_{2}, 2a_{3}) \).
\begin{solution}
  Our first goal is to show that \( T  \) is linear. Let \( x,y \in \R^{3} \) and \( c \in \R \) where \( x = (a_{1}, a_{2}, a_{3}) \) and \( y = (b_{1}, b_{2}, b_{3}) \). Then observe that   
  \begin{align*}
      T(cx +y) &= T(ca_{1} + b_{1}, ca_{2} + b_{2}, ca_{3} + b_{3}) \\
               &= ( [ ca_{1} + b_{1}] - [ca_{2} - b_{2}], 2(ca_{3} + b_{3})) \\
               &= ([ca_{1} - ca_{2}] + [b_{1} - b_{2}], 2ca_{3} + 2b_{3}) \\
               &= (c(a_{1} - a_{2}), 2ca_{3}) + (b_{1} - b_{2}, 2b_{3}) \\
               &= c(a_{1} -a_{2}, 2a_{3}) + (b_{1} - b_{2}, 2b_{3}) \\
               &= cT(x) + T(y).
  \end{align*}
  Hence, \( T  \) is linear.

  Now, let's compute the bases for both \( N(T)  \) and \( R(T) \). To compute \( N(T) \), we need to have all the solutions needed for
  \[ T(a_{1}, a_{2}, a_{3}) = (a_{1} - a_{2}, 2a_{3}) = 0   \]
  to hold. Solving the equation above gives us the following solutions
  \begin{center}
      \( a_{1} = a_{2} \) and \( a_{3} = 0  \).
  \end{center}
  Hence, for any \( a \in \R  \) we have
  \[  N(T) = \{ (a,a,0) : a \in \R  \}. \]
  Note that \( N(T)  \) is generated by the linearly independent set \( \{ (1,1,0) \}  \).  Thus, \( \{ (1,1,0)  \}  \) is a basis for \( N(T) \) and contains \( 1  \) vector. So, \( \text{nullity}(T) = 1  \) by the Dimension Theorem. This immediately tells us that \( T \) is not injective since \( \text{nullity}(T) \neq 0  \) and hence \( \text{rank}(T) = 2  \). But \( \text{rank}(T) = \text{dim}(\R^{2}) \). Hence, \( T \) must be surjective. Since \( R(T) = \R^{2} \), the basis is just \( \{ (1,0), (0,1) \}  \).
\end{solution}

\subsubsection{Exercise 2.1.8} \( T: \R^{2} \to \R^{3}  \) defined by \( T(a_{1},a_{2}) = (a_{1} + a_{2}, 0, 2a_{1} - a_{2}) \).
\begin{solution}
   First, we show that \( T  \) is linear.     
\end{solution}



\subsubsection{Exercise 2.1.7} Prove properties \( 1,2,3, \) and \( 4 \) on page 65.
\begin{proof}
See proof in notes.
\end{proof}

\subsubsection{Exercise 2.1.8} Prove that the transformations in Example 2 and 3 are linear.
\begin{proof}
    First we prove that \( T_{\theta}: \R^{2} \to \R^{2} \) defined by 
    \[  T_{\theta}(a_{1}, a_{2}) = (a_{1}\cos \theta - a_{2} \sin \theta, a_{1} \cos \theta + a_{2} \sin \theta) \]
    is linear. Let \( x,y \in \R^{2}  \) defined by \( x = (a_{1}, a_{2}) \) and \( y = (b_{1}, b_{2}) \). Let \( c \in F  \) such that \( cx = (ca_{1}, ca_{2}) \). To make the computation less difficult, we have
    \[  cx + y = (ca_{1} + b_{1}, ca_{2} + b_{2}). \]
    Using the definition \( T_{\theta} \) now, we have that 
    \begin{align*}
        T_{\theta}(cx + y) &= ((ca_{1} + b_{1})\cos \theta - (ca_{2} + b_{2}) \sin \theta,  \\
                           &(ca_{1} + b_{1}) \cos \theta + (ca_{2} + b_{2}) \sin \theta ) \\
                           &= (c (a_{1}\cos \theta  - a_{2} \sin \theta) + (b_{1} \cos \theta - b_{2} \sin \theta ), \\
                           &c ( a_{1} \cos \theta + a_{2} \sin \theta ) + (b_{1} \cos \theta + b_{2} \sin \theta)           ) \\
                           &= c (a_{1} \cos \theta - a_{2} \sin \theta, a_{1} \cos \theta + a_{2} \sin \theta)  \\
                           &+ (b_{1} \cos \theta - b_{2} \sin \theta , b_{1} \cos \theta + b_{2} \sin \theta) \\
                           &= c T_{\theta}(a_{1}, a_{2}) + T_{\theta}(b_{1}, b_{2}) \\
                           &= c T_{\theta}(x) + T_{\theta}(y).
    \end{align*}
    Hence, we get that \( T_{\theta} \) is linear.

    Using the same process, we show that \( T: \R^{2} \to \R^{2}  \) defined by \( T(a_{1}, a_{2}) = (a_{1}, - a_{2})\) is linear. That is, we have
    \begin{align*}
        T(cx + y) &= (ca_{1} + b_{1}, - (ca_{2} + b_{2})) \\
                  &= (ca_{1} + b_{1}, -ca_{2} - b_{2}) \\ 
                  &= c(a_{1}, -a_{2}) + (b_{1}, -b_{2}) \\
                  &= c T(a_{1}, a_{2}) + T(b_{1}, b_{2}) \\
                  &= cT(x) + T(y).
    \end{align*}
    Hence, \( T  \) is also linear.
\end{proof}

\subsubsection{Exercise 2.1.10} Suppose that \( T: \R^{2} \to \R^{2} \) is linear, \( T(1,0) = (1,4) \), and \( T(1,1) = (2,5) \).
\begin{solution}
    First, observe that \( (2,3) \in \R^{2} \) can be expressed in terms of a linear combination of \( (1,0) \) and \( (1,1) \). Hence, we need to find scalars \( a,b \in \R  \) such that
    \[  a(1,0) + b(1,1) = (2,3). \]
    Solving for the scalars gives us the solutions \( a = - 1  \) and \( b = 3  \). Since \( T  \) is linear, we can now compute \( T(2,3)  \) by doing the following:
    \begin{align*}
        T(2,3) &= T(-(1,0) + 3(1,1)) \\
               &= -T(1,0) + 3 T(1,1) \\
               &= - (1,4) + 3(2,5) \\
               &= (5,11).
    \end{align*} 
    Hence, we get that \( T(2,3) = (5,11) \). 

    Note that the domain and codomain have equal finite dimensions. Hence, Theorem 2.5 tells us that \( T  \) must be one-to-one (injective).
\end{solution} 

\subsubsection{Exercise 2.1.11} Prove that there exists a linear transformation \( T: \R^{2} \to \R^{3} \) such that \( T(1,1) = (1,0,2) \) and \( T(2,3) = (1,-1,4) \). What is \( T(8,11)  \)?
\begin{solution}
    Note that \( \beta = \{ (1,0), (0,1) \}  \) is a basis for \( \R^{2} \). By Theorem 2.6, \( T \) being linear implies that there exists a unique linear transformation \( T: \R^{2} \to  \R^{3} \) such that \( T(1,1) = (1,0,2)  \) and \( T(2,3) = (1,-1,4) \). 

    To compute \( T(8,11) \), we need to find scalars \( a,b \in \R  \) such that
    \[  a(1,1) + b(2,3) = (8,11). \]
    Solving for \( a  \) and \( b \) gives us the following linear combination:
    \[ 2(1,1) + 3(2,3) = (8,11). \]
    Since \( T  \) is linear, we find that
    \begin{align*}
       T(8,11) &= T( 2(1,1)  + 3(2,3) ) \\
               &= 2T(1,1) + 3T(2,3) \\
               &= 2(1,0,2) + 3(1,-1,4) \\
               &= (5,-3, 16)
    \end{align*}
\end{solution}


\subsubsection{Exercise 2.1.13} Let \( V  \) and \( W  \) be vector spaces, let \( T: V \to W  \) be linear, and let \( \{ w_{1}, w_{2}, \dots, w_{k } \}  \) be a linearly independent subset of \( R(T) \). If \( S = \{ v_{1}, v_{2}, \dots, v_{k} \}  \) is chosen so that \( T(v_{i}) \) for \( i = 1,2,\dots, k  \) then \( S  \) is linearly independent.
\begin{proof}
    Consider
    \[  \sum_{ i=1 }^{ k  } a_{i} v_{i} = 0 \tag{1}  \]
    for some scalars \( a_{1}, a_{2}, \dots, a_{k } \in F  \). In order to show that \( S  \) is linearly independent, we need to show that \( a_{i} = 0   \) for all \( 1 \leq i \leq k  \). Since \( T  \) is linear, we get that \( T(0) = 0  \) implies
    \[  T \Big( \sum_{ i=1 }^{ k  } a_{i} v_{i} \Big) = 0.  \]
    Since \( T  \) is linear and \( S  \) is chosen so that \( T(v_{i}) = w_{i}  \) for \( 1 \leq i \leq k  \), we get that
    \[  \sum_{ i=1 }^{ k  } a_{i} T(v_{i}) =  0 \iff \sum_{ i=1 }^{ k  } a_{i} w_{i} = 0.   \]
    Since \( \{ w_{1}, w_{2}, \dots, w_{k} \}  \) is linearly independent, we must have \( a_{i} = 0  \) for all \( 1 \leq i \leq k  \). But this tell us that (1) must have the trivial representation. Hence, \( S  \) must also be linearly independent. 
\end{proof}

\subsubsection{Exercise 2.1.14} Let \( V \) and \( W \) be vector spaces and \( T: V \to W  \) be linear.
\begin{enumerate}
    \item[(a)] Prove that \( T \) is injective if and only if \( T  \) carries linearly independent subsets of \( V  \) onto linearly independent subsets of \( W  \).
        \begin{proof}
            (\( \Rightarrow \)) Define \( T: S \to \mathcal{W} \) where \( S  \) and \( \mathcal{W} \) are linearly independent subsets of \( V \) and \( W  \) respectively. Since \( T  \) is injective, we have that \( T  \) is an onto map by Theorem 2.5. 

            (\( \Leftarrow \)) Suppose \( T  \) carries linearly independent subsets of \( V  \) onto linearly independent subsets of \( W  \). Our goal is to show that \( T \) is injective. Suppose  
            \[  T \Big( \sum_{ i=1 }^{ n } a_{i} v_{i}  \Big) = T \Big( \sum_{ i=1 }^{ n } b_{i} v_{i} \Big) \tag{1} \]
            for some scalars \( a_{i}, b_{i}  \) and vectors \( v_{i} \in S  \) for all \( 1 \leq i \leq  k  \). Since \( T  \) is linear and \( T: S \to \mathcal{W}  \) is onto, we can re-write (1) into the following form:
            \[ \sum_{ i=1 }^{ k  } a_{i} T(v_{i}) = \sum_{ i=1 }^{ k  } b_{i} T(v_{i}) \tag{2}  \]
            which manipulating again, we get that
            \[  \sum_{ i=1 }^{ k  } (a_{i} - b_{i}) T(v_{i}) = 0. \tag{3}  \]
            Since \( T(v_{i}) \in W   \) and \( W  \) is linearly independent, we must have \( a_{i} - b_{i} = 0  \) if and only if \( a_{i} = b_{i} \). Hence, we have
            \[  \sum_{ i=1 }^{ n } a_{i} v_{i} = \sum_{ i=1 }^{ n  } b_{i} v_{i}   \]
            and so \( T  \) must be injective.
        \end{proof}
    \item[(b)] Suppose that \( T  \) is injective and that \( S  \) is a subset of \( V  \). Prove that \( S  \) is linearly independent if and only if \( T(S) \) is linearly independent.
        \begin{proof}
            (\( \Rightarrow \)) In order to show that \( T(S)  \) is linearly independent, we must show that 
            \[  \sum_{ i=1 }^{ n } a_{i} T(v_{i}) = 0   \tag{1} \] contains the trivial-representation. Since \( T  \) is linear, we can write (1) into the following form
            \[  T \Big( \sum_{ i=1 }^{ n } a_{i} v_{i} \Big) = 0. \] 
            This implies that
            \[  \sum_{ i=1 }^{ n } a_{i} v_{i} \in N(T). \]
            Since \( T  \) is injective, we know that \( N(T) = \{ 0  \}   \) (by Theorem 2.4), and so we must have
            \[  \sum_{ i=1 }^{ n } a_{i} v_{i} = 0. \tag{2} \]
            But \( v_{i} \in S  \) for \( 1 \leq i \leq k   \) is linearly independent, and so \( a_{i} = 0  \) for all \( 1 \leq i \leq k  \). This tells us that (1) contains the trivial-representation. Hence, \( T(S)  \) is linearly independent.

            (\( \Rightarrow \)) Suppose \( T(S) \) is linearly independent. Then observe that
            \[  \sum_{ i=1 }^{ n } a_{i} T(v_{i}) = 0  \]
        for some scalars \( a_{i}  \) and \( T(v_{i}) \in T(S)  \) for \( 1 \leq i \leq k  \) such that \( a_{i} =0  \). Since \( T  \) is linear and \( T  \) is injective, we can write
        \[  T \Big( \sum_{ i=1 }^{ n } a_{i} v_{i}  \Big) = 0 \iff \sum_{ i=1 }^{ n } a_{i} v_{i} = 0. \]
        Since \( a_{i} = 0  \) and \( v_{i} \in S  \), we also find that \( S  \) is a linearly independent set.
        \end{proof}
    \item[(c)] Suppose \( \beta = \{ v_{1}, v_{2}, \dots, v_{n} \}  \) is a basis for \( V  \) and \( T  \) is injective and surjective. Prove that \( T(\beta) = \{ T(v_{1}), T(v_{2}), \dots, T(v_{n}) \}  \) is a basis for \( W  \). 
        \begin{proof}
            Since \( \beta  \) is a basis for \( V  \), we get that \( \text{span}(T(\beta)) = R(T) \) by Theorem 2.2. Since \( T  \) is surjective, we have \( R(T) = W  \), so \( \text{span}(T(\beta))  = W \). Hence, \( T(\beta) \) generates \( W  \). 
            Since \( \beta \) is a linearly independent subset of \( V  \) and \( T  \) is an injective linear map, we must also have \( T(\beta) \) as a linearly independent subset of \( W \) by part (b).  Hence, \( T(\beta)  \) is a basis for \( W  \).
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 2.1.15} Recall the definition of \( P(\R) \) on page 10. Define 
    \[ T: P(\R) \to P(\R)  \ \text{by} \   T(f(x)) = \int_{ 0 }^{ x }  f(t) \ dt. \]
Prove that \( T  \) is linear and injective, but not surjective.
\begin{proof}
First, we show that \( T: P(\R) \to P(\R)   \) defined by 
\[  T(f(x)) = \int_{ 0 }^{ x }  f(t) \ dt. \]
Let \( cf(x) + g(x)  \in P(\R) \) where \( c \in F  \). Then observe that
\begin{align*}
    T(cf(x) + g(x) ) &= \int_{ 0 }^{ x }  [cf(t) + g(t)] \ dt \\
                     &= \int_{ 0 }^{ x } cf(t) \ dt + \int_{ 0 }^{ x } g(t) \ dt \\
                     &= c \int_{ 0 }^{ x }  f(t) \ dt + \int_{ 0 }^{ x }  g(t) \ dt \\
                     &= c T(f(x)) + T(g(x)).
\end{align*}
Hence, \( T: P(\R) \to P(\R)  \) is a linear map. Let \( f(x) , g(x) \in P(\R) \), then
\begin{align*}
    T(f(x))&= T(g(x)) \\
    \int_{ 0 }^{ x }  f(t) \ dt &= \int_{ 0 }^{ x }  g(t) \ dt \\
    \frac{d  }{d x }  \Big[ \int_{ 0 }^{ x }  f(t) \ dt  \Big] &= \frac{d  }{d x }  \Big[ \int_{ 0 }^{ x }  g(t) \ dt  \Big] \\
    f(x) &= g(x).
\end{align*}
Hence, \( T  \) is an injective map. To see why \( T  \) fails to be surjective, observe that \( 1 \in P(\R) \) but we cannot find a polynomial \( f(x)  \) such that \( T(f(x)) = 1  \); that is, the integration of any polynomial can never yield 1. 
\end{proof}

\subsubsection{Exercise 2.1.16} Let \( T: P(\R) \to P(\R) \) be defined by \( T(f(x)) = f'(x)  \). Recall that \( T  \) is linear. Prove that \( T  \) is surjective, but not injective.
\begin{proof}
    Let \( T: P(\R) \to P(\R) \) be defined by \( T(f(x)) = f'(x) = \frac{d  }{d x } [f(x)]  \). For \( cf(x) + g(x)  \in P(\R)  \) where \( c \in F  \), observe that
    \begin{align*}
        T(cf(x) + g(x)) &= \frac{d  }{d x }  [cf(x) + g(x)] \\
                        &= \frac{d  }{d x } [cf(x)] + \frac{d  }{d x } [g(x)] \\
                        &= c \frac{d  }{d x } [f(x)] + \frac{d  }{d x } [g(x)] \\
                        &= cT(f(x)) + T(g(x)).
    \end{align*}
    Hence, \( T  \) is a linear map. Now, we want to show that \( T  \) is surjective. Define 
    \[  f(x) = \int_{ 0 }^{ x }  g(t)  \ dt. \]
    Then 
    \[  T(f(x)) = \frac{d  }{d x } [f(x)] = \frac{d  }{d x }  \Big[ \int_{ 0 }^{ x }  g(t) \ dt \Big] = g(x) \]
    by the Second Fundamental Theorem of Calculus. Hence, \( T  \) is surjective. 

    To show that \( T  \) is not injective, let \( x^{n} + a, x^{n} + b \in P(\R) \) where \( a, b \in \R  \) such that \( a \neq b   \) and \( n > 0   \). Clearly, we have \( x^{n} + a \neq x^{n} + b  \). But, we have
    \[  T(x^{n} + a) = \frac{d  }{d x }  [x^{n} + a]  = n x^{n-1}\]
    and 
    \[  T(x^{n} + b) = \frac{d  }{d x} [x^{n} + b] = nx^{n-1}.  \]
    Note that \( x^{n} + a \neq x^{n} + b \) yet \( T(x^{n} +a) = T(x^{n} +b)  \). Hence, \( T  \) is not injective.
\end{proof}

\subsubsection{Exercise 2.1.17} Let \( V  \) and \( W  \) be finite-dimensional vector spaces and \( T: V \to W  \) be linear.
\begin{enumerate}
    \item[(a)] Prove that if \( \text{dim}(V) < \text{dim}(W) \), then \( T  \) cannot be surjective.
        \begin{proof}
        Using the Dimension Theorem, we find that 
        \[  \text{rank}(T) = \text{dim}(V) - \text{nullity}(T). \]
        Since \( \text{dim}(V) < \text{dim}(W) \), we find that 
        \[  \text{rank}(T) < \text{dim}(W) - \text{nullity}(T) < \text{dim}(W). \]
        Hence, \( T  \) cannot be surjective in this case.
        \end{proof}
    \item[(b)] Prove that if \( \text{dim}(V) > \text{dim}(W) \), then \( T \) cannot be injective.
        \begin{proof}
        Using the Dimension Theorem again and \( \text{dim}(V) > \text{dim}(W) \), we find that
        \[  \text{nullity}(T) = \text{dim}(V) - \text{rank}(T) > \text{dim}(W) - \text{rank}(T) > 0. \]
        This means that \( \text{nullity}(T)  \) can never be zero, otherwise \( T  \) is injective. Hence, \( T  \) can never be injective if \( \text{dim}(V) > \text{dim}(W) \).
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 2.1.20} Let \( V  \) and \( W \) be vector spaces with subspaces \( V_{1}  \) and \( W_{1}  \), respectively. If \( T:V \to W  \) is linear, prove that \( T(V_{1})  \) is a subspace of \( W  \) and that \( \{ x \in V : T(x) \in W_{1} \}   \) is a subspace of \( V  \).
\begin{proof}
First, we prove that \( T(V_{1}) \) is a subspace of \( W  \). Let \( x,y \in T(V_{1}) \) and \( c \in F  \). Since \( V_{1}  \) is a subspace of \( V  \), we know that \( 0_{V} \in V_{1} \), \( x + y \in V_{1} \), and \( cx \in V_{1} \). Observe that \( T(0_{V}) = 0_{W} \) since \( T \) is linear. Hence, \( 0_{W} \in T(V_{1})\). Let \( x,y \in T(V_{1}) \). Since \( T  \) is linear and \( x + y \in V_{1} \), we have \( T(x+y) = T(x) + T(y) \). Hence, we must have \( x + y \in T(V_{1})  \). Now, \( cx \in V_{1} \) implies \( T(cx) = c T(x)    \). Hence, \( cx \in T(V_{1}) \). This tells us that \( T(V_{1}) \) is a subspace of \( W  \).

Now, we show \( S =  \{ x \in V : T(x) \in W_{1} \}  \) is a subspace of \( V  \). Since \( 0_{W} \in W_{1} \) (because \( W_{1}  \) is a subspace of \( W \)) and \( T \) is linear, we have that \( T(0_{V}) = 0_{W}. \) Hence, \( 0_{V} \in S  \). Now, let \( x,y \in S  \). Hence, \( T(x), T(y) \in W_{1} \) implies \( T(x) + T(y) \in W_{1} \) since \( W_{1} \) is a subspace of \( W \). Since \( T  \) is linear, we have \( T(x) + T(y) = T(x+y) \), and so \( x + y \in S \). Now, let \( c \in F  \) and \( x \in S  \). Again, \( W_{1} \) is a subspace so \( c T(x) \in W_{1} \). Thus, \( T \) being linear implies that \( cT(x) = T(cx)\). Hence, \( cx \in S  \). Thus, \( S  \) is a subspace of \( V  \).
\end{proof}

\subsubsection{Exercise 2.1.21} Let \( V  \) be the vector space of sequences described in Example 5 of Section 1.2. Define the functions \( T,U : V \to V  \) by 
\[  T(a_{1}, a_{2}, \dots ) = (a_{2}, a_{3}, \dots ) \ \  \text{and} \ \ U(a_{1}, a_{2}, \dots ) = (0, a_{1}, a_{2}, \dots).  \]
\( T \) and \( U  \) are called the \textbf{left shift} and \textbf{right shift} operators on \( V  \), respectively. 
\begin{enumerate}
    \item[(a)] Prove that \( T  \) and \( U  \) are linear.
        \begin{proof}
        Let \( (x_{n}), (y_{n}) \in V  \) with \( (x_{n}) = (a_{1}, a_{2}, \dots ) \) and \( (y_{n}) = (b_{1}, b_{2}, \dots ) \). Let \( c \in F  \). Then we have 
        \begin{align*}
            T(cx_{n} + y_{n}) &= (ca_{2} + b_{2}, ca_{3} + b_{3}, \dots ) \\
                              &= (ca_{2}, ca_{3}, \dots ) + (b_{2},  b_{3}, \dots ) \\
                              &= c(a_{2}, a_{3}, \dots ) + (b_{2}, b_{3}, \dots) \\
                              &= c T(x_{n}) + T(y_{n}).
        \end{align*}
        Hence, \( T: V \to V   \) is a linear map.

        Now with \( U: V \to V  \) observe that
        \begin{align*}
            U(cx_{n} + y_{n}) &= (0, ca_{1} + b_{1}, ca_{2} + b_{1}, \dots ) \\
                              &= (0, ca_{1}, ca_{2}, \dots) + (0, b_{1}, b_{2}, \dots) \\
                              &= c(0, a_{1}, a_{2}, \dots ) + (0, b_{1}, b_{2}, \dots ) \\ 
                              &= c U(x_{n}) + U(y_{n}).
        \end{align*}
        Hence, \( U: V \to V   \) is a linear map.
        \end{proof}
    \item[(b)] Prove that \( T \) is surjective, but not injective.
        \begin{proof}
        Let \( \{ b_{n} \}  \in V  \). Define this sequence as 
        \[  \{ b_{n} \}  =  (b_{1}, b_{2}, \dots ). \]
        We can construct a sequence such that \( b_{i} = a_{i+1}  \) for \( i \in \N \). Hence, 
        \[  (b_{1}, b_{2}, \dots)  = (a_{2}, a_{3}, \dots ).\]
        By definition of \( T  \), we get that
        \[  T(a_{1}, a_{2}, \dots ) = (a_{2}, a_{3}, \dots) = (b_{1}, b_{2}, \dots). \]
        Since \( \{ b_{n} \} \in V  \), we get that \( T  \) is surjective. 

        To see why \( T  \) is not injective, suppose we take two sequences distinct \( \{ x_{n} \} , \{ y_{n} \}  \in V  \) defined by
        \begin{center}
            \( \{ x_{n}  \}  = (a, 0, 0 , \dots ) \) and \( \{ y_{n} \}  = (b, 0, 0 , \dots ) \)
        \end{center}
        respectively. But we have \( T(\{ x_{n} \} ) = 0_{n} = T(\{ y_{n} \} ) \) where \( 0_{n}  \) is the sequence with all terms being zero. Hence, \( T  \) cannot be injective.
    \end{proof}
    \item[(c)] Prove that \( U  \) is injective, but not surjective.
        \begin{proof}
        Let \( \{ a_{n} \} , \{ b_{n} \}  \in V  \) defined by 
        \begin{center}
            \( \{ a_{n} \} = (a_{1}, a_{2}, \dots )  \) and \( \{ b_{n} \}  = (b_{1}, b_{2}, \dots) \).
        \end{center}
        Then observe that
        \begin{align*}
           U(a_{1}, a_{2}, \dots ) &= U(b_{1}, b_{2}, \dots ) \\
           (0, a_{1}, a_{2}, \dots ) &= (0, b_{1}, b_{2}, \dots ).
        \end{align*}
        Equating entries, we get that \( a_{i} = b_{i}   \) for all \( i \in \N  \) and so, \( \{ a_{n} \} = \{ b_{n} \}  \). Hence, \( U  \) is injective.

         Observe that \( (x_{1}, 0, 0, \dots ) \in V  \) but there does not exists a sequence \( \{ x_{n} \} \in V  \) such that \( U(x_{1}, x_{2}, \dots ) = (x_{1}, 0, 0 ) \). Hence, \( U  \) is not surjective.
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 2.1.22} Let \( T: \R^{3} \to \R  \) be linear. Show that there exist scalars \( a,b,  \) and \( c  \) such that  \( T(x,y,z) = ax + by + cz \) for all \( (x,y,z) \in \R^{3} \). Can you generalize this result for \( T: F^{n} \to F  \)? State and prove an analogous result for \( T: F^{n} \to F^{m} \).
\begin{proof}
Let \( (x,y,z) \in \R^{3} \) arbitrary. Observe that 
\[  (x,y,z) = (x,0,0) + (0,y,0) + (0,0,z) = x e_{1} + y e_{2} + z e_{3}.  \]
since \( \beta = \{ e_{1}, e_{2}, e_{3} \}  \) is a basis for \( \R^{3} \). Since \( T  \) is linear, we know, by Theorem 2.2, that \( \text{span}(T(\beta)) = R(T) \). So, we have
\begin{align*}
    T(x,y,z) &= T( xe_{1} + ye_{2} + ze_{3}) \\
             &= T(e_{1})x + T(e_{2})y  + T(e_{3})z .
\end{align*}
Since \( T: \R^{3} \to \R  \), we know that \( T(e_{1}), T(e_{2}), T(e_{3}) \in \R  \) are just scalars, so denote \( T(e_{1}) = a, T(e_{2}) = b, T(e_{3}) = c  \). Hence, we have
\[  T(x,y,z) = ax + by + cz. \]

Now for the \( n \)th case, our basis \( \beta  \) for \( F^{n} \) now contains \( n \). So, we have
\[  T(x_{1}, x_{2}, x_{3}, \dots, x_{n} ) = a_{1} x_{1} + a_{2} x_{2} + \dots + a_{n} x_{n}   \]
for some scalars \( a_{1}, a_{2}, \dots ,a_{n} \in F  \) using the same argument above.
\end{proof}

\subsubsection{Exercise 2.1.23} Let \( T: \R^{3} \to \R  \) be linear. Describe geometrically the possibilities for the null space of \( T  \). 
\begin{proof}
    By definition of \( T  \) described in Exercise 2.1.22, the null space is the described as the set
    \[ N(T) = \{ ax + by + cz = 0: a,b,c \in \R \ \text{and} \ (x,y,z) \in \R^{3} \}.   \]
    This represents two vectors in \( \R^{3} \) where we fix \( (x,y,z) \in \R^{3}  \) such that  we find a \( (a,b,c) \in \R^{3} \) such that the two vectors are perpendicular to each other.
\end{proof}


\begin{definition}[Projections]
   Let \( V  \) be a vector space and \( W_{1} \) and \( W_{2}  \) be subspaces of \( V  \) such that \( V = W_{1} \oplus W_{2} \). A function \( T: V \to V  \) is called the \textbf{projection on \( W_{1} \) along \( W_{2} \)} if, for \( x = x_{1} + x_{2} \) with \( x_{1} \in W_{1} \) and \( x_{2} \in W_{2} \), we have \( T(x) = x_{1} \). 
\end{definition}

\subsubsection{Exercise 2.1.24} Let \( T: \R^{2} \to \R^{2} \). Include figures for each of the following parts.
\begin{enumerate}
    \item[(a)] Find a formula for \( T(a,b) \), where \( T  \) represents the projection on the \( y- \)axis along the \( x- \)axis.
        \begin{solution}
        Since \( (a,b) = (a,0) + (0,b)  \) and \( T  \) represents the projection on the \( y- \)axis along the \( x- \)axis, we must have \( T(a,b) = (0,b) \) by definition of projections.
        \end{solution}
    \item[(b)] Find a formula for \( T(a,b)  \), where \( T \) represents the projection on the \( y- \)axis along the line \( L = \{ (s,s) : s \in \R  \}  \).
        \begin{solution}
        Observe that \( (a,b) \in \R^{2} \) can be written in the following form:
        \[  (a,b) = (a,0) + (0,b) = (a, a - a ) + (0,b) = (0,b-a) + (a,a).  \]
        So, \( T(a,b) = (0,b-a) \) by definition of projection.
        \end{solution}
\end{enumerate}

\subsubsection{Exercise 2.1.25} Let \( T: \R^{3} \to \R^{3} \).
\begin{enumerate}
    \item[(a)] If \( T(a,b,c) = (a,b,0)  \), show that \( T  \) is the projection on the \( xy- \)plane along the \( z- \)axis.
        \begin{solution}
        Observe that \( (a,b,c) \in \R^{3} \) can be written in the following form:
        \[  (a,b,c) = \underbrace{(a,b, 0)}_{\in \R^{2}} + \underbrace{(0,0,z)}_{\text{z-axis}}. \tag{1} \]
        Since \( T(a,b,c) = (a,b,0)  \), we know that \( T  \) represents the projection on the \( xy- \)plane along the \( z- \)axis. 
        \end{solution}
    \item[(b)] Find a formula for \( T(a,b,c) \), where \( T  \) represents the projection on the \( z- \)axis along the \( xy- \)plane.
        \begin{solution}
        Let \( (a,b,c) \in \R^{3}  \). Then the projection on the \( z- \)axis along the \( xy- \)plane should be \( T(a,b,c) = (0,0,c) \) since \( (a,b,c) = (a,b,0) + (0,0,c) \) where \( (a,b,0) \in \R^{2} \) and \( (0,0,c)  \) is a point on the \( z- \)axis.
        \end{solution}
    \item[(c)] If \( T(a,b,c) = (a-c,b,0) \), show that \( T  \) is the projection on the \( xy- \)plane along the line \( L = \{ (a,0,a) :  a \in \R  \}  \).
        \begin{solution}
        Let \( (a,b,c) \in \R^{3} \). Then we can write this vector in the following way:
        \begin{align*}  
        (a,b,c) &= (a,b,0) + (0,0,c) \\  
                &=  (a -c + c , b , 0) + (0,0,c) \\ 
                &= (a-c, b , 0 ) + (c, 0 , c ) 
        \end{align*}
        where \( (a-c,b,0) \in \R^{2}  \) and \( (c,0,c) \in L   \). Since \( T(a,b,c) = (a-c,b,0) \), \( T \) must be the projection on the \( xy- \)plane along the line \( L \).  
    \end{solution}
\end{enumerate}

\subsubsection{Exercise 2.1.26} Using the notation in the definition above, assume that \( T: V \to V  \) is the projection on \( W_{1} \) along \( W_{2} \).
\begin{enumerate}
    \item[(a)] Prove that \( T  \) is linear and \( W_{1} = \{ x \in V : T(x) = x  \}  \).
        \begin{proof}
            Let \( x,y \in V  \). We need to show that \( T  \) is linear. Since \( T  \) is the projection on \( W_{1}  \) along \( W_{2} \), we must have \( x = x_{1} + x_{2}  \) and \( y = y_{1} + y_{2} \) with \( T(x) = x_{1}  \) and \( T(y) = y_{1} \) respectively. Observe that
            \begin{align*}
                cx  + y &= c(x_{1} + x_{2}) + (y_{1} + y_{2}) \\
                        &= ( cx_{1} + y_{1} )  + (cx_{2} + y_{2}).
            \end{align*}
            Let \( c \in F  \). Since \( T: V \to V   \) is the projection on \( W_{1} \) along \( W_{2} \), we must have
            \[  T(cx+y) = cx_{1} + y_{1} = c T(x) + T(y).  \]
             Hence, \( T  \) is linear.

            Now, let's show that \( W_{1} = \{ x \in V : T(x) = x  \}  \). Denote \( S = \{ x \in V : T(x) = x  \}  \). Let \( x \in W_{1} \). Since \( T  \) is the projection on \( W_{1} \) along \( W_{2} \) and \( x = x + 0_{V} \), we have \( T(x) = x \). Hence, \( W_{1} \subseteq S   \). Conversely, let \( x \in S \). Then \( T(x)  = x  \). Since \( T  \) is the projection on \( W_{1} \) along \( W_{2} \), this would mean that \( x \in W_{1}  \). Hence, 
            \[  W_{1} = \{ x \in V : T(x) = x  \}.  \]
        \end{proof}
    \item[(b)] Prove that \( W_{1} = R(T)  \) and \( W_{2} = N(T) \).
        \begin{proof}
        Let's show that \( W_{1} = R(T) \). Let \( x \in W_{1} \). Since \( W_{1} \subseteq V  \), we know that \( x \in V  \) where \( x = x + 0_{V} \). Since \( T  \) is the projection on \( W_{1} \) along \( W_{2} \), we have that 
        \( T(x) = x  \). Hence, \( x \in R(T) \). Conversely, \( x \in R(T) \). Then for some \( z \in V  \), we have \( T(x) = z  \). Since \( T  \) is the projection on \( W_{1}  \) along \( W_{2} \), we have that \( x  = z + 0_{V} \) implies that \( z \in W_{1} \). Thus, \( x \in W_{1} \) and so, 
        \[  W_{1} = R(T). \]
      
        Now, let's show that \( W_{2} = N(T) \). Let \( x \in W_{2} \). Since \( T  \) is a projection on \( W_{1} \) along \( W_{2} \), we have \( x = 0_{V} + x \) implies \( T(x) = 0_{V} \). This tell us that \( x \in N(T) \). On the other hand, assume \( x \in N(T) \). Then \( T(x) = 0_{V} \).
    Since \( x \in V  \) and \( W_{1} + W_{2} = V  \), we have \( x = x_{1} + x_{2}  \) with \( x_{1} \in W_{1} \) and \( x_{2} \in W_{2}  \). Since \( T(x) =  0_{V} \), either \( x_{1} = 0  \) or \( x_{2} = 0  \). Assume \( x_{2} = 0 \), then \( x = x_{1} \) and \(x \in W_{1}  \) which is a contradiction. If \( x_{1} = 0  \), then \( x_{2} = x  \), so \( T(x_{2}) = 0_{V}   \). Hence, \( x \in W_{2} \). Thus, 
    \[ W_{2} = N(T). \]
        \end{proof}

    \item[(c)] Describe \( T  \) if \( W_{1} = V  \).
        \begin{solution}
        If \( W_{1} = V  \), then we simply have \( T(x) = x  \) as described in part (a). Moreover, \( N(T) = \{ 0 \}  \) since \( V  = W_{1} + W_{2}  \) implies \( N(T) = W_{2} = \{ 0 \}  \). Thus, \( T \) must be injective. 
        \end{solution}
    \item[(d)] Describe \( T  \) if \( W_{1}  \) is the zero subspace.
        \begin{solution}
        If \( W_{1}  \) is the zero subspace, then \( \text{dim}(W_{1}) = 0  \). This implies that \( \text{dim}(V) = \text{dim}(W_{1} + W_{2}) = \text{dim}(W_{1}) + \text{dim}(W_{2}) + \text{dim}(W_{1} \cap W_{2}) = \text{dim}(W_{2}) \) (note that \( \text{dim}(W_{1} \cap W_{2}) = 0  \)). So, \( \text{dim}(V) = \text{dim}(W_{2})  \) implies that \( V = W_{2} \). Since \( T  \) is a projection on \( W_{1} \) along \( W_{2} \), then \( T(x) = 0_{V}  \) for all \( x \in V  \).
        \end{solution}
\end{enumerate}

\subsubsection{Exercise 2.1.27} Suppose that \( W  \) is a subspace of a finite-dimensional vector space \( V  \).
\begin{enumerate}
    \item[(a)] Prove that there exists a subspace \( W' \) and a function \( T: V \to V  \) such that \( T  \) is a projection on \( W  \) along \( W' \).
        \begin{proof}
        Since \( V  \) is finite-dimensional and \( W  \) is a subspace of \( V  \), we know by Exercise 1.6.34 that there exists a subspace \( W' \) such that \( W_{1} \oplus  W_{2} = V  \). Note that any \( x \in V  \) can defined by \( x = x_{1} + x_{2} \) where \( x_{1} \in W  \) and \( x_{2} \in W' \). Define the function 
        \begin{center}
            \( T: V \to V  \) by  \( T(x) = x_{1} \).
        \end{center} 
        Since \( W + W' = V  \) and \( W \cap W' = \{ 0  \}  \), we get a unique representation of any \( x \in V  \). Thus, we have that \( T \) is well-defined and that \( T  \) is a projection on \( W  \) along \( W' \). 
        \end{proof}
        \begin{remark}
           I am not quite sure how to show that \( T  \) is a projection here. I am not sure if showing that \( T  \) is well-defined is enough to do it.
        \end{remark}
    \item[(b)] Give an example of a subspace \( W  \) of a vector space \( V  \) such that there are two projections on \( W  \) along two (distinct) subspaces.
       \begin{solution}
       
       \end{solution} 
\end{enumerate}

\begin{definition}[Invariance]
    Let \( V  \) be a vector space, and let \( T: V \to V  \) be linear. A subspace \( W  \) of \( V  \) is said to be \textbf{\( T \)-invariant} if \( T(x) \in W  \) for every \( x \in W  \), that is, \( T(W) \subseteq W  \). If \( W  \) is  \( T- \)invariant, we define the \textbf{restriction of \( T \) on \( W  \)} to be the function \( T_{W} : W \to W  \) defined by \( T_{W}(x) = T(x) \) for all \( x \in W  \).
\end{definition}


Exercises 28-32 assume that \( W  \) is a subspace of a vector space \( V  \) and that \( T:V \to V  \) is linear. \textit{Warning:} Do not assume that \( W  \) is \( T- \)invariant or that \( T  \) is a projection unless explicitly stated. 

\subsubsection{Exercise 2.1.28} Prove that the subspaces \( \{ 0 \} , V , R(T),  \) and \( N(T) \) are all \( T- \)invariant.
\begin{proof}
\begin{enumerate}
    \item[(a)] Let \( x \in \{ 0 \}  \). Since \( T  \) is linear, we must have \( x = 0_{V}  \) such that \( T(0_{V}) = 0_{V} \). Hence, \( \{ 0 \}  \) is \( T- \)invariant.
    \item[(b)] Since \( V  \) is a subspace of itself, and \( T(x) \in V  \) for all \( x \in V  \), we have that \( V  \) is \( T- \)invariant. 
    \item[(c)] Let \( x \in R(T) \). Then there exists \( w \in V  \) such that \( T(x) = w \in R(T)  \). Hence, \( R(T) \) is \( T- \)invariant.
    \item[(d)] Let \( x \in N(T) \). Then by definition of \( N(T) \), we have \( T(x) = 0_{V} \in N(T) \). Hence, \( N(T) \) is \( T- \)invariant.
\end{enumerate}
\end{proof}


\subsubsection{Exercise 2.1.29} If \( W  \) is \( T- \)invariant, prove that \( T_{W}  \) is linear.
\begin{proof}
Since \( W  \) is \( T- \)invariant, we have a restriction of \( T  \) on \( W  \) such that \( T_{W} : W \to W   \) is defined by \( T_{W}(x) = T(x)  \) for all \( x \in W  \). Let \( cx + y \in W  \). Since \( T  \) is linear, we know that 
\begin{align*}
    T_{W}(cx+y) &= T(cx+y) \\
                &= cT(x) + T(y) \\
                &= c T_{W}(x) + T_{W}(y).
\end{align*}
Hence, \( T_{W}  \) is linear.
\end{proof}

\subsubsection{Exercise 2.1.30} Suppose that \( T  \) is the projection on \( W  \) along some subspace \( W'  \). Prove that \( W  \) is \( T- \)invariant and that \( T_{W} = I_{W} \).
\begin{proof}
Let \( x \in W  \). Since \( T  \) is the projection on \( W  \) along some subspace \( W' \), we must have \( T(x) = x  \). Since \( x \in W  \), this must imply that \( T(x) \in W   \) as well and so, we have that \( T- \)invariant. In fact, the linearity of \( T  \) and \( W  \) being \( T- \)invariant implies that \( T  \) is linear (by exercise 29) and that 
\[  T_{W}(x) = T(x) = x = I_{W}(x) \]
for any arbitrary \( x \in W  \). Hence, we also have that \( T_{W} = I_{W} \).
\end{proof}

\subsubsection{Exercise 2.1.31} Suppose that \( V = R(T) \oplus W  \) and \( W  \) is \( T- \)invariant. 

\begin{enumerate}
    \item[(a)] Prove that \( W \subseteq N(T) \). 
        \begin{proof}
        
        \end{proof}
    \item[(b)] Show that if \( V  \) is finite-dimensional, then \( W = N(T) \).
        \begin{proof}
        
        \end{proof}
    \item[(c)] Show by example that the conclusion of (b) is not necessarily true if \( V  \) is not finite-dimensional.
        \begin{proof}
        
        \end{proof}
\end{enumerate}
