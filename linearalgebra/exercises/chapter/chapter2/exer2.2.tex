\section{The Matrix Representation of a Linear Transformation}

\subsubsection{Exercise 2.2.1} Label the following statements as true or false. Assume that \( V  \) and \( W  \) are finite-dimensional vector spaces with ordered bases \( \beta \) and \( \gamma \), respectively, and \( T,U : V \to W  \) are linear transformations.
\begin{enumerate}
    \item[(a)] For any scalar \( a \), \( aT + U  \) is a linear transformation from \( V  \) to \( W  \). 
        \begin{solution}
        True by Theorem 2.7.
        \end{solution}
    \item[(b)] \( [T]_{\beta}^{\gamma}  = [U]_{\beta}^{\gamma}  \) implies \( T = U  \).
        \begin{solution}
        True by corollary to Theorem 2.6.
        \end{solution}
    \item[(c)] If \( m = \text{dim}(V) \) and \( n = \text{dim}(W) \), then \( [T]_{\beta}^{\gamma}  \) is an \( m \times n  \) matrix.
        \begin{solution}
         True.
        \end{solution}
    \item[(d)] \( [T+U]_{\beta}^{\gamma}  = [T]_{\beta}^{\gamma}  + [U]_{\beta}^{\gamma} . \) 
        \begin{solution}
        True by Theorem 2.8.
        \end{solution}
    \item[(e)] \( \mathcal{L}(V, W)  \) is a vector space.
        \begin{solution}
        True.
        \end{solution}
\item[(f)] \( \mathcal{L}(V,W) = \mathcal{L}(W,V) \).
        \begin{solution}
        Unless \( V = W  \), this statement is false.
        \end{solution}
\end{enumerate}

\subsubsection{Exercise 2.2.6} Complete the proof of part (b) of Theorem 2.7.
\begin{proof}
See proof in notes.
\end{proof}

\subsubsection{Exercise 2.2.7} Prove part (b) of Theorem 2.8.
\begin{proof}
See proof in notes.
\end{proof}

\subsubsection{Exercise 2.2.8} Let \( V  \) be an \( n \)-dimensional vector space with an ordered basis \( \beta \). Define \( T: V \to F^{n} \) by \( T(x) = [x]_{\beta}^{}  \). Prove that \( T  \) is linear.
\begin{proof}
    Let \( x,y \in V  \). Since \( \beta \) is an ordered basis for \( V  \), we have distinct vectors \( {v}_{1}, {v}_{2}, \dots, {v}_{n} \in \beta \) where
    \[  x = \sum_{ i=1 }^{ n }{a}_{i} {v}_{i} \]
    and
    \[  y = \sum_{ i=1 }^{ n } {b}_{i} {v}_{i} \] for scalars \( {a}_{1}, {a}_{2}, \dots, {a}_{n}  \) and \( {b}_{1}, {b}_{2}, \dots, {b}_{n}  \), respectively. Let \( c \in F  \). Then we have \( cx + y \in V  \) implies
    \[  cx + y = \sum_{ i=1 }^{ n } (c{a}_{i} + {b}_{i}) {v}_{i}. \]
    By definition \( T  \), we must have 
    \begin{align*}
        T(cx+y) &= [cx+y]_{\beta}^{}  \\
                &= \begin{pmatrix}
                    {ca}_{1} + {b}_{1} \\
                    {ca}_{2} + {b}_{2} \\ 
                    \vdots \\
                    {ca}_{n} + {b}_{n}
                \end{pmatrix} \\
                &= c\begin{pmatrix}
                    {a}_{1} \\
                    {a}_{2} \\
                    \vdots \\
                    {a}_{n}
                \end{pmatrix} + \begin{pmatrix}
                    {b}_{1} \\
                    {b}_{2} \\
                    \vdots \\
                    {b}_{n}
                \end{pmatrix} \\
                &= c {[x]}_{\beta} + {[y]}_{\beta} \\
                &= cT(x) + T(y).
    \end{align*}
    Hence, \( T  \) is a linear map.

\end{proof}

\subsubsection{Exercise 2.2.9} Let \( V  \) be the vector space of complex numbers over the field \( \R \). Define \( T: V \to V  \) by \( T(z) = \overline{z} \), where \( \overline{z} \) is the complex conjugate of \( z  \). Prove that \( T  \) is linear, and compute \( [T]_{\beta}^{}  \), where \( \{ 1,i  \} \). (Compare this to {\hyperref[Exercise 2.1.38]{Exercise 2.1.38}} )
\begin{proof}
Let \( cx + y \in \C  \) where \( c \in \R  \) and \( x,y \in \C  \). Observe that 
\begin{center}
    \( x = a + bi  \) and \( y = v + wi \)
\end{center}
for \( a,b,v,w \in \R  \). So,  
\[ cx + y = (ca + v) + (cb + w)i   \]
Furthermore, 
By definition of \( T \) and definition of conjugate, we write
\begin{align*}
    T(cx +y) &= \overline{cx+y} \\
             &= (ca+v) - (cb + w)i \\
             &= c(a - bi) + (v - wi) \\
             &= c \overline{x} + \overline{y} \\ 
             &= c T(x) + T(y).
\end{align*}
Hence, \( T  \) is linear. 
Now, let's compute \( [T]_{\beta}^{}   \) with \( \beta = \{ 1,i \}  \) as our ordered basis for \( \C  \). So, 
\begin{center}
    \( T(1) = 1 = 1 \cdot 1 + 0 \cdot i \) and \( T(i) = -i = 0 \cdot 1 - 1 \cdot i  \).
\end{center}
Hence, 
\[  [T]_{\beta}^{} = \begin{pmatrix}
    1 & 0 \\
    0 & -1 
\end{pmatrix}. \]
\end{proof}

\subsubsection{Exercise 2.2.10} Let \( V  \) be a vector space with the ordered basis \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}.  \) Define \( {v}_{0} = 0  \). By Theorem 2.6, there exists a linear transformation \( T: V \to V  \) such that \( T({v}_{j}) = {v}_{j} - {v}_{j-1}  \) for \( j = 1, 2, \dots, n \). Compute \( [T]_{\beta}^{}  \).
\begin{solution}
For \( 1 \leq j \leq n  \), we see that 
\begin{align*}
    T({v}_{1}) &= {v}_{1} + {v}_{0} = {v}_{1}  \\
    T({v}_{2}) &= {v}_{2} + {v}_{1} \\ 
    T({v}_{3}) &= {v}_{3} + {v}_{2} \\
               &\vdots \\
    T({v}_{n}) &= {v}_{n} + {v}_{n-1}.
\end{align*}
Then we have
\[ {[T]}_{\beta} = \begin{pmatrix}
    1 & 1 & 0 & \cdots & 0 \\
    0 & 1 & 1 & \cdots & 0 \\
    0 & 0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \ddots  & \ddots & \vdots \\
    0 & 0 &  \cdots & \ddots  &    1 \\
    0 & 0 & \cdots & 0 & 1 
\end{pmatrix}.  \]
\end{solution}


\subsubsection{Exercise 2.2.11} Let \( V  \) be an \( n- \)dimensional vector space, and let \( T: V \to V  \) be a linear transformation. Suppose that \( W  \) is a {\hyperref[Invariance]{\( T- \)invariant}} subspace of \( V  \) having dimension \( k  \). Show that there is a basis \( \beta  \) for \( V  \) such that \( [T]_{\beta}^{}   \) has the form 
\[  \begin{pmatrix}
    A & B \\
    O & C 
\end{pmatrix}, \]
where \( A  \) is a \( k \times k  \) matrix and \( O  \) is the \( (n-k) \times k  \) zero matrix.
\begin{proof}
    Let \( \text{dim}(V) = n  \). Since \( W  \) is a subspace of \( V  \), let \( \alpha = \{ {w}_{1}, {w}_{2}, \dots, {w}_{k}   \}  \) be an ordered basis for \( W  \). By corollary to Theorem 1.11, we extend \( \alpha  \) to a basis for \( V  \) by adding distinct and linear independent vectors \( {w}_{k+1}, {w}_{k+2}, \dots, {w}_{n} \). Denote this basis for \( V \) as \( \beta \) with 
    \[  \beta = \{ {w}_{1}, {w}_{2}, \dots, {w}_{n} \}.  \]
    Hence, for \( 1 \leq i \leq n  \) we have
    \[  T({w}_{j}) = \sum_{ i=1 }^{ n } {a}_{ij} {w}_{i} \ \text{ for } 1 \leq j \leq n. \]
    Since \( W  \) is \( T- \)invariant, we know that \( T({w}_{j}) \) for every \( {w}_{j} \in W  \) with \( 1 \leq j \leq k  \). Hence, \( T({w}_{j}) \) for \( 1 \leq j \leq  k  \) can be written as a linear combination of vectors in \( \alpha  \); that is, 
    \[  T({w}_{j}) = \sum_{ i=1 }^{ k } {a}_{ij} {w}_{i} \ \text{ for } 1 \leq j \leq k.  \]
    Since \( \beta  \) is a basis for \( V  \), we know that for \( k +1 \leq i \leq  n  \), we know that
    \[  T({w}_{j}) = \sum_{ i=1 }^{ k  } {a}_{ij} {w}_{i} + \sum_{ i=k+1 }^{ n  } {a}_{ij} {w}_{i} = \sum_{ i=1 }^{ k  } {a}_{ij} {w}_{i} + 0.  \] where \( {a}_{ij} = 0  \) for \( k+ 1 \leq i \leq n  \). Notice that the second term above, represents the O matrix that \( (n-k) \times  k  \). On the other hand, for \( 1 \leq i \leq k  \) and \( 1 \leq j \leq k   \) we can see that \( T: W \to W  \) (since \( W  \) is \( T- \)invariant) can be represented as the matrix \( A  \) that is \( k \times  k  \). Then for \( 1 \leq i \leq n  \) and \( 1 \leq j \leq k  \), we get that
    \[ [T({w}_{j})]_{\beta} = \begin{pmatrix}
        A \\
        O 
    \end{pmatrix}. \]
    Now, if we let \( k + 1 \leq j \leq n \), then we would see that \( T({w}_{j}) \) can be written as 
    \[  T({w}_{j}) = \sum_{ i=1 }^{ n } {a}_{ij} {w}_{i}. \]
Together with \( 1 \leq i \leq n  \) and \( k+1 \leq j \leq n   \) where 
\[  [T({w}_{j})]_{\beta} = \begin{pmatrix}
    T({w}_{k+1}) & T({w}_{k+2}) & \cdots & T({w}_{n})
\end{pmatrix}  \]
which can be denoted with \( B  \) as a \( k \times (n-k)  \) matrix and \( C  \) as a \( (n-k) \times (n-k)  \) matrix where  
    \[ 
        [T({w}_{j})]_{\beta} = \begin{pmatrix}
            B \\
            C
        \end{pmatrix}
    \]
Hence, \( {[T]}_{\beta} \) has the following matrix form
\[  \begin{pmatrix}
    A & B \\
    O & C 
\end{pmatrix}. \]
 \end{proof}

\subsubsection{Exercise 2.2.12} Let \( V  \) be a finite-dimensional vector space and \( T \) be the projection on \( W  \) along \( W' \), where \( W  \) and \( W'  \) are subspaces of \( V  \). Find an ordered basis \( \beta  \) for \( V  \) such that \( [T]_{\beta}^{}   \) is a diagonal matrix.
\begin{solution}
    Let \( \text{dim}(V) = n  \) since \( V \) is a finite-dimensional vector space. Since \( W \subseteq V  \) is a subspace, we know that \( W  \) must be finite-dimensional as well. Let \( \text{dim}(W) = k  \) and let \( \lambda = \{ {w}_{1}, {w}_{2}, \dots, {w}_{k } \}  \) be a basis for \( W  \). By corollary to Theorem 1.11, we can extend \( \lambda  \) to be a basis for \( V  \) by adding \( {w}_{k+1}, {w}_{k+2}, \dots, {w}_{n} \in W  \) into \( \lambda \). Denote this ordered basis for \( V  \) as \( \beta  \) where 
    \[  \beta = \{ {w}_{1}, {w}_{2}, \dots, {w}_{n} \}. \]
    Since \( W  \) is a \( T: V \to V  \) is a projection on \( W  \) along \( W' \) (\( W' \) is also a subspace), then
    \[ {w}_{j} =  T({w}_{j}) = \sum_{ i=1 }^{ n } {a}_{ij} {w}_{i} \ \text{ for } 1 \leq j \leq n  \]
    for every \( {w}_{j} \in W  \). This tells us that each \( {w}_{j}  \) can be expressed a linear combination if and only if \( {a}_{ij} = 1  \) whenever \( i = j  \) and \( {a}_{ij} = 0  \) otherwise. Hence, \( [T]_{\beta}^{}  \) is a diagonal matrix.  
\end{solution}

\subsubsection{Exercise 2.2.13} Let \( V  \) and \( W  \) be vector spaces, and let \( T  \) and \( U  \) be nonzero linear transformations from \( V  \) to \( W  \). If \( R(T) \cap R(U) = \{ 0  \}   \), prove that \( \{ T, U  \}   \) is a linearly independent subset of \( \mathcal{L}(V, W ) \).
\begin{proof}
Let \( T \) and \( U  \) be nonzero linear transformations from \( V  \) to \( W  \). For \( a,b \in F  \), we need to show that 
\[  aT + bU = {T}_{0} \] with \( a  \) and \( b  \) both zero. Note that \( {T}_{0} \) is the zero linear transformation. Let \( x \in V  \). Then we have
\begin{align*}
    (aT + bU)(x) &= {T}_{0}(x)  \\
    (aT)(x) + (bU)(x) &= 0. 
\end{align*}
Hence, we have
\[  (aT)(x) = -(bU)(x).  \]
Since \( (aT)(x) \in R(T) \cap R(U) \) where \( R(T) \cap R(U) = \{ 0  \}   \) by assumption, we get that
\[  (aT)(x) = 0 \iff aT(x) = 0. \]
Since \( T(x) \neq 0  \), we must have that \( a = 0  \) when dividing \( T(x)  \) on both sides on the equation above. This also implies that \( b = 0  \) since \( U(x) \neq 0  \) and so \( \{ T, U  \}  \) must be linearly independent.
\end{proof}

\subsubsection{Exercise 2.2.14} Let \( V = P(\R) \), and for \( j \geq 1   \) define \( {T}_{j}(f(x)) = f^{(j)}(x)  \), where \( f^{(j)}(x) \) is the \( j \)th derivative of \( f(x) \). Prove that the set \( \{ {T}_{1}, {T}_{2}, \dots, {T}_{n} \}  \) is a linearly independent subset of \( \mathcal{L}(V) \) for any positive integer \( n \).
\begin{proof}
Let \( j \geq 1  \). We want to show that for scalars \( {a}_{1}, {a}_{2}, \dots, {a}_{n} \in \R  \), we have
\[  {a}_{1} {T}_{1} + {a}_{2} {T}_{2} + \cdots + {a}_{n} {T}_{n} = {T}_{0}. \]
Let \( f(x) \in P(\R) \) be arbitrary. Since each \( {T}_{j} \) is linear, we have that
\[  {a}_{1} {T}_{1}(f(x)) + {a}_{2} {T}_{2}(f(x)) + \cdots + {a}_{n} {T}_{n}(f(x)) = 0 \]
which can re-written to be
\[  {a}_{1} f^{(1)}(x) + {a}_{2} f^{(2)}(x) + \cdots + {a}_{n} f^{(n)}(x) = 0 \tag{1} \]
with each \( f^{(j)}(x) \) being the \( j \)th derivative of \( f(x) \). Since no two polynomials in the set \( \{ f^{(1)}(x), f^{(2)}(x), \dots, f^{(n)}(x) \}  \) have the same degree, we know that the representation in (1) contains the trivial solution; that is, \( {a}_{n} = 0  \) for any \( n \in \N   \). Hence, the set \[  \{ {T}_{1}, {T}_{2}, \dots, {T}_{n} \}  \] must be linearly independent by Exercise 1.5.18.
% Correct the proof in Exercise 1.5.18.
\end{proof}

\subsubsection{Exercise 2.2.15} Let \( V  \) and \( W  \) be vector spaces, and let \( S  \) be a subset of \( V  \). Define \( S^{0} = \{ T \in \mathcal{L}(V,W) : T(x) = 0 \ \text{ for all } x \in S  \}  \). Prove the following statements.
\begin{enumerate}
    \item[(a)] \( S^{0}  \) is a subspace of \( \mathcal{L}(V,W) \).
        \begin{proof}
        \begin{enumerate}
            \item[(i)] Note that \( {T}_{0} \in S^{0} \) since \( {T}_{0}(x) = 0  \) for all \( x \in S  \).
            \item[(ii)] Let \( T, U \in S^{0} \). Then \( T(x) = 0  \) and \( U(x) = 0  \) for all \( x \in S  \). Then 
                \[ (T+U)(x) = T(x) + U(x) = 0 + 0 = 0. \]
                So \( T + U \in S^{0} \).
            \item[(iii)] Let \( c \in F  \) and \( T \in S^{0} \). Then \( T(x) = 0  \) for all \( x \in S  \). Thus, 
                \[  (cT)(x) = c T(x) = c \cdot 0 = 0. \]
                So, \( cT \in S^{0} \).
        \end{enumerate} 
        Hence, \( S^{0} \) is a subspace of \( \mathcal{L}(V,W) \).
        \end{proof}
    \item[(b)] If \( {S}_{1} \) and \( {S}_{2} \) are subsets of \( V  \) and \( {S}_{1} \subseteq {S}_{2} \), then \( {S}_{2}^{0} \subseteq {S}_{1}^{0} \).
        \begin{proof}
        Let \( {x}_{1} \in {S}_{1} \). Since \( {S}_{1} \subseteq {S}_{2}  \), we have that \( {x}_{1} \in {S}_{2} \). If \( T \in {S}_{2}^{0}  \), then \( T({x}_{1}) = 0  \) for \( {x}_{1} \in {S}_{2} \). Since \( {x}_{1} \in {S}_{1} \), we must also have \( T \in {S}_{1}^{0}  \). Hence, \( {S}_{2}^{0} \subseteq {S}_{1}^{0}\).
        \end{proof}
    \item[(c)] If \( {V}_{1} \) and \( {V}_{2} \) are subspaces of \( V  \), then \( ({V}_{1} + {V}_{2})^{0} = {V}_{1}^{0} \cap V_{2}^{0} \).
        \begin{proof}
        Since \( {V}_{1} + {V}_{2}  \) and \( {V}_{1} \cap {V}_{2} \) are subsets of \( V  \) and that \(  {V}_{1} \cap {V}_{2} \subseteq {V}_{1} + {V}_{2}  \), we know that \( ({V}_{1} + {V}_{2})^{0} \subseteq {V}_{1}^{0} \cap {V}_{2}^{0} \).

        Now, let \( T \in {V}_{1}^{0} \cap V_{2}^{0} \). Then \( T \in {V}_{1}^{0} \) and \( T \in {V}_{2}^{0} \) implies \( T({x}_{1}) = 0  \) and \( T({x}_{2}) = 0  \) for all \( {x}_{1} \in {V}_{1} \) and \( {x}_{2} \in {V}_{2} \). Since \( T  \) is linear, we must have 
        \[   0 = 0 + 0 =  T({x}_{1}) + T({x}_{2}) = T({x}_{1} + {x}_{2}).  \]
        Hence, \( T \in ({V}_{1} + {V}_{2})^{0}  \) and so \( {V}_{1}^{0} \cap V_{2}^{0} \subseteq ({V}_{1} + {V}_{2})^{0} \). Thus, 
        \[ ({V}_{1} + {V}_{2})^{0} = {V}_{1}^{0} \cap {V}_{2}^{0}. \]
        \end{proof}
\end{enumerate}

\subsubsection{Exercise 2.2.16} Let \( V  \) and \( W  \) be vector spaces such that \( \text{dim}(V) = \text{dim}(W) \), and let \( T: V \to W  \) be linear. Show that there exists ordered bases \( \beta  \) and \( \gamma \) for \( V  \) and \( W  \), respectively, such that \( [T]_{\beta}^{\gamma}  \) is a diagonal matrix.
\begin{proof}
    Using the same process found in the dimension theorem, let \( \lambda = \{ {v}_{1}, {v}_{2}, \dots, {v}_{k } \}   \) be a basis for \( N(T) \). We can extend \( \lambda  \) into a basis for \( V  \) by adding distinct linearly independent vectors \( {v}_{k+1}, {v}_{k+2}, \dots, {v}_{n}   \) into \( \lambda \). Denote this new basis as \( \beta  \) where
\[  \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}. \]
Since \( \text{dim}(V) = \text{dim}(W) \), we get that
\[  \text{dim}(W) = \text{dim}(N(T)) + \text{dim}(R(T)). \tag{1} \] 
Note that \( \{ T({v}_{k+1}), T({v}_{k+2}), \dots, T({v}_{n}) \}  \) is a basis for \( R(T) \) which can be extended to be a basis for \( W  \). Hence, we have \( \gamma \) defined by
\[  \gamma = \{ T({v}_{1}), T({v}_{2}) , \dots,  T({v}_{n})\}  \]
with \( T({v}_{i}) = {w}_{i} \) for \( 1 \leq i \leq n \) to a basis for \( W  \). Now, note that for \( 1 \leq j \leq n \), we have 
\[  T({v}_{j}) = \sum_{ i=1 }^{ n } {a}_{ij} {w}_{i} = \sum_{ i=1 }^{ k  } {a}_{ij} {w}_{i} + \sum_{ i=k+1 }^{ n }{a}_{ij} {w}_{i}.  \]  
By definition of \( N(T) \), we know that for \( 1 \leq j \leq k  \) and \( 1 \leq i \leq k  \) that  
\[  0 = \sum_{ i=1 }^{ k  } {a}_{ij} {w}_{i}. \]
For \( k +1 \leq i \leq  n \) and \( k + 1 \leq j \leq n  \), 
\[  {w}_{j} = T({v}_{j}) = \sum_{ i=k+1 }^{ n } {a}_{ij} {w}_{i} \] so we have \( {a}_{ij} = 1  \) whenever \( i = j  \) and \( {a}_{ij} = 0  \) otherwise. So, the matrix representation of \( T  \) can be written as 
\[  [T]_{\beta}^{\gamma}  = \begin{pmatrix}
    O & O \\
    O & I 
\end{pmatrix} \] where \( I  \) is the \( (n-k) \times (n-k ) \) matrix. Thus, we can see that \( [T]_{\beta}^{\gamma}  \) is a diagonal matrix. 
\end{proof}

