\section{The Change of Coordinate Matrix}

\subsection*{Exercise 2.5.8} Prove the following generalization of Theorem 2.23. Let \( T : V \to W  \) be a linear transformation from a finite-dimensional vector space \( V  \) to a finite-dimensional vector space \( W  \). Let \( \beta  \) and \( \beta'  \) be ordered bases for \( V  \), and let \( \gamma  \) and \( \gamma'  \) be ordered bases for \( W  \). Then \(  [T]_{\beta'}^{\gamma'} = P^{-1} [T]_{\beta}^{\gamma} Q  \) where \( Q  \) is the matrix that changes \( \beta'- \)coordinates into \( \beta- \)coordinates and \( P  \) is the matrix that changes \( \gamma'-  \)coordinates into \( \gamma- \)coordinates. 
\begin{proof}
Our goal is to show that 
\[  P [T]_{\beta'}^{\gamma'} = [T]_{\beta}^{\gamma} Q \tag{1} \]
where \( P = [{I}_{W}]_{\gamma'}^{\gamma}  \) and \( Q = [{I}_{V}]_{\beta'}^{\beta}  \). Using Theorem 2.11, we can write that
\begin{align*}
    P [T]_{\beta'}^{\gamma'} = [{I}_{W}]_{\gamma'}^{\gamma} [T]_{\beta'}^{\gamma'}  &= [{I}_{W} T]_{\beta'}^{\gamma}  \\
                             &= [T]_{\beta'}^{\gamma}  \\
                             &= [T {I}_{V}]_{\beta'}^{\gamma}  \\
                             &= [T]_{\beta}^{\gamma} [{I}_{V}]_{\beta'}^{\beta} \\
                             &= [T]_{\beta}^{\gamma} Q.
\end{align*}
Hence, we have
\[  P [T]_{\beta'}^{\gamma'} = [T]_{\beta}^{\gamma} Q. \]
Since \( P  \) is invertible (by Theorem 2.22), we can do a left-multiplication of \( P^{-1} \) on both sides of (1) to get our desired result
\[  [T]_{\beta'}^{\gamma'} = P^{-1} [T]_{\beta}^{\gamma} Q. \]
\end{proof}

\subsection*{Exercise 2.5.9} Prove that "is similar to" is an equivalence relation on \( {M}_{n \times n }(F) \).
\begin{proof}
Let \( A \in {M}_{n \times n }(F)  \). Note that
\[  AI = IA = A  \] and that \( I   \) is invertible. Thus, we have
\[  A = I^{-1} A I \]
and that \( A \sim A  \). 

Let \( A, B \in {M}_{n \times n }(F) \). Suppose \( A \sim B  \). Then there exists an invertible matrix \( Q  \) such that 
\[  A = Q^{-1} B Q. \] Using the invertibility of \( Q  \), we get our desired result
\[  B = Q A Q^{-1} = P^{-1} A P  \]
where \( Q = P^{-1} \) and \( Q^{-1} = P  \). Hence, \( B \sim A  \).

Now, let \( A, B , C \in {M}_{n \times n}(F) \). Suppose \( A \sim B  \) and \( B \sim C  \). We want to show that \( A  \sim C  \); that is, we want to show that there exists an invertible \( Q \in {M}_{n \times n}(F)  \) such that
\[ A = Q^{-1} C Q.   \]
Since \( A \sim B  \), there exists an invertible matrix \( P  \) such that
\[ A = P^{-1} B P.   \]
Likewise, \( B \sim C  \) implies that there exists an invertible matrix \( L  \) such that
\[  B = L^{-1} C L. \]
Using {\hyperref[Exercise 2.4.4]{Exercise 2.4.4}}, we can write that
\begin{align*}
    A &= P^{-1} B P  \\
      &= P^{-1} (L^{-1} C L) P \\ 
      &= (LP)^{-1} C (LP) \\
      &= Q^{-1} C Q
\end{align*}
where \( (LP)^{-1} = Q^{-1} \) and \( LP = Q  \). Hence, \( A \sim C  \).
\end{proof}

\subsection*{Exercise 2.5.10}\label{Exercise 2.5.10} Prove that if \( A  \) and \( B  \) are similar \( n \times n  \) matrices, then \( \text{tr}(A) = \text{tr}(B) \).
\begin{proof}
    Suppose \( A  \) and \( B  \) are similar \( n \times n \) matrices. Then there exists an invertible matrix \( Q  \) such that
    \[  A = Q^{-1} B Q. \]
    Using the fact that \( \text{tr}(AB) = \text{tr}(BA) \) and that matrix multiplication is associative, we can write that
    \begin{align*}
        \text{tr}(A) &= \text{tr}((Q^{-1}B) Q) \\
                     &= \text{tr}(Q (Q^{-1}B)) \\
                     &= \text{tr}((Q Q^{-1})B) \\ 
                     &= \text{tr}({I}_{n}B ) \\
                     &=  \text{tr}(B). 
    \end{align*}
    Hence, we have that \( \text{tr}(A ) = \text{tr}(B) \).
\end{proof}

\subsection*{Exercise 2.5.11} Let \( V  \) be a finite-dimensional vector space with ordered bases \( \alpha, \beta  \) and \( \gamma \).
\begin{enumerate}
    \item[(a)] Prove that if \( Q  \) and \( R  \) are the change of coordinate matrices that change \( \alpha- \)coordinates into \( \beta- \)coordinates and \( \beta- \)coordinates into \( \gamma- \)coordinates, respectively, then \( RQ  \) is the change of coordinate matrix that changes \( \alpha-  \)coordinates into \( \gamma- \)coordinates. 
        \begin{proof}
        Since \( Q  \) is the matrix that changes \( \alpha-\) coordinates into \( \beta- \)coordinates, we have \( Q = [{I}_{V}]_{\alpha}^{\beta}  \). Likewise, \( R  \) is the matrix that changes \( \beta - \)coordinates into \( \gamma-\) coordinates. So, \( R = [{I}_{V}]_{\beta}^{\gamma}  \). Thus, we have
        \begin{align*}
            RQ &= [{I}_{V}]_{\alpha}^{\beta} [{I}_{V}]_{\beta}^{\gamma}  \\
               &= [{I}_{V}]_{\alpha}^{\gamma} 
        \end{align*}
        by Theorem 2.11. Thus, \( RQ  \) is the change of coordinate matrix that changes \( \alpha-  \)coordinates into \( \gamma- \)coordinates
        \end{proof}
    \item[(b)] Prove that if \( Q  \) changes \( \alpha- \)coordinates into \( \beta- \)coordinates, then \( Q^{-1} \) changes \( \beta-  \)coordinates into \( \alpha- \)coordinates.
        \begin{proof}
            Suppose \( Q  \) changes \( \alpha- \)coordinates into \( \beta- \)coordinates, we have for any \( v \in V  \) that
            \[  [v]_{\alpha} = Q [v]_{\beta}. \tag{1} \]
            Since \( Q  \) is invertible, we have
            \begin{align*} Q^{-1} (Q [v]_{\alpha}) = Q^{-1} [v]_{\alpha} &\implies (Q^{-1}Q) [v]_{\beta} = Q^{-1} [v]_{\alpha} \\
            &\implies [v]_{\beta} = Q^{-1} [v]_{\alpha} \\
            \end{align*}
            Hence, \( Q^{-1} \) changes \( \beta- \)coordinates into \( \alpha - \)coordinates.
        \end{proof}
\end{enumerate}

\subsection*{Exercise 2.5.12} Prove the corollary to Theorem 2.23.
\begin{proof}
Observe that part (a) of Theorem 2.15 implies that \( [{L}_{A}]_{\gamma} = A  \). Note that \( Q = [{I}_{F^{n}}]_{\gamma} \) and that \( {I}_{F^{n}}{L}_{A} = {L}_{A} {I}_{F^{n}}  \) so we write
\begin{align*}
    Q [{L}_{A}]_{\gamma} &= [{I}_{F^{n}}]_{\gamma} [{L}_{A}]_{\gamma}  \\
                         &= [{I}_{F^{n}} {L}_{A}]_{ \gamma} \\
                         &=  [{L}_{A} {I}_{F^{n}}]_{\gamma} \\
                         &=  [{L}_{A}]_{\gamma} [{I}_{F^{n}}]_{\gamma} \\
                         &= AQ.
\end{align*} 
Hence, we have \[ Q [{L}_{A}]_{\gamma} = AQ \tag{1} \]. Since \( Q  \) is invertible, we can do left-multiplication of \( Q^{-1} \) on both sides of (1) to get our desired result
\[  [{L}_{A}]_{\gamma} = Q^{-1} A Q. \]
\end{proof}

\subsection*{Exercise 2.5.13} Let \( V  \) be a finite-dimensional vector space over a field \( F  \), and let \( \beta = \{ {x}_{1}, {x}_{2}, \dots, {x}_{n} \}  \) be an ordered basis for \( V  \). Let \( Q  \) be an \( n \times n \) invertible matrix with entries from \( F  \). Define 
\[  {x}_{j}' = \sum_{ i=1  }^{ n  } {Q}_{ij} {x}_{i} \ \text{ for } \ 1 \leq j \leq n, \]
and set \( \beta' = \{ {x}_{1}', {x}_{2}', \dots, {x}_{n}' \}  \). Prove that \( \beta'  \) is a basis for \( V  \) and hence that \( Q  \) is the change of coordinate matrix changing \( \beta'-  \)coordinates into \( \beta- \)coordinates.
\begin{proof}
Let \( 1 \leq j \leq n \). First, we need to show that \( \beta'  \) is linearly independent; that is, there exists scalars \( {a}_{1}, {a}_{2}, \dots, {a}_{n}  \) such that
\[  \sum_{ j=1  }^{ n } {a}_{j} {x}_{j} = 0 \tag{1}  \]
    where \( {a}_{j} = 0   \) for all \( j\).
Since   
\[  {x}_{j}' = \sum_{ i=1 }^{ n } {Q}_{ij} {x}_{i}, \]
we can re-write the left side of (1) into
\[  \sum_{ j=1  }^{ n } {a}_{j} \Big( \sum_{ i=1 }^{ n } {Q}_{ij} {x}_{i} \Big) = \sum_{ i=1  }^{ n } \Big( \sum_{ j=1 }^{ n } {a}_{j} {Q}_{ij} \Big) {x}_{i}.   \]
But since \( \beta = \{ {x}_{1}, {x}_{2}, \dots, {x}_{n} \}   \) is linearly independent, we have that
\[  \sum_{ j=1  }^{ n } {a}_{j} {Q}_{ij} = 0 \iff {a}_{j} Q = O \]
where \( O  \) is the zero-matrix. Since \( Q  \) is an invertible \( n \times n  \) matrix, we can multiply \( Q^{-1} \) on both sides of \( {a}_{j} Q = O  \), to write
\begin{align*}
    ({a}_{j}Q) Q^{-1} &= {a}_{j} (Q Q^{-1}) \\
                      &= {a}_{j} {I}_{n}
\end{align*}
Thus, \( {a}_{j} {I}_{n} = O  \) implies that \( {a}_{j} = 0  \) for all \( 1 \leq j \leq n \). Hence, \( \beta'  \) is linearly independent. Now, let \( v \in V   \) but not in \( \beta'  \). Then adjoining \( v  \) into \( \beta'  \) will produce a linearly dependent set. Thus, Theorem 2.2 implies that \( \text{span}(\beta') = V  \). So, \( \beta'  \) is a basis for \( V  \).
\end{proof}

\subsection*{Exercise 2.5.14}\label{Exercise 2.5.14} Prove the converse of Exercise 8: If \( A  \) and \( B  \) are each \( m \times n  \) matrices with entries from a field \( F \), and if there exists invertible \( m \times m  \) and \( n \times n  \) matrices \( P  \) and \( Q  \), respectively, such that \( B = P^{-1} A Q  \), then there exist an \( n- \)dimensional vector space \( V  \) and an \( m- \)dimensional vector space \( W  \) (both over \( F \)), ordered bases \( \beta \) and \( \beta'  \) for \( V  \) and \( \gamma \) and \( \gamma'  \) for \( W  \), and a linear transformation \( T: V \to W  \) such that
\[  A = [T]_{\beta}^{\gamma}  \ \text{ and } \ B = [T]_{\beta'}^{\gamma'}. \]
\begin{proof}
    Since \( A  \) and \( B  \) are \( m \times n  \) with entries from a field \( F  \), let \( V = F^{n} \) and \( W = F^{m} \). Since \( V \) and \( W  \) are finite-dimensional, they contain ordered bases \( \beta =  \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \} \) and \( \gamma = \{ {w}_{1}, {w}_{2}, \dots, {w}_{m} \}   \). By Theorem 2.6, there must exists a linear transformation \( T: V \to W  \) such that 
    \[  T({v}_{j}) = \sum_{ i=1  }^{ m } {A}_{ij} {w}_{i} \ \text{ for } \ 1 \leq j \leq n. \]
    Since \( V = F^{n} \) and \( W = F^{m} \), let \( T = {L}_{A} \). Hence, we have
    \[  [T]_{\beta}^{\gamma} = [{L}_{A}]_{\beta}^{\gamma} = A  \]
    by part (a) of Theorem 2.15.
    
    Now, define ordered bases \( \beta' = \{ {x}_{1}', {x}_{2}', \dots, {x}_{n}' \}  \) and \( \gamma' = \{ {w}_{1}', {w}_{2}', \dots, {w}_{m}' \}  \) for \( V  \) and \( W  \), respectively. Since there exists invertible \( n \times n \) and \( m \times m  \) matrices \( Q  \) and \( P  \) respectively, we can apply the result from Exercise 2.5.13 to imply that
    \[  {x}_{j }' = \sum_{ i=1  }^{ n } {Q}_{ij} {x}_{i} \ \text{ for } \ 1 \leq j \leq n  \] and 
    \[  {w}_{j}' = \sum_{ i=1  }^{ m } {P}_{ij} {w}_{i} \ \text{ for } \ 1 \leq j \leq n ,  \]
    implying that \( Q  \) and \( P  \) are change of coordinate matrices that changes \( \beta'- \)coordinates to \( \beta- \)coordinates and \( \gamma'-\)coordinates to \( \gamma- \)coordinates, respectively. 

    Finally, we show that \( B = [T]_{\beta'}^{\gamma'}  \). Using Theorem 2.11, we can write
    \begin{align*}
        B = P^{-1} A Q &= [{I}_{V}]_{\gamma}^{\gamma'} [T]_{\beta}^{\gamma} [{I}_{W}]_{\beta'}^{\beta}    \\
                       &= [{I}_{V} T ]_{\beta}^{\gamma'} [{I}_{W}]_{\beta'}^{\beta}  \\
                       &= [T]_{\beta}^{\gamma'} [{I}_{W}]_{\beta'}^{\beta}  \\
                       &= [T {I}_{W}]_{\beta'}^{\gamma'}  \\
                       &= [T]_{\beta'}^{\gamma'}.
    \end{align*}
    Hence, \( B = [T]_{\beta'}^{\gamma'}  \) and we are done.
\end{proof}

