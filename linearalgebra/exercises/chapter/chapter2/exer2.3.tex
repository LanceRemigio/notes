\section{Composition of Linear Transformations and Matrix Multiplication}


\subsection*{Exercise 2.3.5} Complete the proof of Theorem 2.12 and its corollary. 
\begin{proof}
    Let \( A  \) be an \( m \times n  \) matrix, \( B  \) and \(  C  \) be \( n \times p  \) matrices, and \( D  \) and \( E  \) be \( q \times  m  \) matrices. Then
\begin{enumerate}
    \item[(a)] Let \( 1 \leq i \leq m  \) and \( 1 \leq j \leq p  \). By definition of the product of two matrices, we have 
        \begin{align*}
            (A(B+C))_{ij} &= \sum_{ k=1 }^{ n } {A}_{ik } {(B+C)}_{kj} \\ 
               &= \sum_{ k=1 }^{ n } {A}_{ik } ({B}_{kj} + {C}_{kj} ) \\
               &= \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj} + \sum_{ k=1 }^{ n } {A}_{ik} {C}_{kj} \\ 
               &= (AB)_{ij} + (AC)_{ij}.    
        \end{align*}
        Hence, \( A(B+C) = AB + AC  \).
        
        Now, let \( 1 \leq i \leq q  \) and \( 1 \leq j \leq n  \). For the second formula, we can use the same definition to write
    \begin{align*}
        ((D+E) A)_{ij} &= \sum_{ k=1 }^{ m } {(D+E)}_{ik} {A}_{kj}    \\
                &= \sum_{ k=1 }^{ m } ({D}_{ik } + {E}_{ik }) {A}_{kj} \\
                &= \sum_{ k=1 }^{ m } {D}_{ik} {A}_{kj } + \sum_{ i=1 }^{ m   } {E}_{ik } {A}_{kj} \\
                &= (DA)_{ij} + (EA)_{ij}.
    \end{align*}
    Hence, \( (D+E) A = DA + EA \).
    \item[(b)] Let \(  1 \leq i \leq m  \) and \( 1 \leq j \leq p  \). Let \( a \in F  \). Then using the definition of the product once again, we have
        \begin{align*}
            a (AB)_{ij} &= a \sum_{ k=1 }^{ n } {A}_{ik } {B}_{kj} \\
                   &= \sum_{ k=1 }^{ n } a ({A}_{ik } {B}_{kj }) \\
                   &= \sum_{ k=1 }^{ n } ({aA}_{ik}) {B}_{kj } \\
                   &= \sum_{ k=1 }^{ n } {(aA)}_{ik } {B}_{kj} \\
                   &= ((aA)B)_{ij}.
        \end{align*}
    Then observe that 
    \begin{align*}  
        ((aA)B)_{ij} &= \sum_{ k = 1  }^{ n  } (a {A}_{ik } ) {B}_{kj} \\ 
    &= \sum_{ k=1 }^{ n } ({A}_{ik } a ) {B}_{kj}  \\
    &= \sum_{ k=1 }^{ n } {A}_{ik } (a {B}_{kj})  \\
    &= \sum_{ k=1 }^{ n } {A}_{ik } {(aB)}_{kj} \\
    &= (A (aB))_{ij}.
    \end{align*}
    Hence, \( (aA)B = A(aB) \). Thus, we conclude that
    \[ a(AB) = (aA)B = A(aB).  \]
\item[(c)] Let \(   1 \leq i \leq m  \) and \(  1 \leq j \leq n  \). Since \( {\delta}_{ik} = 1   \) only when \( i = k  \) and \( 0  \) otherwise, we must have
    \begin{align*}
        ({I}_{m} A)_{ij} &= \sum_{ k=1  }^{ m  } {\delta}_{ik } {A}_{kj}  = {A}_{ij }. \\
    \end{align*}
    Likewise, 
    \[  {(A {I}_{n} )}_{ij } = \sum_{ k= 1  }^{ n  } {A}_{ik } {\delta}_{k j } = {A}_{ij }  \] by the same reasoning. Hence, we have
    \[  {I}_{m} A = A = A {I}_{n}. \]
    \item[(d)] Let \( V  \) be an \( n- \)dimensional vector space with \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) as an ordered basis. Since \( {I}_{V}({v}_{j}) = {v}_{j }  \) for all \( 1 \leq j \leq n  \), we must have
        \[  {v}_{j} = {I}_{V}({v}_{j}) = \sum_{ i=1 }^{ n } {a}_{ij} {v}_{i}   \]
        which holds only if \(  {a}_{ij} = 1  \) for all \( i = j  \) and \( 0  \) otherwise. But this means that \( {a}_{ij} = {\delta}_{ij}  \), so \( [{I}_{V}]_{\beta}^{}  = {I}_{n} \).
        
\end{enumerate}
\end{proof}

\begin{proof}
Let \( A  \) be an \( m \times n  \) matrix and \( {B}_{1}, {B}_{2}, \dots, {B}_{k } \) be \( n \times p  \) matrices. Let \( 1 \leq \ell \leq m  \) and \( 1 \leq s  \leq p  \). Then
\begin{align*}
    \Big[ A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i} \Big) \Big]_{\ell s } &= \sum_{ \lambda = 1  }^{ n } {A}_{ \ell \lambda }  \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i} \Big)_{\lambda s }  \\
                                                                        &= \sum_{ \lambda = 1  }^{ n  } {A}_{ \ell \lambda } \Big(  \sum_{ i=1  }^{  k  } {({a}_{i} {B}_{i})}_{\lambda s } \Big)  \\
                                                                        &=  \sum_{ \lambda = 1  }^{ n  } {A}_{ \ell \lambda } \Big( \sum_{ i=1 }^{ k  } {a}_{i} ({B}_{i})_{\lambda s } \Big) \\
                                                                        &= \sum_{ i=1  }^{ k  } {a}_{i} \Big( \sum_{ \lambda =1  }^{ n  } {A}_{ \ell \lambda } ({B}_{i})_{\lambda s } \Big) \tag{part (a) of Theorem 2.12}  \\
                                                                        &= \sum_{ i=1  }^{ k  } {a}_{i}  (A {B}_{i})_{\ell s } .
\end{align*}
Hence, we have
\[  A \Big( \sum_{ i=1 }^{ k  } {a}_{i} {B}_{i}  \Big) = \sum_{ i=1  }^{  k  } {a}_{i} {AB}_{i}.\]

To show the second formula, let \( 1 \leq \ell \leq q  \) and \( 1 \leq  s  \leq n   \). Then
\begin{align*}
    \Big[ \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i}  \Big) A  \Big]_{\ell s }  &= \sum_{ \lambda = 1  }^{ n   } \Big( \sum_{ i=1 }^{ k  } {a}_{i} {C}_{i} \Big)_{\ell \lambda } {A}_{\lambda s }  \\
                                                                                     &= \sum_{ \lambda = 1  }^{ n } \Big( \sum_{ i=1 }^{  k   } ({a}_{i} {C}_{i})_{\ell \lambda }  \Big) {A}_{\lambda s } \\
                                                                                     &= \sum_{ \lambda = 1  }^{ n } \Big( \sum_{ i=1 }^{  k   } {a}_{i} ({C}_{i})_{\ell \lambda }  \Big) {A}_{\lambda s } \\
                                                                                     &= \sum_{ i =1  }^{ k  } {a}_{i} \Big( \sum_{ i=1  }^{ k  } {({C}_{i})}_{\ell \lambda } {A}_{\lambda s }  \Big) \tag{part (a) of Theorem 2.12}   \\
                                                                                     &= \sum_{ i=1  }^{ k  } {a}_{i} ({C}_{i} A  )_{\ell s }.
\end{align*}
Hence, we have
\[  \Big( \sum_{ i=1  }^{  k  } {a}_{i} {C}_{i}  \Big) A = \sum_{ i=1  }^{ k  } {a}_{i} {C}_{i} A. \]
\end{proof}



\subsection*{Exercise 2.3.6} Prove (b) of Theorem 2.13.
\begin{proof}
To show the other equation, we apply part (c) of Theorem 2.12, to write
\begin{align*}
   {v}_{j}  &= \begin{pmatrix}
       {B}_{1j } \\
       {B}_{2j } \\
       \vdots \\
       {B}_{nj } 
   \end{pmatrix} = 
   \begin{pmatrix}
        (B {I}_{p})_{1j} \\  
        {(B{I}_{p})}_{2j} \\
        \vdots \\
        {(B {I}_{p})}_{nj}
   \end{pmatrix} = \begin{pmatrix}
       \sum_{ k=1  }^{ n } {B}_{1k} {\delta}_{kj} \\
       \sum_{ k=1  }^{ n } {B}_{2k} {\delta}_{kj } \\
       \vdots \\
       \sum_{ k=1  }^{ n } {B}_{nk} {\delta}_{kj } 
       \end{pmatrix} = B \begin{pmatrix}
       {\delta}_{1j } \\
       {\delta}_{2j } \\
       \vdots \\
       {\delta}_{pj } 
   \end{pmatrix} = B {e}_{j}
\end{align*}
where \( {\delta}_{1j} \) are the Kronecker delta constants.
\end{proof}

\subsection*{Exercise 2.3.7} Prove (c) and (f) of Theorem 2.15.
\begin{proof}
\begin{enumerate}
    \item[(c)] Using the sum rule for matrices, we must have
            \[  {L}_{A+B}(x) = (A+B)(x) = A(x) + B(x) = {L}_{A}(x) + {L}_{B}(x). \]
            Hence, \( {L}_{A+B} = {L}_{A} + {L}_{B} \). Now, let \( a \in F  \). Using the same reasoning, we have  
            \[  {L}_{aA}(x) = (aA)(x) = a (A(x)) = a {L}_{A}(x).  \]
            Hence, \( {L}_{aA} = a{L}_{A}. \)
        \item[(f)] Let \( 1 \leq j \leq n  \). Then
                \begin{align*}  {L}_{{I}_{n}}({e}_{j}) = {I}_{n}({e}_{j}) 
                &= {e}_{j} \\ 
                &= {I}_{F^{n}}({e}_{j}) \tag{Part (d) of Theorem 2.3.4}.
                \end{align*}
            Hence, \( {L}_{{I}_{n}} = {I}_{F^{n}} \).

\end{enumerate}
\end{proof}

\subsection*{Exercise 2.3.8} Prove Theorem 2.10. Now state and prove a more general result involving linear transformations with domains unequal to their codomains.
\begin{prop}
    Let \( V , W, Y,  \) and \( Z  \) be vector spaces. Then we have the following properties:
    \begin{enumerate}
        \item[(a)] Let \( U, {U}_{1}, {U}_{2} \in \mathcal{L}(V,W)  \) and \( T, {T}_{1}, {T}_{2} \in \mathcal{L}(W,Z) \). Then we have
            \begin{center}
                \( T({U}_{1} + {U}_{2}) = {TU}_{1} + {TU}_{2}  \) and \( ({T}_{1} + {T}_{2}) U = {T}_{1} U + {T}_{2} U  \).
            \end{center}
        \item[(b)] Let \( U \in \mathcal{L}(Y,Z),  {T}_{1} \in \mathcal{L}(W,Y) , {T}_{2} \in \mathcal{L}(V,W)   \). Then we have
            \[  U({T}_{1} {T}_{2}) = (U {T}_{1}){T}_{2}. \]
        \item[(c)] Let \( I \in \mathcal{L}(V,V)  \) and \( I \in \mathcal{L}(W,W)  \). Then \( T \in \mathcal{L}(V,W)  \) implies that 
            \[  TI = IT = T. \]
        \item[(d)] Let \( a \in F  \) and let \( {U}_{1}, {U}_{2} \in \mathcal{L}(V,W) \). Then
            \[  a({U}_{1} {U}_{2}) = ({aU}_{1}) {U}_{2} = {U}_{1} ({aU}_{2}). \]
    \end{enumerate}
\end{prop}
\begin{proof}
The proof is similar in the notes.
\end{proof}

\subsection*{Exercise 2.3.10} Let \( A  \) be an \( n \times n  \) matrix. Prove that \( A  \) is a diagonal matrix if and only if \( {A}_{ij} = {\delta}_{ij} {A}_{ij}  \) for all \( i  \) and \( j \).
\begin{proof}
For the forwards direction, suppose \( A  \) is a diagonal matrix. Let \( 1 \leq j \leq  n   \) and \(  1 \leq i \leq  n  \). Since \( A  \) is a diagonal matrix, we know that \( {A}_{ij} = 0  \) whenever \( i \neq j  \). Furthermore, we have that \( {\delta}_{ij} = 1  \) whenever \( i  = j  \) and \(  0  \) otherwise. Hence, we have \( A = IA  \) by Theorem 2.12 which implies 
\[  {A}_{ij} = {({I}_{n} A )}_{ij} = \sum_{ k=1}^{ n } {\delta}_{ik } {A}_{kj } = {\delta}_{ij} {A}_{ij} \]
for \( 1 \leq j \leq n \) and \( 1 \leq j \leq n \).

For the backwards direction, assume \( {A}_{ij} = {\delta}_{ij} {A}_{ij}  \). Since \( {\delta}_{ij} = 1   \) whenever \( i = j  \) and \( 0  \) whenever \(  i \neq j  \), we get that  \( {A}_{ij} = {A}_{ij}  \) and \( {A}_{ij} = 0  \) respectively. Hence, \( A  \) is a diagonal matrix.

\end{proof}

\subsection*{Exercise 2.3.11} Let \( V  \) be a vector space, and let \( T: V \to V  \) be linear. Prove that \( T^{2} = {T}_{0} \) if and only if \( R(T) \subseteq N(T) \).
\begin{proof}
For the forwards direction, let \( T^{2} = {T}_{0}  \). Let \( y \in R(T)  \). Then for some \( x \in V  \), we have \( y = T(x) \). Then we have
\[
    T(y) = T(T(x)) 
         = T^{2}(x) 
         = {T}_{0}(x) 
         = 0.
\]
Hence, \( y \in N(T) \).

For the backwards direction, let \( x \in V  \). Then we have 
\[  T^{2}(x) = T(T(x)). \]
Note that \( T(x) \in R(T)   \) and \( R(T) \subseteq N(T)  \) implies that \( T^{2}(x) = 0  \). But this also means that \( {T}_{0}(x) = 0  \). Hence, we have \( T^{2}(x) = {T}_{0}(x) \).


\end{proof}

\subsection*{Exercise 2.3.12} Let \( V, W,  \) and \( Z  \) be vector spaces, and let \( T:  V \to W  \) and \( U: W \to Z  \) be linear.
\begin{enumerate}
    \item[(a)] Prove that if \( UT \) is injective, then \( T  \) is injective. Must \( U  \) also be injective?
        \begin{proof}
        Suppose for sake of contradiction that \( T \) is not injective. Then there exists \( x,y \in V  \) such that \( x \neq y  \) implies \( T(x) = T(y) \). But \( UT  \) being injective must imply that for any \( x,y \in V  \), we have \(  UT(x) = UT(y)  \) implies \( x = y  \) which is a contradiction. Hence, \( T  \) must be injective. Note that \( U  \) need not be injective in this case.
        \end{proof}
    \item[(b)] Prove that if \( UT  \) is surjective, then \( U  \) is surjective. Must \( T  \) also be surjective? 
        \begin{proof}
        Suppose that \( UT  \) is surjective. Let \( y \in R(UT) \). By default, we know that \( R(U) \subseteq Z) \). Then for some \( x \in V  \), we have 
        \[  y = UT(x) = U(T(x)). \]
      Hence, \( y \in R(U) \) as well which implies that \( Z \subseteq R(U) \). Thus, \( U  \) must be surjective. Notice that \( T  \) need not be surjective for the equation above to be true.
        \end{proof}
    \item[(c)] Prove that if \( U  \) and \( T  \) are injective and surjective, then \( UT  \) is also.
        \begin{proof}
            First, we show that \( UT  \) is injective. Let \( x,y \in V  \). Then
            \begin{align*}
                UT(x) &= UT(y) \\
                U(T(x)) &= U(T(y)).
            \end{align*}
            But \( U  \) being injective, implies that \( T(x) = T(y)  \). Since \( T  \) is also injective, we must have \( x = y  \). Hence, \( UT  \) is injective. 

            Now, we show that \( UT  \) is surjective. Let \( z \in R(U)  \). Since \( U  \) is surjective, we have that for some \( y \in W   \), 
            \[  z = U(y). \]
            Since \( T  \) is also surjective, we have that \( y \in W  \) implies that \( T(x) = y  \) for some \( x \in V  \). Hence, we have
            \[  z = U(y) = U(T(x)) = UT(x) \]
            and so we have \( UT  \) surjective.
            
        \end{proof}
\end{enumerate}

\subsection*{Exercise 2.3.13} Let \( A  \) and \( B  \) be \( n \times n  \) matrices. Recall that the trace of \( A  \) is defined by
\[  \text{tr}(A) = \sum_{ i=1  }^{ n } {A}_{ii}. \]
Prove that \( \text{tr}(AB) = \text{tr}(BA) \) and \( \text{tr}(A) = \text{tr}(A^{t}) \).
\begin{proof}
    Let \( 1 \leq i \leq n \). Observe that
    \begin{align*}
        \text{tr}(AB) &= \sum_{ i=1  }^{ n } {(AB)}_{ii} \\
                      &= \sum_{ i=1  }^{ n } \Big( \sum_{ k=1  }^{ n } {A}_{ik } {B}_{ki} \Big) \\
                      &= \sum_{ k=1  }^{ n } \Big( \sum_{ i=1  }^{ n } {B}_{ki } {A}_{ik } \Big) \\
                      &= \sum_{ k=1  }^{ n } {(BA)}_{k k } \\
                      &= \text{tr}(BA).
    \end{align*}
    Hence, \( \text{tr}(AB) = \text{tr}(BA ) \).

   For the second formula, observe that
   \[  \text{tr}(A^{t}) = \sum_{ i=1  }^{ n   } {(A^{t})}_{ii} = \sum_{ i=1  }^{ n } {A}_{ii} = \text{tr}(A). \]
\end{proof}

\subsection*{Exercise 2.3.14}\label{Exercise 2.3.14} Assume the notation in Theorem 2.13.
\begin{enumerate}
    \item[(a)] Suppose that \( z  \) is a (column) vector in \( F^{p} \). Use Theorem 2.13(b) to prove that \( Bz  \) is a linear combination of the columns of \( B  \). In particular, if \( z = ({a}_{1}, {a}_{2}, \dots, {a}_{p})^{t} \), then show that  
        \[  Bz = \sum_{ j=1  }^{ p  } {a}_{j} {v}_{j}. \]
        \begin{proof}
        Note that \( B  \) is an \( n \times  p  \) matrix and that 
        \[  z = ({a}_{1}, {a}_{2}, \dots, {a}_{p})^{t} = \begin{pmatrix}
            {a}_{1} \\
            {a}_{2} \\
            \vdots \\
            {a}_{p}
        \end{pmatrix}. \]
        Observe that \( z  \) can be re-written in the following way:
        \begin{align*}
            z = {a}_{1} {e}_{1} + {a}_{2} {e}_{2} + \cdots + {a}_{p} {e}_{p}  
              = \sum_{ j=1  }^{ p  } {a}_{j } {e}_{j }.
        \end{align*}
        Using theorem 2.13, we find that
        \begin{align*}
            Bz = B \Big( \sum_{ j=1  }^{ p } {a}_{j} {e}_{j} \Big)
               = \sum_{ j=1  }^{ p } {a}_{j} (B {e}_{j}) 
               =  \sum_{ j=1  }^{ p } {a}_{j} {v}_{j}.
        \end{align*}
        Hence, we have
        \[  Bz = \sum_{ j=1  }^{ p } {a}_{j} {v}_{j}. \]


        \end{proof}
    \item[(b)] Extend (a) to prove that column \( j  \) of \( AB  \) is a linear combination of the columns of \( A  \) with coefficients in the linear combination being entries of column \( j  \) of \(  B \). 
        \begin{proof}
        Denote the column vectors of \( A  \) as \( {x}_{i} \) with \( 1 \leq i \leq n  \) and note that \( A  \) is an \( m \times n  \) matrix. Observe that the \( j \)th column of \( B  \) can be written as
        \[  {v}_{j} = \sum_{ i=1  }^{ n } {B}_{ij} {e}_{i} \]
        for \( 1 \leq j \leq p  \). Using Theorem 2.13 again, we find that
        \begin{align*}
            {u}_{j}  = A {v}_{j}  &= A \Big( \sum_{ i=1  }^{ n } {B}_{ij} {e}_{i} \Big) \\
                                  &= \sum_{ i=1  }^{ n } {B}_{ij} ({Ae}_{i}) \\
                                  &= \sum_{ i=1  }^{ n } {B}_{ij} {x}_{i}
        \end{align*}
        where \( {x}_{i}  \) is a column vector in \( F^{m} \) with entries in \(  A  \). Hence, we have

        \[  {u}_{j} = \sum_{ i=1  }^{ n } {B}_{ij} {x}_{i} \ \text{ for } 1 \leq j \leq p. \]
        \end{proof}
    \item[(c)] For any row vector \( w \in F^{m} \), prove that \( wA  \) is a linear combination of the rows of \( A  \) with the coefficients in the linear combination being the coordinates of \( w  \).
        \begin{proof}
        Let \( w \in F^{m} \) be a row vector with entries \( {b}_{j}  \) for \(  1 \leq i \leq m  \). Denote \( {x}_{i}  \) as the \( i \)th row of \( A  \) which is an \(  m \times n  \) matrix. Using the properties of transpose, we write
        \begin{align*}
            wA = (A^{t} w^{t})^{t} &= \Big( \sum_{ i=1 }^{ m } {b}_{i} {x}^{t}_{i}  \Big)^{t} \\
                                   &= \sum_{ i=1  }^{ m } ({b}_{i} {x}^{t}_{i})^{t} \\
                                   &= \sum_{ i=1  }^{ m } {b}_{i} {x}_{i}.
        \end{align*}
        Hence, 
        \[  wA = \sum_{ i=1  }^{ m } {b}_{i} {x}_{i}. \]
        \end{proof}
    \item[(d)] Prove the analogous result to (b) about rows: Row \( i \) of \( AB  \) is a linear combination of the rows of \( B  \) with the coefficients in the linear combination being the entries of row \( i  \) of \( A  \).
        \begin{proof}
        Let \( {u}_{i}  \) denote the \( i \)th row of the matrix \( AB  \). Note that \( AB  \) is an \( m \times p  \) matrix. By taking a similar approach to proving part (a) of Theorem 2.13, we have 
        \[  {u}_{i} = \begin{pmatrix}
            {(AB)}_{i1} \\
            {(AB)}_{i2} \\
            \vdots \\
            {(AB)}_{ip}
        \end{pmatrix}^{t} = \begin{pmatrix}
            \sum_{ k=1 }^{ n } {A}_{ik } {B}_{k1} \\
            \sum_{ k=1  }^{ n } {A}_{ik } {B}_{k2 } \\
            \vdots \\
            \sum_{ k=1  }^{ n } {A}_{ik } {b}_{k p}
        \end{pmatrix}^{t} = \begin{pmatrix}
            {A}_{i1 } \\ 
            {A}_{i2} \\
            \vdots \\
            {A}_{in }
        \end{pmatrix}^{t} B = {x}_{i} B  \]
        where \( {x}_{i}  \) is the \( i   \)th row of \( A  \). Apply part (c), we have
        \[  {u}_{i} = {x}_{i} B = \sum_{ j=1  }^{ n } {A}_{ij} {v}_{j} \ \text{ for } \  1 \leq i \leq m \]
        where \( {v}_{j}  \) is the \( j \)th row of \( B  \) that has dimensions \( 1 \times  p  \).
        \end{proof}

\end{enumerate}


\subsection*{Exercise 2.3.15} Let \( M  \) and \( A  \) be matrices for which the product matrix \( MA  \) is defined. If the \( j \)th column of \( A  \) is a linear combination of a set of columns of \( A  \), prove that the \( j \)th column of \( MA  \) is a linear combination of the corresponding columns of \( MA  \) with the same corresponding coefficients.
\begin{proof}
   Let \( M  \) be an \( m \times n  \) matrix. Let \( {x}_{j}  \) be the \( j \)th column of \( A  \) where \( A  \) is an \( n \times p  \) matrix. Let \( {u}_{j}  \) be the \( j \)th column of \( MA  \) where \( MA  \) is an \( m \times p  \) matrix. Define this \( j \)th column vector as 
   \[  {u}_{j} = M {x}_{j} \]
   by Theorem 2.13. Note that
   \[  {x}_{j} = \begin{pmatrix}
       {A}_{1j } \\
       {A}_{2j } \\
       \vdots \\
       {A}_{nj}
   \end{pmatrix} = \sum_{ i=1  }^{ n } {A}_{ij} {e}_{i} \]
   where \( {e}_{i}  \) is the standard basis vector for \( F^{n} \). We can see by Corollary to Theorem 2.12 that
   \[  {u}_{j} = M {x}_{j} = M \Big( \sum_{ i=1  }^{ n } {A}_{ij} {e}_{i}  \Big) = \sum_{ i=1  }^{ n } {A}_{ij} (M {e}_{i}). \]
   Note that \( {e}_{i}  \) is an \( n \times 1  \) matrix and \( M  \) is an \( m \times n  \) matrix. This means that \( M {e}_{i} \) is in \( F^{m} \). Denote this column vector as \( {v}_{i} = M {e}_{i}  \) where \( i  \) is the \( i \)th column of \( M  \). Hence, we have
   \[  {u}_{j} = \sum_{ i=1  }^{ n } {A}_{ij} {v}_{i} \ \text{for} \ 1 \leq j \leq  p. \]
\end{proof}

\subsection*{Exercise 2.3.16} Let \( V  \) be finite-dimensional vector space, and let \( T: V \to V  \) be linear.
\begin{enumerate}
    \item[(a)] If \( \text{rank}(T) = \text{rank}(T^{2}) \), prove that \( R(T) \cap N(T) = \{ 0  \}   \). Deduce that \( V = R(T) \oplus N(T)  \) (see the exercises of Section 1.3).
        \begin{proof}
        Since \( V  \) is finite-dimensional, we know that 
        \[  \text{dim}(V) = \text{nullity}(T) + \text{rank}(T) \tag{1} \]
            by the dimension theorem. By the same reasoning, we also get that
            \[  \text{dim}(V) = \text{nullity}(T^{2}) + \text{rank}(T^{2}). \tag{2} \]
            Equating (1) and (2) together, we get that
            \[  \text{nullity}(T) + \text{rank}(T) = \text{nullity}(T^{2}) + \text{rank}(T^{2}). \]
            Since \( \text{rank}(T) = \text{rank}(T^{2} )  \), we find that \( \text{nullity}(T) = \text{nullity}(T^{2}) \). Let \( x \in N(T)  \) be arbitrary. Then we have that \( T(x) = 0  \) with \( x \in N(T)  \) implying \( x = 0  \). But since \( x \in N(T^{2}) \) as well, we have that
            \[  T^{2}(x) = 0. \]
    Note that 
            \[  T^{2}(x) = 0 \iff T(T(x)) = 0  \]
            with \( T(x) \in N(T)  \) implying \( T(x) = 0  \) since \( T  \) is injective. Note that 
            \[  T(x) = x = 0  \]
            where \( T(x) \in R(T)  \) and \( x \in N(T)  \). Hence, \( R(T) \cap N(T) = \{ 0  \}   \).

            By the dimension theorem and Exercise 1.6.29, we have
            \begin{align*}
                \text{dim}(R(T) + N(T)) &= \text{dim}(R(T)) + \text{dim}(N(T)) - \text{dim}(R(T) \cap N(T)) \\
                                        &=  \text{dim}(R(T)) + \text{dim}(N(T)) \\
                                        &= \text{dim}(V).
            \end{align*}
Hence, \( V = R(T) + N(T)  \) by Theorem 1.11. Thus, we have
\[  V = R(T) \oplus N(T). \]
        \end{proof}

    \item[(b)] Prove that \( V = R(T^{k }) \oplus N(T^{k }) \) for some positive integer \( k  \).
            \begin{proof}
            
            Pick a positive integer \( k  \) such that \( \text{rank}(T^{k }) = \text{rank}(T^{k+1})  \). By dimension theorem, we can write 
            \[  \text{dim}(V) = \text{rank}(T^{k }) + \text{nullity}(T^{k }). \tag{1}\] By the same reasoning, we also have
            \[  \text{dim}(V) = \text{rank}(T^{k+1}) + \text{nullity}(T^{k + 1 }). \tag{2} \] By setting (1) and (2) equal to each other and using the fact that \( \text{rank}(T^{k } ) = \text{rank}(T^{k+1}) \), we have
            \[  \text{nullity}(T^{k }) = \text{nullity}(T^{k+1}).  \]
            Thus, \( N(T^{k }) = N(T^{k+1}) \) by Theorem 1.11. Now, let \( x \in N(T^{k+1})  \). Then we have \( T^{k+1}(x) = 0  \). Since \( x \in N(T^{k }) \), we also have \( T^{k } (x) = 0  \). But note that 
            \[  T^{k+1}(x) = T^{k }(T(x)) = 0. \]
            So, we have
            \[  T^{k}(T(x)) = T^{k}(x) = 0.  \]
            Since \( {T}_{k} \) is injective, we know that 
            \[  T(x) = x = 0.   \]
            But note that \( T(x) \in R(T^{k})  \) and \( x \in N(T^{k}) \). Hence, \( R(T^{k}) \cap N(T^{k}) = \{ 0  \}  \).
            To show that \( V = R(T^{k}) + N(T^{k })   \), we can just follow the same process shown in part (a). Hence, we have 
            \[  V = R(T^{k }) \oplus N(T^{k }). \]
        \end{proof}
        \end{enumerate}

\subsection*{Exercise 2.3.18} Using only the definition of matrix multiplication, prove that multiplication of matrices is associative. 
\begin{proof}
   Define \( A, B    \) and \( C  \) as \( m \times n  \), \( n \times p  \), and \( p \times \ell  \) matrices respectively. This establishes that \( (AB) C  \) is an \( m \times \ell  \) matrix. Using the definition of matrix multiplication, we get that 
   \begin{align*}
       \Big( (AB) C  \Big)_{ij} &= \sum_{ k=1  }^{ p  } {(AB)}_{ik } {C}_{kj }  \\
                                &= \sum_{ k=1  }^{ p  } \Big( \sum_{ \lambda = 1  }^{ n } {A}_{i \lambda } {B}_{\lambda k  } \Big) {C}_{kj} \\
                                &= \sum_{ \lambda = 1  }^{ n  } {A}_{i \lambda } \Big( \sum_{ k=1  }^{ p  } {B}_{\lambda k } {C}_{ k j } \Big) \\
                                &= \sum_{ \lambda = 1  }^{ n  } {A}_{i \lambda } {(BC)}_{\lambda j } \\
                                &= \Big( A(BC)  \Big)_{ij}
   \end{align*}
   where \( 1 \leq i \leq m  \) and \( 1 \leq j \leq \ell  \). Hence, we have \( (AB)C = A(BC) \).
\end{proof}
