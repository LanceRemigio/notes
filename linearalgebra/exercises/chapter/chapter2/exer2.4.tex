\section{Invertibility and Isomorphisms}

\subsection*{Exercise 2.4.4}\label{Exercise 2.4.4} Let \( A  \) and \( B  \) be \( n \times n  \) invertible matrices. Prove that \( AB  \) is invertible and \( (AB)^{-1} = B^{-1} A^{-1} \).
\begin{proof}
Let \( A  \) and \( B  \) be \( n \times n  \) invertible matrices. We need to show that \( (AB)B^{-1}A^{-1} = {I}_{n} \) and \( B^{-1} A^{-1} (AB) = {I}_{n} \). Observe that
\begin{align*}
    B B^{-1} = {I}_{n}   &\iff A(B B^{-1}) = A   \\
                         &\iff  (AB) B^{-1} = A  \\
                         &\iff (AB) B^{-1}A^{-1} = A A^{-1} \\ 
                         &\iff (AB) B^{-1} A^{-1} = {I}_{n}.
\end{align*}
Similarly, we have 
\begin{align*}
     A^{-1} A  = {I}_{n} &\iff B^{-1}(A^{-1} A ) = B^{-1}  \\
                         &\iff (B^{-1} A^{-1}) A = B^{-1} \\ 
                         &\iff (B^{-1} A^{-1}) AB = B^{-1}B \\
                         &\iff (B^{-1}A^{-1}) AB = {I}_{n}.
\end{align*}
This tells us that \( AB  \) is invertible and that  
\[  B^{-1}A^{-1} = (AB)^{-1}. \]
\end{proof}

\subsection*{Exercise 2.4.5} Let \( A  \) be invertible. Prove that \( A^{t}  \) is invertible and \( (A^{t})^{-1} = (A^{-1})^{t} \).
\begin{proof}
Let \( A   \) be an invertible. Observe that \( (A^{t})^{t} = A  \). Thus, we have
\[  {I}_{n} = A A^{-1} = (A^{t})^{t} A^{-1} = \Big( (A^{-1})^{t} A^{t} \Big)^{t}. \]
Taking the transpose on both sides gives us the following equation
\[  (A^{-1})^{t} A^{t} = {I}_{n} \] where \( ({I}_{n})^{t} = {I}_{n} \).
Similarly, we have
\[  {I}_{n} =  A^{-1} A = A^{-1} (A^{t})^{t} = \Big( A^{t} (A^{-1})^{t} \Big)^{t}  \]
which implies
\[ \Big( A^{t} (A^{-1})^{t} \Big)^{t} = {I}_{n}.  \]
Taking the transpose again then gives us
\[  A^{t} (A^{-1})^{t} = {I}_{n}. \]
Therefore, \( A^{t} \) is invertible and that 
% \[  (A^{-1})^{t} = (A^{t})^{-1}. \]
\end{proof}

\subsection*{Exercise 2.4.6} Prove that if \( A  \) is invertible and \( AB = O  \), then \( B = O  \).
\begin{proof}
Suppose that \( A  \) is an invertible matrix. Let \( AB  \) be a defined matrix product where \( AB = O  \). Since \( A  \) is invertible, we have \( A^{-1}A = A A^{-1} = I \). Thus, we have
\begin{align*}
    AB = O &\implies A^{-1}(AB) = A^{-1}O   \\
           &\implies (A^{-1}A)B = O \\
           &\implies IB = O \\
           &\implies B = O. 
\end{align*}
Hence, we have \( B = O  \).
\end{proof}

\subsection*{Exercise 2.4.7} Let \( A  \) be an \( n \times n  \) matrix.
\begin{enumerate}
    \item[(a)] Suppose that \( A^{2} = O  \). Prove that \( A  \) is not invertible.
        \begin{proof}
        Let \( A^{2} = O  \) where \( A  \) is an \( n \times n  \) matrix. Suppose for sake of contradiction that \( A  \) is invertible. Thus, we have \( A A^{-1} = A^{-1} A = {I}_{n}  \). Now, observe that multiplying \( A^{-1}  \) on the left side of \( A^{2} = O  \) produces the following  
        \begin{align*}
            A^{-1} (A^{2}) = A^{-1}O &\implies A^{-1}(AA ) = O       \\
                                     &\implies (A^{-1}A)A = O  \\ 
                                     &\implies {I}_{n}A = O \\ 
                                     &\implies A = O.
        \end{align*}
        \end{proof}
    \item[(b)] Suppose that \( AB = O  \) for some nonzero \( n \times n  \) matrix \( B  \). Could \( A  \) be invertible? Explain.
        \begin{solution}
        The matrix \( A  \) cannot be invertible in this case since the nonzero matrix \( B   \) implies that \( A = O   \) for \( AB = O  \) to hold and that we know that the zero matrix \( O  \) cannot be invertible.
        \end{solution}
\end{enumerate}

\subsection*{Exercise 2.4.8} Prove Corollaries 1 and 2 of Theorem 2.18.
\begin{proof}
    Let \( T: V \to V  \) be linear and let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}   \) be an ordered basis for \( V  \). Suppose \( T  \) is invertible. Then there exists a unique linear transformation denoted by \( T^{-1}: V \to V  \) such that 
    \[
         TT^{-1} = T^{-1}T = {I}_{V}. 
    \]
    By using the Corollary to Theorem 2.11 and part (d) of Theorem 2.12, we must have that
    \[ [T]_{\beta} [T^{-1}]_{\beta} = [T T^{-1}]_{\beta} = [{I}_{V}]_{\beta} = {I}_{n}  \]
    and similarly,
    \[  [T^{-1}]_{\beta} [T]_{\beta} = [T^{-1} T]_{ \beta} = [{I}_{V}]_{\beta} = {I}_{n}. \]
    This tells us that \( [T]_{\beta}  \) is invertible and that 
    \[  [T^{-1}]_{\beta} = ([T]_{\beta})^{-1}. \]

   For the backwards direction, the ordered basis \( \beta \) defined earlier implies that there exists a unique linear transformation \( U: V \to V  \) defined by  
   \[  U({v}_{j}) = \sum_{ i=1  }^{ n } {A}_{ij} {v}_{i} \ \text{for} \ 1 \leq j \leq n. \]
   We need to show that \( U = T^{-1} \). Using the fact that \( [T]_{\beta} \) is invertible, we can write
   \[  [T U]_{\beta} = [T]_{\beta} [U]_{\beta} = {I}_{n} = [{I}_{V}]_{\beta}  \]
   and similarly
   \[  [UT]_{\beta} = [U]_{\beta} [T]_{\beta} = {I}_{n} = [{I}_{V}]_{\beta}.  \]
   But this tells us that \( UT = TU  = {I}_{V} \). So, \( U = T^{-1}  \) and that \( T \) is invertible.
\end{proof}

\begin{proof}
Let \( {L}_{A}: F^{n} \to F^{n} \). Let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) be a basis for \( F^{n} \). By part (a) of {\hyperref[Prop of LMT]{Theorem 2.15}}, we have that \( [{L}_{A}]_{\beta} = A   \). Since \( A \) is invertible, we know that \( {L}_{A} \) must also be invertible by Corollary to Theorem 2.18. Furthermore, we have that \( {L}_{A} {L}_{A^{-1}} = I   \) implies that 
\[  {L}_{A^{-1}} = ({L}_{A})^{-1}.  \]

Conversely, \( {L}_{A} \) invertible implies that \( [{L}_{A}]_{\beta}  \) is invertible by Corollary to Theorem 2.18. By Theorem 2.15, we must have that \( [L_{A}]_{\beta}  = A  \). But this means that \( A  \) is invertible.
\end{proof}

\subsection*{Exercise 2.4.9} Let \( A  \) and \( B  \) be \( n \times n \) matrices such that \( AB  \) is invertible. Prove that \( A  \) and \( B  \) are invertible. Given an example to show that arbitrary matrices \( A  \) and \( B  \) need not be invertible if \( AB  \) is invertible.
\begin{proof}

\end{proof}

\subsection*{Exercise 2.4.10} Let \( A  \) and \( B  \) be \( n \times n  \) matrices such that \( AB = {I}_{n} \). 
\begin{enumerate}
    \item[(a)] Use Exercise 9 to conclude that \(  A  \) and \( B  \) are invertible.
        \begin{solution}
        Apply Exercise 9.
        \end{solution}
    \item[(b)] Prove \( A = B^{-1} \) (and hence \( B = A^{-1} \)).
        \begin{proof}
        Since \( A  \) is an \( n \times n  \) invertible matrix, we can write that
        \begin{align*}
            AB = {I}_{n} &\implies A^{-1}(AB) = A^{-1} \\
                         &\implies (A^{-1}A)B = A^{-1} \\
                         &\implies {I}_{n}B = A^{-1} \\
                         &\implies B = A^{-1}.
        \end{align*}
        Likewise, \( B  \) being an \(  n \times n  \) invertible matrix implies that
        \begin{align*}
            AB = {I}_{n} &\implies (AB)B^{-1} = B^{-1}k \\
                         &\implies A(B B^{-1}) = B^{-1} \\
                         &\implies A {I}_{n} = {B}^{-1} \\
                         &\implies A = B^{-1}.
        \end{align*}
        \end{proof}
    \item[(c)] State and prove analogous results for linear transformations defined on finite-dimensional vector spaces.
        \begin{proof}
        Define \( T: V \to V  \) and \( U: V \to V  \) with \( V  \) being an arbitrary finite-dimensional vector space. Let \( \beta = \{ {v}_{1}, {v}_{2}, \dots ,{v}_{n} \}   \) be an ordered basis for \( V  \) such that \( [T]_{\beta}  \) and \( [U]_{\beta} \) are properly defined and that \( [T]_{\beta} [U]_{\beta} = {I}_{n} \). Then we have
       \begin{center}
           \( [T]_{\beta} = ([U]_{\beta})^{-1} \) and \( [U]_{\beta} = ([T]_{\beta})^{-1} \).
       \end{center} 
       To prove this, we can let \( A = [T]_{\beta} \) and \( B = [U]_{\beta} \) and use parts (a) and (b) to get our desired result.
        \end{proof}
\end{enumerate}

\subsection*{Exercise 2.4.11} Verify that the transformation in Example 5 is injective.
\begin{proof}
Since \( \text{dim}({P}_{3}(\R)) = \text{dim}({M}_{2 \times 2}(\R)) \), we must have that \( {P}_{3}(\R)  \) is isomorphic to \( {M}_{2 \times 2}(\R) \) by Theorem 2.19. By definition, this means that \( T  \) is invertible which further implies that \( T  \) is injective.
\end{proof}

\subsection*{Exercise 2.4.12} Prove Theorem 2.21.
\begin{proof}
    Let \( \text{dim}(V) = n  \). Let \( \beta = \{ {v}_{1}, { v }_{2}, \dots, {v}_{n} \}   \) be an ordered basis for \( V  \). We can show that \( {\phi}_{\beta}(y)  \) is an isomorphism by showing that \( {\phi}_{\beta} \) is a surjective and injective linear map. Suppose \( {\phi}_{\beta}(x) = {\phi}_{\beta} \). Then by definition of \( {\phi}_{\beta} \), we must have that \( [x]_{\beta} = [y]_{\beta} \) which further implies that \( x = y  \). Now, let \( y \in V  \). Since \( \beta \) is an ordered basis for \( V  \), we can find scalars \( {\delta}_{1}, {\delta}_{2}, \dots, {\delta}_{n} \) such that 
    \[  y = \sum_{ i=1  }^{ n } {\delta}_{i} {v}_{i}. \]
    This implies that we have constructed a coordinate vector such that \( [y]_{\beta} = {\phi}_{\beta}(y)\). Hence, \( {\phi}_{\beta} \) is surjective. Thus, we find that \( {\phi}_{\beta} \) is an isomorphism.
\end{proof}

\subsection*{Exercise 2.4.13} Let \( \sim \) mean "is isomorphic to." Prove that \(  \sim \) is an equivalence relation on the class of vector spaces over \( F  \). 
\begin{proof}
Let \( S  \) be the class of vector spaces over \( F \). Let \( V \in S  \). Notice that \( V  \) is isomorphic to itself since the identity linear transformation \( {I}_{V}: V \to V  \) is invertible. Thus, \( V \sim V  \) .

Let \( V, W \in S \). Suppose that \( V \sim W  \). Then there exists an invertible linear map \(  T: V \to W   \). This means that \( T  \) contains an inverse \( T^{-1}: W \to V  \) such that \( T T^{-1} = {I}_{W} \) and \( T^{-1}T = {I}_{V} \). Note that \( T^{-1}  \) is linear and that \( T^{-1}  \) is also invertible since \( T  \) is its inverse. Thus, \( W \sim V  \).

Let \( V, W, Z \in S   \). Now, suppose that \( V \sim W  \) and \( W \sim V  \). This means that there exists invertible linear maps \( T: V \to W  \) and \( U: W \to Z  \). Let \( L: V \to Z  \) be defined by \( L = UT  \). Let \( x,y \in V  \). Suppose \( L(x) = L(y)  \). Then
\begin{align*}
    L(x) &= L(y) \\
    UT(x) &= UT(y) \\
    U(T(x)) &= U(T(y)). 
\end{align*}
Notice that \( U  \) is an injective map which tells us that \( T(x) = T(y) \). But \( T  \) is also injective, so we must have \( x = y \). Hence, \( L  \) is an injective map.

    Now, let \( z \in Z  \). Since \( U  \) is surjective, we must have \( z = U(y) \) for some \( y \in W    \). But note that \( T \) is surjective implies that \( y = T(x)  \) for some \( x \in V  \). But this tells us that  
    \[  z = U(y) = U(T(x)) = UT(x) = L(x). \]
    Hence, \( L  \) is a surjective map and that \( L  \) is an invertible map. Thus, \( V \sim Z  \). 
\end{proof}

\subsection*{Exercise 2.4.15} Let \( V  \) and \( W  \) be finite-dimensional vector spaces, and let \( T: V \to W  \) be a linear transformation. Suppose that \( \beta \) is a basis for \( V  \). Prove that \( T  \) is an isomorphism if and only if \( T(\beta)  \) is a basis for \( W  \).
\begin{proof}
    For the forwards direction, suppose \( T: V \to W   \) is an isomorphism where \( V  \) and \( W  \) are finite-dimensional vector spaces. Hence, \( T  \) is invertible by definition. Let \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) be an ordered basis for \( V  \). Since \( T  \) is also injective and that \( \beta \) is a linearly independent subset of \( V  \), we know that \( T(\beta) \) is a linearly independent subset of \( W  \) by {\hyperref[Exercise 2.1.14]{part (b) of Exercise 2.1.14}}. Furthermore, \( \beta  \) being a basis for \( V  \) implies that \( \text{span}(T(\beta)) = R(T)  \) by Theorem 2.2. But notice that \( T  \) is also surjective since it is an isomorphism. Hence, we must have that  \( R(T) = W  \) and that \( T(\beta) \) spans \( W  \). Thus, we have that \( T(\beta) \) is a basis for \( W  \).

    For the backwards direction, assume \( T(\beta) \) is a basis for \( W  \). In order to show that \( T  \) is an isomorphism, we must show that \( T  \) is a bijective linear map. Let \( x,y \in V  \). Since \( \beta \) is a basis for \( V  \), we must have
    \[  x = \sum_{ i=1  }^{ n } {a}_{i} {v}_{i} \ \text{and} \ y = \sum_{ i=1 }^{ n }{b}_{i} {v}_{i} \]
    for some scalars \( {a}_{1}, {a}_{2}, \dots, {a}_{n} \) and \( {b}_{1}, {b}_{2}, \dots, {b}_{n} \). Suppose \( T(x) = T(y) \). Then observe that \( T  \) being linear implies that
    \begin{align*}
        T(x) &= T(y) \\
        T \Big( \sum_{ i=1 }^{ n } {a}_{i} {v}_{i}  \Big) &= T \Big( \sum_{ i=1 }^{ n }{b}_{i} {v}_{i} \Big) \\
                                             \sum_{ i=1 }^{ n } {a}_{i} T({v}_{i})   &= \sum_{ i=1 }^{ n } {b}_{i} T({v}_{i})
    \end{align*}
    which subsequently leads to 
    \[  \sum_{ i=1 }^{ n } ({a}_{i} - {b}_{i}) T({v}_{i}) = 0 \iff {a}_{i} = {b}_{i} \ \text{for all} \ i  \]
    since \( T(\beta) \) is a linearly independent subset of \( W  \). Hence, \( x = y  \) and that \( T  \) is injective.

    Now, let \( y \in W  \). Since \( T(\beta) \) is a basis for \( W  \) and \( T  \) linear, we know that there exists scalars \( {a}_{1},{a}_{2}, \dots, {a}_{n} \) such that 
    \[  y = \sum_{ i=1 }^{ n } {a}_{i} T({v}_{i})  = T \Big( \sum_{ i=1 }^{ n } {a}_{i} {b}_{i} \Big).  \]
    But this means that
    \[ x = \sum_{ i=1 }^{ n } {a}_{i} {b}_{i}   \]
    for some \( x \in V  \). Hence, \( T  \) is surjective and thus \( T  \) is an isomorphism.
\end{proof}

\subsection*{Exercise 2.4.16} Let \( B  \) be an \( n \times n  \) invertible matrix. Define \( \Phi: {M}_{n \times n}(F) \to {M}_{n \times n }(F) \) by \( \Phi(A) = B^{-1} A B  \). Prove that \( \Phi  \) is an isomorphism.
\begin{proof}
    Let \( B  \) be an \( n \times n  \) matrix. First, we show that \( \Phi: {M}_{n \times n}(F) \to {M}_{n \times n}(F)   \) is linear. 
    Let \( a \in F  \) and \( D, C \in {M}_{n \times n}(F) \). Then by using the algebraic properties of matrices, we must have
    \begin{align*}
        \Phi(aC + D) &= B^{-1}(aC + D)B \\
                  &=  \Big( B^{-1}(aC) + B^{-1}D \Big) B \\ 
                  &= B^{-1}(aC)B + B^{-1} D B \\
                  &= a (B^{-1} C B) + B^{-1}D B \\
                  &= a\Phi(C) + \Phi(D)
    \end{align*}
    Hence, \( \Phi  \) is a linear map. Let \( A \in {M}_{n \times n }(F) \). By definition of \( \Phi  \), we know that 
    \[  \Phi(A) = B^{-1}A B. \]
    So, define the linear map \( U: {M}_{n \times n}(F) \to {M}_{n \times n}(F) \) by \( U(A) = B A B^{-1} \).
    Notice that \( B  \) is invertible, so we can write
    \[  B (B^{-1} A B) B^{-1}  = B^{-1} (B A B^{-1}) B = IA = AI = A  \] where \( I  \) is the identity linear map for \( {M}_{n \times n } \).
    Hence, we have \( \Phi U = U \Phi = I  \). Hence, \( \Phi  \) is an invertible map and thus \( \Phi  \) is an isomorphism.
\end{proof}

\subsection*{Exercise 2.4.17} Let \( V  \) and \( W  \) be finite-dimensional vector spaces and \( T: V \to W  \) be an isomorphism. Let \( {V}_{0} \) be a subspace of \( V  \).
\begin{enumerate}
    \item[(a)] Prove that \( T({V}_{0})  \) is a subspace of \( W  \).
        \begin{proof}
            Let \( {V}_{0}  \) be a subspace of \( V \). Observe that \( T({0}_{V}) = {0}_{W} \in T({V}_{0}) \) since \( {0}_{V} \in {V}_{0} \). Let \( z,y \in T({V}_{0}) \). Then \( T(x) = y  \) and \( T(w) = z  \) for \( x,w \in {V}_{0}  \) implies that
            \[  z + y = T(x) + T(w) = T(x+w) \]
            since \( x+w \in {V}_{0} \). Now, let \( c \in F  \). Hence, \( z + y \in T({V}_{0}) \). Then 
            \[  cy = cT(x) = T(cx) \]
            with \( cx \in {V}_{0} \). Hence, \( cy \in T({V}_{0})\).
        \end{proof}
    \item[(b)] Prove that \( \text{dim}({V}_{0}) = \text{dim}(T({V}_{0})) \).
        \begin{proof} Since \( V  \) and \( W  \) are finite-dimensional vector spaces and that \( {V}_{0} \) and \( T({V}_{0}) \) are subspaces of \( V  \) and \( W  \) respectively, we must have that \( {V}_{0}\) and \( T({V}_{0}) \) be both finite-dimensional by Theorem 1.11. Let \( T({V}_{0}) = {W}_{0} \). Since \( T  \) is an isomorphism, we know that \( T \) must be injective and surjective. So, the Dimension Theorem implies that   
        \[ \text{dim}({V}_{0}) = \text{dim}(N(T)) + \text{dim}(R(T)) = \text{dim}({W}_{0}).\]
        Hence, \( \text{dim}({V}_{0}) = \text{dim}({W}_{0}) \).
        \end{proof}
\end{enumerate}

\subsection*{Exercise 2.4.20} Let \( T: V \to W  \) be a linear transformation from an \( n- \)dimensional vector space \( V  \) to an \( m- \)dimensional vector space \( W  \). Let \( \beta  \) and \( \gamma  \) be ordered bases for \( V  \) and \( W  \), respectively. Prove that \( \text{rank}(T) = \text{rank}({L}_{A}) \) and that \( \text{nullity}(T) = \text{nullity}({L}_{A}) \), where \( A = [T]_{\beta}^{\gamma}  \). 
\begin{proof}
    Observe that \( R(T)  \) is a subspace of \( W  \) and \( R({L}_{A})  \) is a subspace \( F^{m} \). Since \( {\phi}_{\gamma}: W \to F^{m} \) is an isomorphism by Theorem 2.21 (that is, \( \text{dim}(W) = \text{dim}(F^{m}) \)), we can use Exercise 17 to state that \( \text{rank}(T) = \text{rank}({L}_{A}) \). Similarly, \( N(T)  \) and \( N({L}_{A}) \) are subspaces of \( V  \) and \( F^{n} \), respectively. Hence, \( {\phi}_{\beta}: V \to F^{n} \) (that is, \( \text{dim}(V) = \text{dim}(F^{n}) \)) being an isomorphism implies that \( \text{nullity}(T) = \text{nullity}({L}_{A})\). 
\end{proof}

\subsection*{Exercise 2.4.21} Let \( V  \) and \( W  \) be finite-dimensional vector spaces with ordered bases \( \beta = \{ {v}_{1}, {v}_{2}, \dots, {v}_{n} \}  \) and \( \gamma = \{ {w}_{1}, {w}_{2}, \dots, {w}_{m} \}  \), respectively. By Theorem 2.6, there exists linear transformations \( {T}_{ij}: V \to W  \) such that
\[  {T}_{ij}({v}_{k }) = 
\begin{cases}
    {w}_{i} &\text{if } k = j \\
    0 &\text{if } k \neq j. 
\end{cases} \]
First prove that \( \{ {T}_{ij} : 1 \leq i \leq m, 1 \leq j \leq n  \}  \) is a basis for \( \mathcal{L}(V,W)  \). Then let \( M^{ij}  \) be the \( m \times n  \) matrix with \( 1  \) in the \( i \)th row and \( j \)th column and \( 0  \) elsewhere, and prove that \( [{T}_{ij}]_{\beta}^{\gamma}  = M^{ij}  \). Again by Theorem 2.6, there exists a linear transformation \( \Phi: \mathcal{L}(V,W) \to {M}_{m \times n}(F)   \) such that \( \Phi({T}_{ij}) = M^{ij} \). Prove that \( \Phi  \) is an isomorphism.
\begin{proof}
    First, we need to show that \( S = \{ {T}_{ij}: 1 \leq i \leq m, 1 \leq j \leq n  \}  \) is a basis for \( \mathcal{L}(V,W) \); that is, we need to show that
    \[  \sum_{ i=1  }^{ m } {a}_{ij} T_{ij}({v}_{k})= 0 \ \text{ for } \ 1 \leq j \leq n. \tag{1}  \]
    for some scalars \( {a}_{ij}  \). By definition of \( {T}_{ij}  \), observe that for \( 1 \leq j \leq n   \) we have \( j = k  \) such that \( {T}_{ij}({v}_{k }) = {w}_{i} \). So, (1) can be re-written as 
    \[ \sum_{ i=1  }^{ m  } {a}_{ij} {w}_{i} = 0.  \]
    Now, we can use the linear independence of \( \gamma = \{ {w}_{1}, {w}_{2}, \dots, {w}_{m} \}  \) to conclude that \( {a}_{ij} = 0  \) for all \( 1 \leq i \leq m  \). But this means that \( S  \) must be linearly independent. Let \( {T}_{ij} \in \mathcal{L}(V,W)  \) but not in \( S  \). Then adjoining \( {T}_{ij}  \) to \( S  \) produces a linearly dependent set such that \( {T}_{ij} \in \text{span}(S)  \) by Theorem 1.7. Hence, \( S  \) is a basis for \( \mathcal{L}(V,W) \). 

    Using the fact that \( S  \) is a basis for \( \mathcal{L}(V,W) \), we can now write that
    \[  {w}_{i} = {T}_{ij}({v}_{j}) = \sum_{ i=1  }^{ m } {a}_{ij} {w}_{i} \ \text{ for } 1 \leq j \leq n. \tag{2} \] We can see that for \( 1 \leq j \leq n  \) that the matrix representation \( [{T}_{ij}]_{\beta}^{\gamma} \) contains entries \( {a}_{ij} = 1  \) whenever \( i = j  \) and \( 0  \) otherwise. But observe that this is just \( M^{ij} \) and hence \( [{T}_{ij}]_{\beta}^{\gamma} = M^{ij}  \). 
    By Theorem 2.6, we can see that there exists a linear transformation \( \Phi : \mathcal{L}(V,W) \to {M}_{n\times n}(F) \) such that \( \Phi({T}_{ij}) = M^{ij} \). Our goal now is to show that \( \Phi  \) is an isomorphism. That is, we will show that \( \Phi  \) is both injective and surjective. Let \( {T}_{ij}, {U}_{ij} \in \mathcal{L}(V,W)  \). Assume \( \Phi({T}_{ij}) = \Phi({U}_{ij}) \). Then we can write
    \begin{align*}
        \Phi({T}_{ij}) &= \Phi({U}_{ij})   \\
        M^{ij} &= N^{ij} \\
        [{T}_{ij}]_{\beta}^{\gamma} &= [{U}_{ij}]_{\beta}^{\gamma}.
    \end{align*}
    Notice that the last equality implies that \( {T}_{ij} = {U}_{ij} \) by corollary to Theorem 2.6. Hence, \( \Phi  \) is an injective map.

    Let \( {U}_{ij} \in \mathcal{L}(V,W) \). Using ordered bases \( \beta \) and \( \gamma \) and the fact that \( S  \) is a basis for \( \mathcal{L}(V,W) \), we can construct \( [{U}_{ij}]_{\beta}^{\gamma} \) such that
    \[ {w}_{i} = {U}_{ij}({v}_{j}) = \sum_{ i=1  }^{ m } {B}_{ij} {w}_{i} \ \text{ for } \ 1 \leq j \leq n.  \]
    Observe that \(  [{U}_{ij}]_{\beta}^{\gamma} =  N^{ij} = \Phi({U}_{ij})   \) and thus \( \Phi  \) is surjective. We can conclude that \( \Phi  \) is an isomorphism.
\end{proof}

\subsection*{Exercise 2.4.24} Let \( T: V \to Z  \) be a linear transformation of a vector space \( V  \) onto a vector space \( Z  \). Define the mapping
\[  \overline{T}: V / N(T) \to Z \ \text{by} \ \overline{T}(v + N(T)) = T(v)  \] 
for any coset \( v + N(T)  \) in \( V / N(T) \).
\begin{enumerate}
    \item[(a)] Prove that \( \overline{T} \) is well-defined; that is, prove that if \( v + N(T) = v' + N(T)  \), then \( T(v) = T(v')  \).
        \begin{proof}
        Suppose \( v + N(T) = v' + N(T) \). Then 
        \begin{align*}
            T(v) &= \overline{T}(v + N(T)) \\
                 &= \overline{T}(v' + N(T)) \\
                 &= T(v').
        \end{align*}
        Hence, \( \overline{T}  \) is well-defined map.
        \end{proof}
    \item[(b)] Prove that \( \overline{T}  \) is linear.
        \begin{proof}
        Let \( a \in F  \) and \( v + N(T), u + N(T) \in V / N(T) \). Then using the operations defined in {\hyperref[Exercise 1.3.31]{Exercise 1.3.31}} and the fact that \( T  \) is linear, we have 
        \begin{align*}
            \overline{T}\Big(a(v + N(T)) + (u + N(T))\Big) &= \overline{T} \Big(  (av + N(T)) + (u + N(T)) \Big) \\
                                                           &= \overline{T}( (av + u) + N(T)) \\
                                                           &= T(av + u) \\
                                                           &= aT(v) + T(u) \\
                                                           &= a \overline{T}(v + N(T)) + \overline{T}(u + N(T)).
        \end{align*}
    Thus, \( \overline{T} \) is linear.
        \end{proof}
    \item[(c)] Prove that \( \overline{T}  \) is an isomorphism.
        \begin{proof}
            First, we show that \( \overline{T} \) is an injective map. Let \( x + N(T), y + N(T) \in V / N(T) \). Then observe that
            \begin{align*}
                \overline{T}(x + N(T)) &= \overline{T}(y + N(T)) \\
                T(x) &= T(y). 
            \end{align*}
            Since \( T  \) is linear, we can write
            \[  T(x) = T(y) \iff T(x-y) = 0. \]
            Hence, \( x - y \in N(T) \). Since \( N(T)  \) is a subspace of \( V  \), we have \( x + N(T) = y + N(T)  \) by Exercise 1.3.31. This implies that \( \overline{T}  \) is injective. 

            Now, we will show that \( \overline{T}  \) is a surjective map. Since \( T  \) is a linear transformation from \( V  \) onto \( Z  \), we know that any arbitrary \( y \in Z  \) can be written as \( T(x) = y  \) for some \( x \in V  \). By definition of \( \overline{T} \), we have
            \[  y = T(x) = \overline{T}(x + N(T)) \] where \( x + N(T) \in V / N(T) \).
            Hence, \( \overline{T} \) is a surjective linear map. Thus, \( \overline{T} \) is an isomorphism.
        \end{proof}
    \item[(d)] Prove that the diagram shown in Figure 2.3 commutes; that is, prove that \( T = \overline{T} \eta \).
        \begin{proof}
        Let \( v \in V  \). Since \( \eta: V \to V / N(T) \) is defined by \( \eta(v) = v + N(T) \), we can write that
        \begin{align*}
            T(v) &= \overline{T}(v + N(T)) \\
                 &= \overline{T}(\eta(v)) \\
                 &= \overline{T}\eta(v).
        \end{align*}
        Since \( v \in V  \) is arbitrary, we know that \( T = \overline{T}\eta \).
        \end{proof}
\end{enumerate}



